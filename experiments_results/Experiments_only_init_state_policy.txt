----------- Resultados de los experimentos usando solo la initial state policy (sin la goal policy)

"""
Resultados (con la initial state policy nueva):

> Cont. consist: nada (solo evitar átomos repetidos y átomos con params. repetidos) - 
	Eventual consist: nada (solo que el estado inicial contenga todos los predicados necesarios)
	> Funciona

> Always pick ontable
	> Funciona

> Cont. consist: nada - (solo evitar átomos repetidos)
	Eventual consist: ejecutar termination condition con num_preds entre 1 y 9
	Funciona sin capas intermedias o una única capa intermedia de la NLM (y lr=5e-3)

	PARECE QUE SI NO USO NINGUNA CAPA INTERMEDIA DE LA NLM ENTONCES SÍ FUNCIONA!!!
	(en ese caso, son las operaciones internas de la NLM las que hacen que se "pierda" la información
	del perc_actions_executed (quizás al ser un valor real en vez de binario))

	2, 2 (con una capa intermedia) -> 1/3
	1.5, 1.5 (con una capa intermedia) -> 4/5
	1, 1 (con una capa intermedia) -> 3/5
	1, 1 (sin capas intermedias) -> 5/5, -> FUNCIONA MEJOR QUE CUANDO USO 1 CAPA INTERMEDIA!!
	1, 1 (dos capas intermedias) -> 0/3 -> FUNCIONA MUCHO PEOR QUE CON SOLO UNA CAPA INTERMEDIA!!
	1, 1 (dos capas intermedias) -> 2/3, 0 usando 10000 training epochs

	Con 2 capas intermedias y 8 predicados para cada ariedad a veces la probabilidad de la term. condition va a 0 (esto no debería pasar)
	Además, necesita una mayor entropy regularization para la política y más iteraciones.

	>> Cuanto más capas añado a la NLM, más dificultad tiene para "recordar" el perc_actions_exec que se le da como input
	    a la primera capa

	>> PROBAR A USAR RELU EN VEZ DE SIGMOID PARA LA NLM!! -> No funciona

	>> Usar residual connections

	----------------- Pruebas con residual connections

	> Always pick ontable
	> Sin residual connections: Funciona (si uso 1,1 para los coefs. de entropy regularization)
	> Con residual connections: Funciona si uso un lr de 1e-3 y una entropy regularization de 0.1 o menor.

	> Termination condition con 1-9 átomos en el estado (lr=1e-3, regularization coeffs=0.1, 0.1)
	> Sin residual connections: 
		> Sin capas intermedias: Funciona
		> Con una capa intermedia [4,4,4,4]: Funciona (con 10000 train epochs)
	> Con residual connections: 
		> Con una capa intermedia [4,4,4,4]: Funciona (con 10000 train epochs)
		> Con una capa intermedia [8,8,8,8]: Funciona (con 10000 train epochs)
		> Con dos capas intermedias [8,8,8,8]: Funciona (con 10000 train epochs)
		> Con tres capas intermedias [8,8,8,8]: Funciona (con 10000 train epochs)

	> Termination condition con 3-7 átomos en el estado (con residual connections, con tres capas intermedias [8,8,8,8])
	> lr=1e-3, reg_coeffs = 0.1: no funciona
	> lr=1e-3, reg_coeffs = 0.05: no funciona
	> lr=5e-3, reg_coeffs = 0.05: no funciona (con ese lr, la probabilidad del term_condition se va a 0!!)
	> lr=5e-4, reg_coeffs = 0.05: Funciona (parece que la NLM necesita una lr de 5e-4 para aprender mejor!)

	> Predicates in current phase (residual connections, dos capas intermedias [4,4,4,4]):
	> lr=5e-4, reg_coeffs = 0.05: funciona, pero a veces escoge acciones incorrectas (creo que porque la política es demasiado estocástica debido a la entropy regularization)
	> lr=5e-4, reg_coeffs = 0.02: funciona perfectamente.

	> Predicates in current phase y termination condition con 3-7 átomos en el estado (residual connections, dos capas intermedias [4,4,4,4]):
	> lr=5e-4, reg_coeffs = 0.02, 10000 training epochs: regular (ejecuta la termination condition una acción demasiado pronto) 
	> lr=5e-4, reg_coeffs = 0.02, 20000 training epochs: regular (ejecuta bien la termination condition pero algunas veces selecciona acciones incorrectas)
	> lr=1e-4, reg_coeffs = 0.02, 30000 training epochs: regular (ejecuta la termination condition una acción demasiado pronto y selecciona una acción incorrecta) 
	> lr=5e-4, reg_coeffs = 0.0, 50000 training epochs: Funciona muy mal (la prob_term_cond va a 0)
	> Tres capas intermedias [8,8,8,8], lr=5e-4, reg_coeffs = 0.00, 30000 training epochs: Funciona muy mal (la prob_term_cond va a 0)
	> Tres capas intermedias [8,8,8,8], lr=5e-5, reg_coeffs = 0.00, 30000 training epochs: Funciona mal (escoge los átomos en el orden incorrecto y selecciona la condición de parada demasiado pronto)
	> Tres capas intermedias [4,4,4,4], lr=1e-5, reg_coeffs = 0.0, 50000 training epochs: Funciona mal (escoge los átomos en el orden incorrecto y selecciona la condición de parada demasiado pronto) -> creo que el lr es demasiado bajo!
	> Tres capas intermedias [4,4,4,4], lr=2e-4, reg_coeffs = 0.02, 200000 training epochs: Funciona regular (ejecuta bien la termination condition pero algunas veces selecciona acciones incorrectas)

	> Tres capas intermedias [4,4,4,4], lr=5e-4, reg_coeffs = 2, 100000 training epochs: Funciona mal (la recompensa media en la gráfica es de -0.8) 
	> Tres capas intermedias [4,4,4,4], lr=5e-4, reg_coeffs = 1, 200000 training epochs: Funciona regular (ejecuta bien la termination condition pero algunas veces selecciona acciones incorrectas)
	> Tres capas intermedias [4,4,4,4], lr=5e-4, reg_coeffs = 0.5, 100000 training epochs: Funciona casi bien (ejecuta bien la termination condition y escoge casi todas las acciones en el orden correcto (aunque no siempre))
	> Tres capas intermedias [4,4,4,4], lr=5e-4, reg_coeffs = 0.2, 100000 training epochs: 
		> Ejecución 1: Funciona perfectamente (escoge bien las acciones en orden y la termination condition)
		> Ejecución 2: Funciona regular (escoge algunas acciones en orden incorrecto pero la termination condition bien)
	> Tres capas intermedias [4,4,4,4], lr=5e-4, reg_coeffs = 0.05, 200000 training epochs: Regular (recompensa llega a -0.5 y después baja)
	> Tres capas intermedias [4,4,4,4], lr=5e-4, reg_coeffs = 0.02, 100000 training epochs: Mal (escoge las acciones en mal orden y mal la termination condition,
		    la recompensa llega a -0.5 y después baja lentamente (funciona parecido a reg_coeffs=0.05))
		
	> Tres capas intermedias [4,4,4,4], reg_coeffs = 0, 200000 training epochs:
		> Ejecución 1, lr=5e-4: PERFECTO (escoge bien todas las acciones y termination condition, la recompensa se aproxima a 0)
		> Ejecución 2, lr=1e-3: PERFECTO (escoge bien todas las acciones y termination condition, la recompensa se aproxima a 0)
		> Ejecución 3, lr=5e-3, 100000 training epochs: PERFECTO
		> Ejecución 4, lr=1e-2 100000 training epochs: PERFECTO
		
	> Pruebas con muy poca entropy reg (lr=1e-2): ---> EL LR DE 1E-2 ES DEMASIADO ALTO!!
		> reg_coeffs = 0.01, 0.01: Muy mal (la recompensa llega a -0.6 y después empieza a caer en picado!)
		> reg_coeffs = 0.01, 0: No aprende (la recompensa no converge a 0)
		> reg_coeffs = 0, 0.01: No aprende (la recompensa no converge a 0)
		> reg_coeffs = 0.001, 0.001: No aprende
		> reg_coeffs = 0.0001, 0.0001: No aprende
		> reg_coeffs = 0.00001, 0.00001: No aprende

		>> lr=1e-2, reg_coeffs = 0, 0: NO APRENDE (la prob term_cond va a 0)
		>> lr=5e-3, reg_coeffs = 0, 0: APRENDE

	> Pruebas con muy poca entropy reg (lr=5e-3):
		> reg_coeffs = 1e-9, 1e-9: Aprende
		> reg_coeffs = 1e-8, 1e-8: Aprende
		> reg_coeffs = 1e-7, 1e-7: Aprende
		> reg_coeffs = 5e-7, 5e-7: No aprende
		> reg_coeffs = 1e-6, 1e-6:
			> Repetición 1: No Aprende
			> Repetición 2: No Aprende

	> Pruebas con varias trayectorias por cada época:
		> 10 trajectories_per_epoch, reg_coeffs = 5e-7, 5e-7: Aprende, aunque converge a r=-0.05
		> 10 trajectories_per_epoch, reg_coeffs = 0, 0: Aprende (y llega a r=0 más rápido que con reg_coeffs = 5e-7, 5e-7)
		> 10 trajectories_per_epoch, reg_coeffs = 1e-2, 1e-2: No aprende (la recompensa termina divergiendo a r=-0.8)



---------------------------------------------------------------------

>> Consistency rules Blocksworld (SIN predicado "on" obligatorio)
	> 3 capas intermedias con 4 preds, reg_coeffs=0 0, lr=5e-3, 10 trajectories_per_epoch, 20000 train its: 
		    Genera los 10 problemas perfectamente y el reward converge a 0. No obstante, de los 10 problemas, 9 son idénticos ->
			< LA POLÍTICA ES DEMASIADO DETERMINISTA >

 	> 4 capas intermedias con 8 preds, reg_coeffs=0 0, lr=5e-3, 10 trajectories_per_epoch, 20000 train its:
		> Ejecución 1: funciona mucho peor que con 4 capas (la recompensa diverge a -0.8)
		> Ejecución 2: funciona bien (recompensa converge a 0 pero la política es demasiado determinista)

>> Consistency rules Blocksworld (CON predicado "on" obligatorio):
	> 4 capas intermedias con 8 preds, reg_coeffs=0 0, lr=5e-3, 10 trajectories_per_epoch, 20000 train its:
		No aprende (la recompensa diverge)

	> 3 capas intermedias con 4 preds, reg_coeffs=0 0, lr=5e-3, 10 trajectories_per_epoch, 20000 train its: 
		No aprende (la recompensa diverge)

	> 3 capas intermedias con 4 preds, reg_coeffs=1e-7 1e-7, lr=5e-3, 10 trajectories_per_epoch, 35000 train its:
		No aprende (la recompensa llega en un punto a -0.05 pero después vuelve a diverger a -0.5)

	> 3 capas intermedias con 4 preds, reg_coeffs=0 0, lr=2e-3, 10 trajectories_per_epoch, 20000 train its:
		No aprende (la recompensa se queda en -0.5) y la gráfica de recompensa es igual de ruidosa


	> 3 capas intermedias con 4 preds, reg_coeffs=5e-7 5e-7, lr=5e-3, 10 trajectories_per_epoch, 20000 train its: 
		No aprende (casi todos los problemas generados son idénticos, con solo predicados de tipo ontable pero ninguno más (excepto
			        algún clear o handempty ocasional)) -> Converge muy rápido a una política subóptima, donde solo aprende
					a añadir átomos de tipo ontable

	> 3 capas intermedias con 4 preds, reg_coeffs=1e-2 1e-2, lr=5e-3, 10 trajectories_per_epoch, 10000 train its:
		La entropía de la política desciende demasiado rápido.

	> 3 capas intermedias con 4 preds, reg_coeffs=1e-1 1e-1, lr=5e-3, 10 trajectories_per_epoch, 10000 train its:
		La entropía de la política desciende demasiado rápido.

	> 3 capas intermedias con 4 preds, reg_coeffs=1 1, lr=5e-3, 10 trajectories_per_epoch, 10000 train its:
		La entropía disminuye lentamente, pero la recompensa no sube de -0.5 (creo, ya que la ejecución se paró a mitad).

	> 4 capas intermedias con 8 preds, reg_coeffs=0.5 0.5, lr=5e-3, 10 trajectories_per_epoch, 20000 train its:
		No aprende (la recompensa diverge)

	> 4 capas intermedias con 8 preds, reg_coeffs=1 1, lr=5e-3, 10 trajectories_per_epoch, 10000 train its:
		No aprende bien (la recompensa se queda alrededor de -0.8) -> Quizás necesita una NLM más compleja o más iteraciones de entrenamiento!!

------------------ PRUEBAS ACTOR-CRITIC (sin PPO) ---------------- 

> Always pick ontable (entropy reg: 0 0)
	> Funciona perfectamente

> Predicates in current phase (entropy reg: 0 0):
	> Funciona perfectamente

>> Consistency rules Blocksworld (CON predicado "on" obligatorio):

	> 4 capas intermedias con 8 preds, reg_coeffs=0 0, lr=5e-3, 10 trajectories_per_epoch, 20000 train its: 
		No aprende (la recompensa converge a -0.44)

	> 4 capas intermedias con 8 preds, reg_coeffs=1 1, lr=5e-3, 10 trajectories_per_epoch, 20000 train its: 
		No aprende (la recompensa termina en -0.5 y la entropía en 0.01) -> Si se dejaran más iteraciones
		(la ejecución se cortó tras 20k), quizás sí podría aprender, ya que la gráfica de recompensa seguía aumentando
		un poco cuando terminó la ejecución

	> 4 capas intermedias con 8 preds, reg_coeffs=0.1 0.1, lr=5e-3, 10 trajectories_per_epoch, 20000 train its:
		No aprende (la recompensa converge a -1) -> parece que da igual el número de iteraciones!

>>> PRUEBA CAMBIANDO EL SIGNO DEL REINFORCE_LOSS (quitando el -):
	En vez de maximizar la recompensa, la minimiza!! -> Es necesario el símbolo "-" para que maximice la recompensa!

------------------ PRUEBAS PPO ---------------- 
	
> Always pick ontable (entropy reg: 0 0)
	> Funciona

> Predicates in current phase (entropy reg: 0 0):
	> Funciona

	> Prueba con lr=1e-3:
		La recompensa sube más lenta, pero el critic loss tiene menos picos

	> lr=5e-3, loss_critic_coeff=0.5, epochs_per_train_it=5:
		La recompensa sube más lenta y el critic loss es peor!	

	> lr=5e-3, loss_critic_coeff=1, epochs_per_train_it=5:
		El critic loss es aún peor que con 	loss_critic_coeff=0.5

	> lr=5e-3, loss_critic_coeff=0.1, epochs_per_train_it=5:
		El critic loss es bueno, pero la recompensa sigue aumentando lentamente.

	> lr=5e-3, loss_critic_coeff=1, epochs_per_train_it=5, critic entrenado en cada época (al igual que el actor):
		Funciona perfectamente! -> La recompensa converge rápidamente y el critic loss es bajo.
		No obstante, no se aprecian mejoras respecto a usar 3 épocas!

	> lr=5e-3, entropy_coeffs=0 0, epochs_per_train_it=3, epsilon=0.1 - 0.2 - 0.3:
		Mismos resultados sin importar el epsilon

	> lr=5e-3, entropy_coeffs=0 0, epochs_per_train_it=3, epsilon=0.2, 5 epochs_per_train_it:
		Tarda más en aprender que con 3 epochs_per_train_it pero es más suave.

	> lr=5e-3, entropy_coeffs=1 1, epochs_per_train_it=3, epsilon=0.2, 3 epochs_per_train_it:
		Aprende, aunque la entropía es demasiado alta (la recompensa converge a -0.1)

	> lr=5e-3, entropy_coeffs=0.1 0.1, epochs_per_train_it=3, epsilon=0.2, 3 epochs_per_train_it:
		Aprende, aunque la entropía es un poco demasiado alta y hace que a veces selecciona acciones incorrectas
		(converge a una r=-0.03, casi perfecta pero no del todo). La entropía de la política es aprox. 0.25.

	> lr=5e-3, entropy_coeffs=0.01 0.01, epochs_per_train_it=3, epsilon=0.2, 3 epochs_per_train_it:
		Mejores resultados que con entropy_coeffs = 0.1 0.1: la recompensa converge a 0, el critic loss
		es más bajo y la entropía es más baja que en el caso anterior pero tampoco demasiado baja

	> NO skip connections, lr=5e-3, entropy_coeffs=0.01 0.01, epochs_per_train_it=3, epsilon=0.2, 3 epochs_per_train_it:
		Al quitar las skip connections empeoran los resultados -> la recompensa converge pero tarda más y la entropía
		final de la política es menor. -> Mejor usar skip connections

	> 6 capas intermedias con 8 preds, Skip connections, lr=5e-3, entropy_coeffs=0.01 0.01, epochs_per_train_it=3, epsilon=0.2, 3 epochs_per_train_it:
		Aprende bien (la recompensa converge a 0, aunque con picos por la entropía). La política converge a una entropía alta
		(aprox. 0.15).


>> Consistency rules Blocksworld (CON predicado "on" obligatorio):

	> lr=5e-3, entropy coeffs=0 0, epsilon=0.2, epochs_per_train_it=3:
		La recompensa converge a -0.45, tal y como sucede con Actor-critic.

	> 6 capas intermedias con 8 preds, residual_connections, lr=5e-3, entropy coeffs=0.01 0.01, epsilon=0.2, epochs_per_train_it=3:
		No aprende (converge a r=-0.45).

	>> 6 capas intermedias con 8 preds, residual_connections, lr=5e-3, entropy coeffs=0.1 0.1, epsilon=0.2, epochs_per_train_it=3:
		>> Aprende!! Converge a r=-0.05 a partir de 600 iteraciones (3h 30 min aprox.) pero después vuelve a diverger ->
			creo que la entropía es demasiado alta!!!

	> 6 capas intermedias con 8 preds, residual_connections, lr=5e-3, entropy coeffs=0.01 0.01, epsilon=0.2, epochs_per_train_it=3 (experimento repetido ahora con más its):
		No aprende -> la recompensa converge a -0.45. Creo que la entropía de la política es demasiado baja como para explorar
		lo suficiente para encontrar la política óptima.

	> 6 capas intermedias con 8 preds, residual_connections, lr=5e-3, entropy coeffs=1 1, epsilon=0.2, epochs_per_train_it=3:
		Aprende, pero la recompensa converge a -0.2. Creo que la entropía es demasiado alta.

	> 6 capas intermedias con 8 preds, residual_connections, lr=5e-3, entropy coeffs=0.5 0.5, epsilon=0.2, epochs_per_train_it=3:
		No aprende: la recompensa converge a -0.55.

	> 6 capas intermedias con 8 preds, residual_connections, lr=5e-3, entropy coeffs=0.1 0.1, epsilon=0.2, epochs_per_train_it=3,
		Entrenando critic solo durante la primera época:
		Mal, el critic loss oscila mucho y no aprende.
		    
	> 6 capas intermedias con 8 preds, residual_connections, lr=5e-3, entropy coeffs=0.1 0.1, epsilon=0.2, epochs_per_train_it=3:
		(segunda ejecución del mismo experimento) -> QUIERO VER SI 0.1 DE ENTROPÍA ES SUFICIENTE O HACE FALTA USAR 1!!
		No aprende (la r converge a -0.5). >> Parece que con entropy_coeffs=0.1 0.1 no siempre converge a r=0!!!

	> 6 capas intermedias con 8 preds, residual_connections, lr=5e-3, entropy coeffs=1 1, epsilon=0.2, epochs_per_train_it=3:
		Aprende! (aunque la recompensa converge a -0.36). Un valor de entropy_coeffs de 1 1 "asegura" que aprenda pero
		la entropía es muy alta por lo que muchas veces coge acciones incorrectas!

	> 6 capas intermedias con 8 preds, residual_connections, lr=5e-3, entropy coeffs=0.1 0.1, <epsilon=0.1>, epochs_per_train_it=3:
		Repetición 1: Aprende y muy rápido!! (converge a r=-0.1) y una entropía bastante alta de 0.15.
		Repetición 2: No aprende (la recompensa converge a r=-0.45)

	> 6 capas intermedias con 8 preds, residual_connections, lr=5e-3, entropy coeffs=1 1, <epsilon=0.1>, epochs_per_train_it=3:
		Repetición 1: Aprende! (aunque la recompensa converge a -0.25 debido a usar una entropía tan alta.)
		Repetición 2: Aprende! (aunque la recompensa converge a -0.25 debido a usar una entropía tan alta.)

	> 6 capas intermedias con 8 preds, residual_connections, lr=5e-3, entropy coeffs=1 1, epsilon=0.1, epochs_per_train_it=3,
		entropy_annealing = (100, 0.1, 0.1):
		Aprende y rápido, aunque la recompensa converge a r=-0.2.

	> 6 capas intermedias con 8 preds, residual_connections, lr=5e-3, entropy coeffs=1 1, epsilon=0.1, epochs_per_train_it=3,
		entropy_annealing = (100, 0.01, 0.01):
		Ejecución 1: Aprende y rápido (converge a r=-0.03).
		Ejecución 2: No aprende (converge a r=-0.44).

	> 6 capas intermedias con 8 preds, residual_connections, lr=5e-3, entropy coeffs=1 1, epsilon=0.1, epochs_per_train_it=3,
		entropy_annealing = (150, 0.01, 0.01):
		Ejecución 1: No aprende (converge a r=-0.45)
			
	> 6 capas intermedias con 8 preds, residual_connections, lr=5e-3, entropy coeffs=1 1, epsilon=0.1, epochs_per_train_it=3,
		entropy_annealing = (200, 0.01, 0.01):
		No aprende (converge a r=-0.5)

	> 6 capas intermedias con 8 preds, residual_connections, lr=5e-3, entropy coeffs=1 1, epsilon=0.1, epochs_per_train_it=3,
		entropy_annealing = (300, 0.01, 0.01):
		Aprende (la recompensa converge a r=0) pero conforme va disminuyendo los entropy reg coeffs a 0.01, la entropía de la política
		cae mucho y la recompensa termina divergiendo a r=-0.45.
			
	> 6 capas intermedias con 8 preds, residual_connections, lr=5e-3, entropy coeffs=1 1, epsilon=0.1, epochs_per_train_it=3,
		entropy_annealing = (100, 0.1, 0.1) (tercera ejecución de este experimento):
			Aprende, aunque la recompensa converge a r=-0.1 y a veces comete errores.

	> 6 capas intermedias con 8 preds, residual_connections, lr=5e-3, entropy coeffs=1 1, <epsilon=0.05>, epochs_per_train_it=3,
		entropy_annealing = (100, 0.1, 0.1)
			No hay diferencia respecto a epsilon=0.1.

	> 6 capas intermedias con 8 preds, residual_connections, lr=5e-3, entropy coeffs=1 1, epsilon=0.1, <epochs_per_train_it=1>,
		entropy_annealing = (100, 0.1, 0.1)
			No aprende (converge a r=-0.5).

	> 6 capas intermedias con 8 preds, residual_connections, lr=5e-3, entropy coeffs=1 1, epsilon=0.1, <epochs_per_train_it=5>,
		entropy_annealing = (100, 0.1, 0.1)
			Aprende más rápido que con 3 epochs_per_train, pero tarda más al usar más iteraciones -> no merece la pena.

	> 6 capas intermedias con 8 preds, residual_connections, lr=5e-3, entropy coeffs=1 1, epsilon=0.1, epochs_per_train_it=3,
		entropy_annealing = (100, 0.1, 0.1), <trajectories_per_train_it=10>, <minibatch_size=25>:
			Funciona peor que con 100 trajectories_per_train_it (aprende al principio (converge a r=-0.2) pero al final diverge).
			Es mejor usar más trayectorias para el entrenamiento!

------

<< El mejor epsilon es 0.1 (se aprende mejor que con epsilon=0.2) >>
<< La mejor entropía es 1 (asegura que aprenda) pero es demasiado alta y coge muchas acciones erróneas >>
<< El mejor entropy_annealing es empezar con entropy_coeffs 1 1 y usar entropy_annealing = (100, 0.1, 0.1)>>

------

>>> TODO experiments:
	> Probar a entrenar menos el critic (que aprenda más lentamente)
		Al entrenarlo solo durante la primera época, no funciona bien.

	> Probar a empezar con entropy coeffs=1 1 e ir disminuyendo hasta entropy coeffs=0.1 0.1
		Funciona muy bien!

	> Aumentar de 0.8 a 0.95 el disc_factor_eventual, para que se le de más importancia a las eventual_consistency rules
		No lo he probado.

	> Probar a añadir una hidden layer a los MLPs de la NLM
		No lo he probado.

	> Probar a ir disminuyendo el learning rate y clipping parameter (epsilon) conforme avanza el entrenamiento
		No lo he probado.
"""