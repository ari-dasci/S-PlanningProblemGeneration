---------------- Training only init policy

> init_policy_entropy_coeffs = 1 1, entropy_annealing_coeffs_init_state_policy = (300, 0.1, 0.1), 
	init_policy_entropy_coeffs = 1 1, entropy_annealing_coeffs_goal_policy = (300, 0.1, 0.1),
	<<only train init policy>>:
		La r_continuous y r_eventual no convergen sino que divergen. Esto sucede justo cuando
		la policy entropy deja de disminuir y empieza a aumentar -> Creo que los entropy_coeffs
		son demasiado altos!

> init_policy_entropy_coeffs = 1 1, <entropy_annealing_coeffs_init_state_policy = (300, 0.01, 0.01)>, 
	init_policy_entropy_coeffs = 1 1, <entropy_annealing_coeffs_goal_policy = (300, 0.01, 0.01)>,
	<<only train init policy>>:
		No anoté los resultados, aunque creo que son parecidos/peores al experimento anterior.

> Pruebas gradient_clip_val:
	> 1e-4 -> Solo converge la r_continuous (a 0), pero la r_eventual se queda en -0.45.
	> 1e-6 -> Igual que con 1e-4 (no sé si el gradient_clip_val funciona).
	> 1e-15 -> NO APRENDE! (parece que el gradient clipping sí funciona)
	> 1e-10 -> Tampoco aprende.
	>>> 1e-8 -> Aprende parcialmente (r_continuous converge a 0 pero r_eventual no cambia).
	> 1e-7 -> La r_continuos converge a 0 (al igual que con gradient_clip_val=1e-8 pero más inestable) y
	          la r_eventual es inestable hasta que converge a r=-0.45 (no aprende).

	>>> Mejor gradient_clip_val hasta ahora -> 1e-8


> init_policy_entropy_coeffs = 1 1, entropy_annealing_coeffs_init_state_policy = (300, 0.01, 0.01), 
	init_policy_entropy_coeffs = 1 1, entropy_annealing_coeffs_goal_policy = (300, 0.01, 0.01),
	only train init policy, <gradient_clip_val=1e-8>, <penalization_eventual_consistency=-2>:
		Mismos resultados que con penalization_eventual_consistency=-1 (r_continuous converge
		a 0 mientras que r_eventual diverge). La entropía de la política disminuye muy rápidamente.

> init_policy_entropy_coeffs = 1 1, entropy_annealing_coeffs_init_state_policy = (300, 0.01, 0.01), 
	init_policy_entropy_coeffs = 1 1, entropy_annealing_coeffs_goal_policy = (300, 0.01, 0.01),
	only train init policy, gradient_clip_val=1e-8, <penalization_eventual_consistency=-10>:
		No funciona, la r_eventual sigue sin ir a 0 mientras que la r_continuous sí.


No sirve de nada cambiar la penalization_eventual_consistency


> init_policy_entropy_coeffs = 1 1, <entropy_annealing_coeffs_init_state_policy = (200, 0.1, 0.1)>, 
	init_policy_entropy_coeffs = 1 1, <entropy_annealing_coeffs_goal_policy = (200, 0.1, 0.1)>,
	only train init policy, <gradient_clip_val=1e-8>, <penalization_eventual_consistency=-1>,
	<trajectories_per_train_it=100>:
		Solo converge a 0 la r_continuous, mientras que la r_eventual termina en alrededor de
		-0.4. La policy entropy vuelve a disminuir muy rápido!

> init_policy_entropy_coeffs = 1 1, <entropy_annealing_coeffs_init_state_policy = (100, 0.1, 0.1)>, 
	init_policy_entropy_coeffs = 1 1, <entropy_annealing_coeffs_goal_policy = (100, 0.1, 0.1)>,
	only train init policy, <SIN gradient_clip_val>, penalization_eventual_consistency=-1,
	<trajectories_per_train_it=100>: 
		>>> APRENDE!! (la r_continuous converge a 0 y la r_eventual también (aunque tarda bastante)).
		    No obstante, la r_continuous es bastante inestable y tiene "picos" (en vez de converger
			perfectamente a 0 converge a una media de -0.1).

> init_policy_entropy_coeffs = 1 1, entropy_annealing_coeffs_init_state_policy = (100, 0.1, 0.1), 
	init_policy_entropy_coeffs = 1 1, entropy_annealing_coeffs_goal_policy = (100, 0.1, 0.1),
	only train init policy, penalization_eventual_consistency=-1,
	trajectories_per_train_it=100, 
	<<pruebas distintos valores gradient_clip_val>>:  
		1e-7 -> El valor es demasiado bajo (la recompensa converge muy lentamente).
		1e-6 -> Demasiado bajo
		1e-5 -> Demasiado bajo
		1e-4 -> Demasiado bajo
		1e-3 -> Demasiado bajo
		1e-2 -> Funciona igual que sin gradient_clip_val
	

Parece que el gradient_clip_val no ayuda a entrenar las políticas!!!



> init_policy_entropy_coeffs = 1 1, entropy_annealing_coeffs_init_state_policy = (100, 0.1, 0.1), 
	init_policy_entropy_coeffs = 1 1, entropy_annealing_coeffs_goal_policy = (100, 0.1, 0.1),
	only train init policy, <SIN gradient_clip_val>, penalization_eventual_consistency=-1,
	<trajectories_per_train_it=50, minibatch_size=125>: 
		Aprende (r_continuous converge a -0.05 y r_eventual a -0.05). No obstante,
		aunque hayan convergido, sigue habiendo algunos picos.


------------- Training both policies

>  init_policy_entropy_coeffs = 1 1, entropy_annealing_coeffs_init_state_policy = (300, 0.01, 0.01), 
   init_policy_entropy_coeffs = 1 1, entropy_annealing_coeffs_goal_policy = (300, 0.01, 0.01):
	Aprende (la reward de la init state policy llega a 1.5 y la de la goal policy a 3.4) pero después
	la reward de la init state policy diverge a -0.83.

> init_policy_entropy_coeffs = 1 1, <entropy_annealing_coeffs_init_state_policy = (300, 0.1, 0.1)>, 
  init_policy_entropy_coeffs = 1 1, <entropy_annealing_coeffs_goal_policy = (300, 0.1, 0.1)>,
  <goal_policy_train_epochs dependiendo del num goal policy train samples>:
	Aprende algo hasta cierto momento (en la goal policy la r_difficulty llega a 3.7 y en la init
	policy la r_eventual llega a -0.15). No obstante, la r_continuous de la init policy no deja
	de disminuir durante todo el entrenamiento y a partir de cierto número de iteraciones la r_difficulty
	se va a 0 ya que los estados iniciales generados no son nunca eventual-consistent.
	>> Creo que la recompensa por difficultad es demasiado grande respecto a r_eventual y r_continuous,
	   por lo que eso hace que la init policy "se olvide" de la r_eventual y r_continuous e intente solo
	   optimizar la dificultad.

> init_policy_entropy_coeffs = 1 1, entropy_annealing_coeffs_init_state_policy = (300, 0.1, 0.1), 
  init_policy_entropy_coeffs = 1 1, entropy_annealing_coeffs_goal_policy = (300, 0.1, 0.1),
  goal_policy_train_epochs dependiendo del num goal policy train samples,
  <r_difficulty rescale_factor 0.1>:
	Resultados parecidos a sin r_difficulty rescale_factor 0.1 (experimento anterior), aunque
	ahora la r_continuous se mantiene alta (cercana a 0) durante parte del entrenamiento.
	>> Cuando la r_continuous aumenta la r_eventual disminuye y viceversa!!

> init_policy_entropy_coeffs = 1 1, <entropy_annealing_coeffs_init_state_policy = (100, 0.1, 0.1)>, 
  init_policy_entropy_coeffs = 1 1, <entropy_annealing_coeffs_goal_policy = (100, 0.1, 0.1)>,
  <goal_policy_train_epochs dependiendo del num goal policy train samples: 0-3>,
  r_difficulty rescale_factor = 0.1,
  <trajectories_per_train_it=50, minibatch_size=125>:
		>>>> Aprende!!!! -> las r_eventual y r_continuous convergen a -0.1 y la r_difficulty (de la goal policy)
		     alcanza 0.55!!
			 No obstante, las recompensas son bastante inestables (suben y bajan con bastantes picos).
			 Después de muchas its, la r_eventual converge a -0.45 y la r_difficulty a 0 -> deja de generar
			 estados iniciales consistentes eventualmente. <<Esto sucede cuando la entropía de la política
			 desciende mucho (a 0.02).>>

			# --- Resultados problem generation (model folder = both_policies_35)
			# 60 its -> poca dificultad y los problemas siguen siendo poco diversos!
			# 100 its -> avg. diff = 54, problemas poco diversos
			# 110 its -> dificultad baja
			# 120 its -> malo (poca dificultad y problemas parecidos)
			# >> 140 its -> avg. diff = 553.9 y los problemas son más o menos diversos!
			# 160 its -> avg. diff = 65 y los problemas son un poco menos diversos -> El entrenamiento es muy inestable y hay grandes cambios de un num_its a otro número parecido!
			# > 170 its -> avg. diff = 1403 <pero los problemas son muy poco diversos!>
			# > 260 its -> avg. diff = 475.4 pero los problemas son poco diversos
			# >> 320 its -> avg. diff = 900 y diversidad media tirando a baja


> init_policy_entropy_coeffs = 1 1, <entropy_annealing_coeffs_init_state_policy = None>, 
  init_policy_entropy_coeffs = 1 1, <entropy_annealing_coeffs_goal_policy = None>,
  <r_difficulty rescale_factor = 0.2>,
  trajectories_per_train_it=50, minibatch_size=125,
  <max_actions_init_state = 30>:
	Aprende y más rápido que en el experimento anterior!: r_continuous converge a -0.15, r_eventual a -0.05
	y r_difficulty (goal_policy) a 0.6 (ojo, al usar rescale_factor = 0.2 es como si fuera r_difficulty=0.3).
	La entropía de la goal policy es casi tan alta como al empezar el entrenamiento.

	# --- Resultados problem generation (model folder = both_policies_36)
	Los problemas generados son bastante diversos (en general), pero poco difíciles de resolver.


> <init_policy_entropy_coeffs = 0 1>, entropy_annealing_coeffs_init_state_policy = None, 
  <init_policy_entropy_coeffs = 0 1>, entropy_annealing_coeffs_goal_policy = None,
  r_difficulty rescale_factor = 0.2,
  trajectories_per_train_it=50, minibatch_size=125,
  max_actions_init_state = 30:
	>> Mejores resultados que en la ejecución anterior: la r_continuous converge a -0.02, r_eventual a -0.05,
	   y r_difficulty (goal policy y con rescale_factor=0.2) a 1.15.
	   La entropía de la goal policy disminuye mucho. La entropía de la init policy también es menor que en el experimento
	   anterior.
	   Los problemas generados ahora, al eliminar la lifted_action_entropy respecto al ejemplo anterior,
	   son muy difíciles pero pocos diversos. << Al aumentar la dificultad de los problemas baja su diversidad y viceversa! >>

	   # --- Resultados problem generation (model folder = both_policies_37)
		Its:
		- 80: avg. diff = 2322, pero los problemas son muy poco diversos!


> init_policy_entropy_coeffs = 0 1, entropy_annealing_coeffs_init_state_policy = None, 
  init_policy_entropy_coeffs = 0 1, entropy_annealing_coeffs_goal_policy = None,
  r_difficulty rescale_factor = 0.2, trajectories_per_train_it=50, minibatch_size=125,
  max_actions_init_state = 30, 
  <entropy bug arreglado (ahora la entropía se escala por el log del número de acciones (y no por el num de acciones))>:
	r_continuous converge a -0.3, r_eventual a -0.05 y r_difficulty (goal policy con rescale_factor=0.2) a 0.7.
	La entropía de la init and goal policies es muy alta!

	# --- Resultados problem generation (model folder = both_policies_38)
	# Los problemas generados son muy diversos pero poco difíciles (creo que la entropía de las generative policies
	# es demasiado alta)


> <init_policy_entropy_coeffs = 0 0.5>, entropy_annealing_coeffs_init_state_policy = None, 
  <init_policy_entropy_coeffs = 0 0.5>, entropy_annealing_coeffs_goal_policy = None,
  r_difficulty rescale_factor = 0.2, trajectories_per_train_it=50, minibatch_size=125,
  max_actions_init_state = 30:
	r_eventual converge a 0, r_continuous primero llega a -0.15 y después converge a -0.65
	y r_difficulty (goal_policy, con rescale_factor=0.2) llega a 1.2.
	Tanto la init como goal policy tienen mucha entropía.
	No obstante, tarda mucho en entrenar (9h en llegar hasta este punto).

  	# --- Resultados problem generation (model folder = both_policies_39)
	# >>>> ES CAPAZ DE GENERAR PROBLEMAS MUY DIFÍCILES Y DIVERSOS!!!
	# - 200 its -> avg. diff = 1890.2, problemas muy diversos!


> init_policy_entropy_coeffs = 0 0.5, entropy_annealing_coeffs_init_state_policy = None, 
  init_policy_entropy_coeffs = 0 0.5, entropy_annealing_coeffs_goal_policy = None,
  r_difficulty rescale_factor = 0.2, <trajectories_per_train_it=20, minibatch_size=100>,
  max_actions_init_state = 30:
	Respecto a la ejecución anterior (usando trajectories_per_train_it=50), la r_eventual
	y r_continuous son parecidas, pero la r_difficulty es peor (es más inestable).
	Además, la entropía de la init policy es peor (es menor) y la de la goal policy es igual.
	>>> Por tanto, las generative policies aprenden con trajectories_per_train_it=20 en vez
	    de 50, pero el resultado es un poco peor (y de 8h de entrenamiento pasamos solo a 
		6h y media, con lo que no hay mucha diferencia). --> Usamos trajectories_per_train_it=50, minibatch_size=125!


> init_policy_entropy_coeffs = 0 0.5, entropy_annealing_coeffs_init_state_policy = None, 
  init_policy_entropy_coeffs = 0 0.5, entropy_annealing_coeffs_goal_policy = None,
  r_difficulty rescale_factor = 0.2, <trajectories_per_train_it=50, minibatch_size=125>,
  max_actions_init_state = 30, <lr_initial_state_nlm=1e-3, lr_goal_nlm=1e-3>:
	Los resultados empeoran! (Al aumentar el learning rate, de hecho tarda más en aprender.)

> init_policy_entropy_coeffs = 0 0.5, entropy_annealing_coeffs_init_state_policy = None, 
  init_policy_entropy_coeffs = 0 0.5, entropy_annealing_coeffs_goal_policy = None,
  r_difficulty rescale_factor = 0.2, trajectories_per_train_it=50, minibatch_size=125,
  max_actions_init_state = 30, <lr_initial_state_nlm=5e-4, lr_goal_nlm=5e-4>,
  <epsilon=0.2>:
	Funciona peor que con epsilon=0.1 (la r_difficulty está a 0 durante más tiempo que
	con epsilon=0.1 -> entren más lento!)


-------- Pruebas problemas con max_atoms_init_state=20 y max_actions_goal_state=20


> init_policy_entropy_coeffs = 0 0.5, entropy_annealing_coeffs_init_state_policy = None, 
  init_policy_entropy_coeffs = 0 0.5, entropy_annealing_coeffs_goal_policy = None,
  r_difficulty rescale_factor = 0.2, trajectories_per_train_it=50, minibatch_size=125,
  max_actions_init_state = 30, lr_initial_state_nlm=5e-4, lr_goal_nlm=5e-4, <epsilon=0.1>,
  <max_atoms_init_state=20, max_actions_init_state=60, max_actions_goal_state=20>:
	La r_eventual no converge sino que diverge.

> init_policy_entropy_coeffs = 0 0.5, entropy_annealing_coeffs_init_state_policy = None, 
  init_policy_entropy_coeffs = 0 0.5, entropy_annealing_coeffs_goal_policy = None,
  r_difficulty rescale_factor = 0.2, trajectories_per_train_it=50, minibatch_size=125,
  max_actions_init_state = 30, lr_initial_state_nlm=5e-4, lr_goal_nlm=5e-4, epsilon=0.1,
  max_atoms_init_state=20, max_actions_init_state=60, max_actions_goal_state=20,
  <disc_factor_event_consistency=0.95>:
	No aprende, la r_eventual converge a -0.6 así que no es capaz de generar
	problemas consistentes en ningún momento. -> Creo que al haber aumentado
	el tamaño de las trayectorias, la probabilidad de que por "azar" genere
	un estado inicial consistente son muy bajas! -> QUIZÁS DEBERÍA EMPEZAR
	GENERANDO PROBLEMAS PEQUEÑOS E IR AUMENTANDO EL TAMAÑO POCO A POCO! (como un automated curriculum)


> <init_policy_entropy_coeffs = 0.5 0.5>, entropy_annealing_coeffs_init_state_policy = (300, 0.1, 0.1), 
  <init_policy_entropy_coeffs = 0.5 0.5>, entropy_annealing_coeffs_goal_policy = (300, 0.1, 0.1),
  r_difficulty rescale_factor = 0.2, trajectories_per_train_it=50, minibatch_size=125,
  max_actions_init_state = 30, lr_initial_state_nlm=5e-4, lr_goal_nlm=5e-4, epsilon=0.1,
  max_atoms_init_state=20, max_actions_init_state=60, max_actions_goal_state=20,
  <disc_factor_event_consistency=0.9>:
	Aprende pero tarda mucho en aprender! La r_continuous converge a 0 rápidamente pero la r_eventual
	tarda mucho (unas 10h de entrenamiento) en converger a 0!
	

> <init_policy_entropy_coeffs = 0 2>, entropy_annealing_coeffs_init_state_policy = None, 
  <init_policy_entropy_coeffs = 0 2>, entropy_annealing_coeffs_goal_policy = None,
  r_difficulty rescale_factor = 0.2, trajectories_per_train_it=50, minibatch_size=125,
  max_actions_init_state = 30, lr_initial_state_nlm=5e-4, lr_goal_nlm=5e-4, epsilon=0.1,
  max_atoms_init_state=20, max_actions_init_state=60, max_actions_goal_state=20,
  disc_factor_event_consistency=0.9:
	La r_eventual converge a 0 pero la r_continuous converge a -0.6 (parece que solo
	aprende las reglas de eventual consistency). La r_difficulty (de la goal_policy y
	rescale_factor=0.2) alcanza 0.7.
	La entropía de la init policy es muy alta y la de la goal_policy es tan alta como
	al principio del entrenamiento!!
	La termination condition probability también es muy alta (0.2 para la goal_policy
	y 0.05 para la init policy).
	Creo que hay que disminuir el entropy_reg_coeffs y usar un valor menor de entropía para
	la goal policy!

	# --- Resultados problem generation (model folder = both_policies_45)
	Los problemas generados son muy sencillos de resolver (excepto unos pocos que de vez en cuando
	salen muy difíciles).


> <init_policy_entropy_coeffs = 0 2>, <entropy_annealing_coeffs_init_state_policy = (300, 0, 0.1)>, 
  <init_policy_entropy_coeffs = 0 1>, <entropy_annealing_coeffs_goal_policy = (100, 0, 0.1)>,
  r_difficulty rescale_factor = 0.2, trajectories_per_train_it=50, minibatch_size=125,
  max_actions_init_state = 30, lr_initial_state_nlm=5e-4, lr_goal_nlm=5e-4, epsilon=0.1,
  max_atoms_init_state=20, max_actions_init_state=60, max_actions_goal_state=20,
  disc_factor_event_consistency=0.9:
	Aprende! La r_continuous y r_eventual convergen a 0 (aunque necesitan unas 7h de entrenamiento)
	y la r_difficulty (de la goal_policy con rescale_factor=0.2) converge a 2!
	No obstante, a partir de cierta iteración se tarda mucho en obtener las training trajectories, conforme aumenta
	la dificultad de los problemas generados. Creo que se debe a que al ser los problemas muy difíciles
	el planner tarda bastante en resolverlos (y por tanto más en obtenerse las trajectories).

	# --- Resultados problem generation (model folder = both_policies_47)
	> 270 its -> avg. diff = 2502358 y los problemas generados son muy diversos!!!
	No obstante, al ser los problemas tan difíciles se tardan mucho en resolver y generar las trayectorias
	de entrenamiento!!!!
	