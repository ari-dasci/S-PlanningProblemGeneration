-------------------------- TODO

>> [Opcional] Ver si resuelvo issue github lifted-pddl

>> Ver si añado support for domain constants

>> Experimentos sokoban
   - Ver si usar, además del random instance generator, otro generador que optimice la dificultad
     >>> ver fichero Sokoban instance generators.txt en carpeta experiments\instance_generators\analysis of existing sokoban generators
      - Usar método de "Data-Driven Sokoban Puzzle Generation with Monte Carlo Tree Search" 

>> Experimentos blocksworld

   -Debería probar más hiperparámetros distintos en blocksworld
    Realicé pruebas en logistics y después usé los mismos hiperparámetros en blocksworld!!


>> Experimentos logistics
   
   - Ver si puedo generar problemas con varias ciudades en logistics sin tener que fijar <min_cities=2>

   > Mejorar generalización y diversidad logistics - Varias alternativas

      >>> Objetivo actual: hacer que se generen problemas con locations y donde la goal_policy
          use tanto la acción fly como drive (para así generar problemas diversos y posiblemente
          generalizar mejor). También se deberían generar problemas con más de dos ciudades (para que
          escale conforme aumentamos el tamaño de problema)

      - Probar distintos valores de init y goal_policy entropy
         - También aumentar la entropía de la goal_policy para ver si así no "converge prematuramente"
           y aprende a usar tanto la acción drive como fly

      - Entrenar sobre problemas más grandes (20 átomos)
         - Quizás se generan problemas sin locations porque, para problemas pequeños, esos son los más
           difíciles

      - Probar a normalizar num_atoms y num_objs cada tipo dados como input a la NLM por el número
        actual de átomos en vez del número máximo

      - Añadir reduce operation a la NLM (suma de proporciones) para que aprenda a contar

      - Aumentar la complejidad de la NLM

      - Diversity reward
         - Creo que la action entropy no es una buena forma de motivar la diversidad (se pueden generar
           los mismos problemas añadiendo los átomos en un orden distinto)
         - Probar distintos coeficientes
         - Tener también en cuenta el num de atoms instanciados en cada tipo de obj
         - Probar también a tener en cuenta el goal del problema (comparar los init states y goal states
           de los problemas)

      - Entrenar en problemas de distintos tamaños
         - Normalizar dificultad de manera separada para cada tamaño de problema

      >> El número de objetos generados con action_entropy_coeffs=0.1 y 1 son muy parecidos!!!
         - Esto parece indicar que esta forma de generar problemas es la que maximiza la dificultad!
            - Si esto es el caso, quizás esté haciendo overfitting a un tamaño de problemas ->
              debería entrenar en problemas de distintos tamaños, para que aprenda a generalizar

         - Opción 2: quizás el problema es la goal policy (al no tener action_entropy_coeff, converge demasiado
           pronto)
            - Debería hacer experimentos usando goal_policy entropy_coeff

         - Opción 3: quizás la NLM no puede aprender "patrones" para generar problemas más complejos
            - Aumentar el número de capas/predicados
            - Añadir counting operations en reduce para que aprenda a contar

>> Consistency language
   
   - Mirar pyprover y ver si lo uso
      - Creo que sí funciona!! (tener cuidado de usar & en vez de "and" y los predicados ponerlos en mayúsculas, mientras que las constantes y variables en minúsculas)
      - Creo que tiene bugs y a veces va muy lento (mirar issue en github)

      - Si creo yo el código, creo que no debería crear un repositorio -> no serviría para hacer logic programming sino solo para evaluar una FOL formula
        dada una PDDL knowledge base, compuesta simplemente de átomos (pero no de fórmulas más complejas)


>> Diversity measurements

   >>> No usar características de Cenamor et al. (son menos características que las que yo ya venía usando)

   - Ver si uso las planning features de Improved Features for Runtime Prediction... o, si los resultados
     no son muy buenos (ej.: la mayoría de features no se pueden extraer para blocksworld), usar las features
     que uso para diversity reward (que además son fáciles de entender)

-------------------------- Analysis

>>> No se generan de manera "natural" problemas con locations. Dos posibilidades:
   - Los problemas más difíciles para tamaños pequeños de problema (<20 átomos)
     no tienen locations
      - Solución: usar diversity reward ("obligar" a la NLM a generar problemas con
                  locations aunque no sean tan difíciles, para que así generalice mejor)

   - La NLM no es suficientemente expresiva para aprender a generar problemas difíciles
     usando locations
      - Solución: aumentar NLM expressivity con counting quantifiers y más predicados

C:\Users\Usuario\Desktop\NeSIG\S-PlanningProblemGeneration\src\problem_generation\controller\directed_generator.py

>> Creo que la dificultad escala mal porque NeSIG no aprende a generar problemas con locations
   y donde la goal_policy use acciones drive. Solución:
      1. Generar problemas más diversos aún, para que hasta los problemas pequeños (15 atoms)
        tengan un número alto de locations -> cambiar diversity reward, usar policy_entropy
      2. Hacer que la goal_policy aprenda a usar acciones drive -> usar goal_policy_entropy
         (quizás empezar en un valor alto y que se haga annealing hasta 0) 


>>> Debería hacer un experimento con goal_policy entropy (y quizás init), bajándola poco a poco
    hasta 0


---------------------- Next experiments

>>> disc_factor_difficulty=1, solo LAMA-first durante el entrenamiento
   - En los últimos experimentos, ambos resultados son muy parecidos.



- Uso max_actions_goal_state=5 porque creo que debería ser más que de sobra para generar goals
  difíciles!!!
   - Si asumimos que, en el peor de los casos, la goal_policy necesita 12 acciones para mover
     cada paquete de un sitio a otro y que el número de paquetes de un problema es aproximadamente
     la mitad del número de max_atoms, entonces la goal_policy necesita 6-7 veces max-atoms acciones
     para mover los paquetes, en el peor de los casos (por ejemplo, si se usa un avión para
     cargar primero todos los paquetes y después descargar todos, entonces se necesitan menos acciones)

- Diversity reward
   - Probar diversity_rescale_factor=100


- Policy entropy (además de diversity reward)
   - Empezar con un policy entropy alto para la init y goal policies e ir reduciéndolo a 0
     Ej.: entropy_coeff_init_state_policy = 0.5, entropy_annealing_coeffs_init_state_policy = (5000, 0)

- Aumentar expresividad NLM
   - Añadir reduce operation for counting
   - Aumentar número de predicados a 16 en cada capa

- Entrenar sobre problemas de varios tamaños


>>> Mejorar eficiencia
   - Usar NLM más sencilla (menos capas y menos predicados)
   - Probar a reducir el número de trayectorias per train it (quizás entonces debería calcular el diversity
     reward utilizando también los problemas de las X últimas trayectorias, para reducir su varianza)

   - Future work: usar otras NNs en vez de las NLMs (ej.: logical neural networks (LNNs))