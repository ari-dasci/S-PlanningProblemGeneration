-------------------------- TODO

>> [Opcional] Ver si resuelvo issue github lifted-pddl

>> Ver si añado support for domain constants

>> Experimentos sokoban
   - Ver si usar, además del random instance generator, otro generador que optimice la dificultad
     >>> ver fichero Sokoban instance generators.txt en carpeta experiments\instance_generators\analysis of existing sokoban generators
      - Usar método de "Data-Driven Sokoban Puzzle Generation with Monte Carlo Tree Search" 

>> Experimentos blocksworld

   -Debería probar más hiperparámetros distintos en blocksworld
    Realicé pruebas en logistics y después usé los mismos hiperparámetros en blocksworld!!


>> Experimentos logistics
   
   - Ver si puedo generar problemas con varias ciudades en logistics sin tener que fijar <min_cities=2>

   > Mejorar generalización y diversidad logistics - Varias alternativas

      >>> Objetivo actual: hacer que se generen problemas con locations y donde la goal_policy
          use tanto la acción fly como drive (para así generar problemas diversos y posiblemente
          generalizar mejor). También se deberían generar problemas con más de dos ciudades (para que
          escale conforme aumentamos el tamaño de problema)

      - Probar distintos valores de init y goal_policy entropy
         - También aumentar la entropía de la goal_policy para ver si así no "converge prematuramente"
           y aprende a usar tanto la acción drive como fly

      - Entrenar sobre problemas más grandes (20 átomos)
         - Quizás se generan problemas sin locations porque, para problemas pequeños, esos son los más
           difíciles

      - Probar a normalizar num_atoms y num_objs cada tipo dados como input a la NLM por el número
        actual de átomos en vez del número máximo

      - Añadir reduce operation a la NLM (suma de proporciones) para que aprenda a contar

      - Aumentar la complejidad de la NLM

      - Diversity reward
         - Creo que la action entropy no es una buena forma de motivar la diversidad (se pueden generar
           los mismos problemas añadiendo los átomos en un orden distinto)
         - Probar distintos coeficientes
         - Tener también en cuenta el num de atoms instanciados en cada tipo de obj
         - Probar también a tener en cuenta el goal del problema (comparar los init states y goal states
           de los problemas)

      - Entrenar en problemas de distintos tamaños
         - Normalizar dificultad de manera separada para cada tamaño de problema

      >> El número de objetos generados con action_entropy_coeffs=0.1 y 1 son muy parecidos!!!
         - Esto parece indicar que esta forma de generar problemas es la que maximiza la dificultad!
            - Si esto es el caso, quizás esté haciendo overfitting a un tamaño de problemas ->
              debería entrenar en problemas de distintos tamaños, para que aprenda a generalizar

         - Opción 2: quizás el problema es la goal policy (al no tener action_entropy_coeff, converge demasiado
           pronto)
            - Debería hacer experimentos usando goal_policy entropy_coeff

         - Opción 3: quizás la NLM no puede aprender "patrones" para generar problemas más complejos
            - Aumentar el número de capas/predicados
            - Añadir counting operations en reduce para que aprenda a contar

>> Consistency language
   
   - Mirar pyprover y ver si lo uso
      - Creo que sí funciona!! (tener cuidado de usar & en vez de "and" y los predicados ponerlos en mayúsculas, mientras que las constantes y variables en minúsculas)
      - Creo que tiene bugs y a veces va muy lento (mirar issue en github)

      - Si creo yo el código, creo que no debería crear un repositorio -> no serviría para hacer logic programming sino solo para evaluar una FOL formula
        dada una PDDL knowledge base, compuesta simplemente de átomos (pero no de fórmulas más complejas)


>> Diversity measurements

   >>> No usar características de Cenamor et al. (son menos características que las que yo ya venía usando)

   - Ver si uso las planning features de Improved Features for Runtime Prediction... o, si los resultados
     no son muy buenos (ej.: la mayoría de features no se pueden extraer para blocksworld), usar las features
     que uso para diversity reward (que además son fáciles de entender)

-------------------------- Analysis

>>> Las NLMs son independientes del orden y número de objetos. No obstante creo que:
   - Para aprender reglas que no dependan del número de objetos, deberían ser entrenados en problemas con distinto
     num de objetos (problemas de distintos tamaños)
   - Deberían entrenarse en problemas con un número suficientemente alto de objetos para que surjan reglas "complejas",
     que relacionen un número alto de objetos (ej.: 3) entre sí
   - Puede que en este caso, las "estrategia de generación" cambie según el tamaño del problema. Por ejemplo, que 
     para problemas pequeños, para hacerlos difíciles haya que generar problemas sin locations pero, para problemas grandes,
     sea necesario generar problemas con locations. Esto puede limitar la generalización a problemas de tamaño muy superior


   >>> En definitiva, si no aprende la NLM con los experimentos de abajo hacer lo siguiente:
      - Entrenar en problemas de distinto max_atoms, desde 10 hasta 20 o 25




>>> No se generan de manera "natural" problemas con locations. Dos posibilidades:
   - Los problemas más difíciles para tamaños pequeños de problema (<20 átomos)
     no tienen locations
      - Solución: usar diversity reward ("obligar" a la NLM a generar problemas con
                  locations aunque no sean tan difíciles, para que así generalice mejor)

   - La NLM no es suficientemente expresiva para aprender a generar problemas difíciles
     usando locations
      - Solución: aumentar NLM expressivity con counting quantifiers y más predicados

C:\Users\Usuario\Desktop\NeSIG\S-PlanningProblemGeneration\src\problem_generation\controller\directed_generator.py

>> Creo que la dificultad escala mal porque NeSIG no aprende a generar problemas con locations
   y donde la goal_policy use acciones drive. Solución:
      1. Generar problemas más diversos aún, para que hasta los problemas pequeños (15 atoms)
        tengan un número alto de locations -> cambiar diversity reward, usar policy_entropy
      2. Hacer que la goal_policy aprenda a usar acciones drive -> usar goal_policy_entropy
         (quizás empezar en un valor alto y que se haga annealing hasta 0) 


>>> Debería hacer un experimento con goal_policy entropy (y quizás init), bajándola poco a poco
    hasta 0


---------------------- Next experiments

>>> disc_factor_difficulty=1, solo LAMA-first durante el entrenamiento
   - En los últimos experimentos, ambos resultados son muy parecidos.


>>> Si estos dos últimos experimentos no salen bien probar a:
   - Usar diversity_reward=100 (o más alto) y policy entropy
   - Usar policy entropy más alto para la init policy que para la goal policy (o usar entropy_coeffs=0, None para la goal policy)
     (tiene más sentido que la init policy tenga mucha entropía, para generar muchos estados
      iniciales distintos, pero que la goal policy tenga menos entropía ya que tiene que saber
      "resolver" cada uno de ellos)
      La idea sería hacer que la init_policy sea muy "random" mientras que la goal_policy sea poco "random", de manera parecida
      a la ablation donde solo se entrena a la goal_policy

- Uso max_actions_goal_state=5 porque creo que debería ser más que de sobra para generar goals
  difíciles!!!
   - Si asumimos que, en el peor de los casos, la goal_policy necesita 12 acciones para mover
     cada paquete de un sitio a otro y que el número de paquetes de un problema es aproximadamente
     la mitad del número de max_atoms, entonces la goal_policy necesita 6-7 veces max-atoms acciones
     para mover los paquetes, en el peor de los casos (por ejemplo, si se usa un avión para
     cargar primero todos los paquetes y después descargar todos, entonces se necesitan menos acciones)


- Aumentar expresividad NLM
   - Añadir reduce operation for counting
   - Aumentar número de predicados a 16 en cada capa

- Entrenar sobre problemas de varios tamaños


>>> Mejorar eficiencia
   - Usar NLM más sencilla (menos capas y menos predicados)
   - Probar a reducir el número de trayectorias per train it (quizás entonces debería calcular el diversity
     reward utilizando también los problemas de las X últimas trayectorias, para reducir su varianza)

   - Future work: usar otras NNs en vez de las NLMs (ej.: logical neural networks (LNNs))