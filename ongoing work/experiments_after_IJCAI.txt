>  <logistics>
   <planners=[lama-first, lazy-greedy, lazy-greedy]>
   max_atoms_init_state=15, max_actions_init_state=1, max_actions_goal_state=2.0

   > logs: init_policy\version_204
   > saved_models: both_policies_257

   > Entrenamiento (its=1.5k)
      - Tiempo: 16h (en DaSCI)
      - r_diff (init_policy)= 3 (seguía aumentando, lo paré a mitad)
      - r_eventual=-0.05
      - init_policy_entropy 0.6, goal_policy_entropy en 0.3 (seguía bajando)
      - term_cond_prob init_policy y goal_policy ambas convergen a 0
      - num_objs:
         - city: 2.8 (y subiendo)
         - airport: 3
         - location: 0.2 (y bajando)
         - airplane: 2.3
         - truck: 3
         - package: 6.3


   > Problemas (its=1.5)
      - max_atoms = 20
         - diff = [68.2 52.9 58.1]
            - problemas con 20 átomos
         - diversidad media-alta
            - la mayoría de problemas no tienen locations
            - problemas con 2, 3, 4 y 5 ciudades
            - el número de paquetes varía moderadamente

      - max_atoms = 30
         - diff = [145.6 96.1 87.4]
            - problemas con 30 átomos
         - diversidad media
            - no aumenta el número de ciudades!!! (siguen estando entre 2 y 5)
            - aumenta sobretodo el número de packages

      - max_atoms = 40
         - diff = [233.2 154.6 153.4]
            - problemas con 40 átomos
         - diversidad media
            - se generan problemas con hasta 7 ciudades
            - aumenta sobretodo el número de packages 

   >> Aunque no terminé el entrenamiento, se puede ver que NeSIG no generaliza bien en logistics a problemas más
      grandes:
         - La dificultad escala mal
         - El número de ciudades escala mal


>  <logistics>
   planners=[lama-first, lazy-greedy, lazy-greedy]
   max_atoms_init_state=15, max_actions_init_state=1, max_actions_goal_state=2.0
   init_policy_entropy_coeffs: 0.1, None
   goal_policy_entropy_coeffs: 0.0, None
   <difficulty_rescale_factor=1>
   <new difficulty normalization>

   > Ejecutándose en DaSCI
   > logs: init_policy/version_206
   > saved_models: both_policies_259

   > Entrenamiento (its=5900)
      - Tiempo (DaSCI): 3d 20h
      - r_diff (goal_policy) = 60
      - r_eventual=-0.01
      - init_policy_entropy 0.6, goal_policy_entropy 0.1
      - term_cond_prob init_policy y goal_policy ambas convergen a 0
      - num_objs:
         - city: 2.1
         - airport: 5.5
         - location: 0.005
         - airplane: 1
         - truck: 2.1 (igual que city)
         - package: 6.2

         - Resumen:
            - Se generan problemas con 2 ciudades, un avión, muchos airports,
              sin locations, un camión por cada ciudad y muchos packages

   > Problemas (its=5000)
      - max_atoms = 15
         - diff = [63.9 53.3 59.9] (muy alta)
            - todos con 15 átomos
         - diversidad baja!!
            - Problemas con 2 y 3 ciudades
            - Entre distintos problemas solo suele variar el número de airports y packages (y esto solo varía un poco)

      - max_atoms = 20
         - diff = [125. 98.6 116.4] (muy alta)
         - Problemas con 2 y 3 ciudades

      - max_atoms = 30  
         - diff = [257.6 221.2 244. ] (medio-alta)
         - Problemas con 3, 4 y 5 ciudades

      - max_atoms = 40
         - diff = [450.7 388.8 454.7] (media)
         - Problemas con hasta 7 ciudades (aunque la media son 5 ciudades)
         - muy parecidos entre sí

   >> Análisis de resultados
      > El número de ciudades sí escala medio-bien pero, a pesar de esto, la dificultad escala mal!!
         <<<Al no haber locations, se pueden llevar los paquetes de un sitio a otro usando solo aviones directamente!! (la goal policy solo usa la acción fly, pero no drive (con trucks))>>>
         - Si quiero que la dificultad escale mejor, creo que tiene que aprender a generar problemas
           donde se usen tanto los aviones como los camiones para llevar paquetes 

            >>> Dos opciones:
               - Entrenar en problemas más grandes
               - Probar a aumentar la entropía de la goal_policy

      > A pesar de usar action_entropy_coeffs=1, los problemas son poco diversos!
         - Creo que la action_entropy no es una buena forma de motivar la diversidad:
           dado que el mismo problema se puede generar de distintas formas (añadiendo los átomos
           en distinto orden), una alta entropía puede resultar en problemas con poca diversidad!!
         
            >>> Debería usar diversity reward!!!


>  <logistics>
   planners=[lama-first, lazy-greedy, lazy-greedy]
   max_atoms_init_state=15, max_actions_init_state=1, max_actions_goal_state=2.0
   <init_policy_entropy_coeffs: 0.01, None>
   goal_policy_entropy_coeffs: 0.0, None
   <difficulty_rescale_factor=1>
   <new difficulty normalization>

   > Ejecutándose en casa
   > logs: init_policy/version_203
   > saved_models: both_policies_260

   > Entrenamiento (its=1800)
      - Tiempo (casa): 14h 15min
      - r_diff (goal_policy)= 40 (seguía aumentando)
      - r_eventual=-0.02
      - init_policy_entropy 0.4, goal_policy_entropy 0.2
      - term_cond_prob init_policy y goal_policy ambas convergen a 0
      - num_objs:
         - city: 3
         - airport: 3
         - location: 0.01
         - airplane: 2.1 (y bajando)
         - truck: 3
         - package: 7 
 
   > Problemas (its=1800)
      - max_atoms = 15
         - diff = [51.9 42.3 39.4]
            - todos con 15 átomos
         - diversidad baja
            - Todos los problemas con 3 ciudades
            - Ningún problema con location!
            - Cada ciudad tiene exactamente un airport!!!
              Se generan problemas donde solo se usan aviones,
              pero no camiones!!

      - max_atoms = 20
         - diff = [90.4 73.6 67.]
            - todos con 20 átomos
         - diversidad media-baja
            - Problemas con 3, 4 y 5 ciudades!

      - max_atoms = 30
         - diff = [202.9 155.4 129.3]
            - todos con 30 átomos
         - diversidad media-baja
            - Problemas con hasta 7 ciudades!

      - max_atoms = 40
         - diff = [455.1 306.6 209.1]
            - todos con 40 átomos
         - diversidad media-baja
            - Problemas con hasta 8 ciudades
               - Parece que el num de ciudades ya deja de escalar bien, lo que más aumenta
                 es el número de paquetes

   >> La dificultad escala regular
   >> La diversidad de los problemas es muy baja!
      >>> Necesito aumentar el action entropy coeff
      >>> 0.01 es demasiado bajo!
   >> Quizás necesito entrenar en problemas más grandes
         - Lo que hace que un problema sea difícil puede depender del tamaño!


>  <logistics>
   planners=[lama-first, lazy-greedy, lazy-greedy]
   max_atoms_init_state=15, max_actions_init_state=1, max_actions_goal_state=2.0
   <init_policy_entropy_coeffs: 1, None>
   goal_policy_entropy_coeffs: 0.0, None
   difficulty_rescale_factor=1
   new difficulty normalization

   > Ejecutándose en casa
   > logs: init_policy/version_204
   > saved_models: both_policies_261

   > Entrenamiento (its=9000)
      - Tiempo (casa): 3d
      - r_diff (goal_policy)= 42
      - r_eventual=-0.02
      - init_policy_entropy=0.85, goal_policy_entropy=0.1
      - term_cond_prob init_policy converge a 0 y la goal_policy_entropy a 0.01
      - num_objs:
         - city: 2
         - airport: 5.5
         - location: 1.3
         - airplane: 1.1
         - truck: 2.4
         - package: 4.7   

      >> El número de objetos es muy parecido
         a cuando se usa init_policy_entropy_coeffs=0.1!!
         La mayor diferencia es que ahora sí se generan
         locations

   > Problemas (its=7500)
      - max_atoms = 15
         - diff = [45.4 39.3 38.9] (alta)
            - todos con 15 átomos
         - diversidad media-alta
            - Todos los problemas con dos ciudades
            - El resto de objetos muy diversos

      - max_atoms = 20
         - diff = [75.4 61.1 72.1] (alta)
         - diversidad media-alta
            - Todos los problemas con dos ciudades
            - El resto de objetos muy diversos

      - max_atoms = 30
         - diff = [171.3 132.4 160.1] (media)
         - diversidad media
            - Todos los problemas con dos ciudades
            - El resto de objetos muy diversos

      - max_atoms = 40
         - diff = [373.3 270.5 353.3] (media-baja)
         - diversidad media
            - Todos los problemas con dos ciudades
            - El resto de objetos muy diversos

   >> Se usa drive para generar los objetivos!!!

   >> La dificultad de los problemas y generalización es peor
      que al usar init_policy_action_entropy=0.1
   >> Ahora, al aumentar el tamaño de problema, no aumenta
      el número de ciudades

   >> init_policy action_coeff es mejor 0.1 que 1!!!
      - No obstante, creo que no es un buen acercamiento para generar problemas diversos
         - Debería usar diversity reward!!!


---- Experimentos para mejorar generalización

>  <logistics>
   planners=[lama-first, lazy-greedy, lazy-greedy]
   max_atoms_init_state=15, max_actions_init_state=1, max_actions_goal_state=2.0
   <init_policy_entropy_coeffs: 0.1, None>
   <goal_policy_entropy_coeffs: 0.1, None>
   difficulty_rescale_factor=1
   new difficulty normalization

   > Ejecutándose en DaSCI
   > logs: init_policy/version_207
   > saved_models: both_policies_262

   > Paré el entrenamiento a mitad, tras 1.6k train its
      - La init y goal policies entropy caen muy rápido!!
      - Se generan problemas con varias ciudades, pero donde cada ciudad
        suele tener un único camión y airport, sin locations! -> Poca diversidad!!


>  <logistics>
   planners=[lama-first, lazy-greedy, lazy-greedy]
   <max_atoms_init_state=20>, max_actions_init_state=1, max_actions_goal_state=2.0
   <init_policy_entropy_coeffs: 0.1, None>
   goal_policy_entropy_coeffs: 0, None
   difficulty_rescale_factor=1
   new difficulty normalization

   > Ejecutándose en casa
   > logs: init_policy/version_205
   > saved_models: both_policies_263


>  <logistics>
   planners=[lama-first, lazy-greedy, lazy-greedy]
   max_atoms_init_state=15, max_actions_init_state=1, max_actions_goal_state=2.0
   <init_policy_entropy_coeffs: 0.2, None>
   <goal_policy_entropy_coeffs: 0.2, None>
   difficulty_rescale_factor=1
   new difficulty normalization

   > Ejecutándose en DaSCI
   > logs: init_policy/version_208
   > saved_models: both_policies_264







------- MIRAR ESTO

   >> Debería intentar que se generen problemas con locations y que la goal policy
      utilice la acción "drive"!!!
      - Entrenar en problemas más grandes (quizás en problemas con 15 átomos
        los problemas son más difíciles si no se usa "drive" ni hay locations)
      - Entrenar en problemas de distintos tamaños
      - Aumentar la entropía de la goal policy

   >> Creo que la action entropy no es una buena forma de motivar la diversidad (policies
      con alta entropía pueden dar lugar a problemas parecidos, simplemente añadiendo los átomos
      en un orden distinto)
      - Debería usar diversity reward

   >> El número de objetos generados con action_entropy_coeffs=0.1 y 1 son muy parecidos!!!
      - Esto parece indicar que esta forma de generar problemas es la que maximiza la dificultad!
         - Si esto es el caso, quizás esté haciendo overfitting a un tamaño de problemas ->
           debería entrenar en problemas de distintos tamaños, para que aprenda a generalizar

      - Opción 2: quizás el problema es la goal policy (al no tener action_entropy_coeff, converge demasiado
        pronto)
         - Debería hacer experimentos usando goal_policy entropy_coeff

      - Opción 3: quizás la NLM no puede aprender "patrones" para generar problemas más complejos
         - Aumentar el número de capas/predicados
         - Añadir counting operations en reduce para que aprenda a contar

   > Si quiero generar problemas con más de dos ciudades debo:
      - Promover la diversidad de otra forma (ej.: diversity reward)

   >> Si veo que la goal_policy entropy es muy baja, quizás debería usar un goal_policy entropy coeff > 0
