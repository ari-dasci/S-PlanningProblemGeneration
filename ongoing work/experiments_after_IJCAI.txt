>  <logistics>
   <planners=[lama-first, lazy-greedy, lazy-greedy]>
   max_atoms_init_state=15, max_actions_init_state=1, max_actions_goal_state=2.0

   > logs: init_policy\version_204
   > saved_models: both_policies_257

   > Entrenamiento (its=1.5k)
      - Tiempo: 16h (en DaSCI)
      - r_diff (init_policy)= 3 (seguía aumentando, lo paré a mitad)
      - r_eventual=-0.05
      - init_policy_entropy 0.6, goal_policy_entropy en 0.3 (seguía bajando)
      - term_cond_prob init_policy y goal_policy ambas convergen a 0
      - num_objs:
         - city: 2.8 (y subiendo)
         - airport: 3
         - location: 0.2 (y bajando)
         - airplane: 2.3
         - truck: 3
         - package: 6.3


   > Problemas (its=1.5)
      - max_atoms = 20
         - diff = [68.2 52.9 58.1]
            - problemas con 20 átomos
         - diversidad media-alta
            - la mayoría de problemas no tienen locations
            - problemas con 2, 3, 4 y 5 ciudades
            - el número de paquetes varía moderadamente

      - max_atoms = 30
         - diff = [145.6 96.1 87.4]
            - problemas con 30 átomos
         - diversidad media
            - no aumenta el número de ciudades!!! (siguen estando entre 2 y 5)
            - aumenta sobretodo el número de packages

      - max_atoms = 40
         - diff = [233.2 154.6 153.4]
            - problemas con 40 átomos
         - diversidad media
            - se generan problemas con hasta 7 ciudades
            - aumenta sobretodo el número de packages 

   >> Aunque no terminé el entrenamiento, se puede ver que NeSIG no generaliza bien en logistics a problemas más
      grandes:
         - La dificultad escala mal
         - El número de ciudades escala mal


>  <logistics>
   planners=[lama-first, lazy-greedy, lazy-greedy]
   max_atoms_init_state=15, max_actions_init_state=1, max_actions_goal_state=2.0
   init_policy_entropy_coeffs: 0.1, None
   goal_policy_entropy_coeffs: 0.0, None
   <difficulty_rescale_factor=1>
   <new difficulty normalization>

   > Ejecutándose en DaSCI
   > logs: init_policy/version_206
   > saved_models: both_policies_259

   > Entrenamiento (its=5900)
      - Tiempo (DaSCI): 3d 20h
      - r_diff (goal_policy) = 60
      - r_eventual=-0.01
      - init_policy_entropy 0.6, goal_policy_entropy 0.1
      - term_cond_prob init_policy y goal_policy ambas convergen a 0
      - num_objs:
         - city: 2.1
         - airport: 5.5
         - location: 0.005
         - airplane: 1
         - truck: 2.1 (igual que city)
         - package: 6.2

         - Resumen:
            - Se generan problemas con 2 ciudades, un avión, muchos airports,
              sin locations, un camión por cada ciudad y muchos packages

   > Problemas (its=5000)
      - max_atoms = 15
         - diff = [63.9 53.3 59.9] (muy alta)
            - todos con 15 átomos
         - diversidad baja!!
            - Problemas con 2 y 3 ciudades
            - Entre distintos problemas solo suele variar el número de airports y packages (y esto solo varía un poco)

      - max_atoms = 20
         - diff = [125. 98.6 116.4] (muy alta)
         - Problemas con 2 y 3 ciudades

      - max_atoms = 30  
         - diff = [257.6 221.2 244. ] (medio-alta)
         - Problemas con 3, 4 y 5 ciudades

      - max_atoms = 40
         - diff = [450.7 388.8 454.7] (media)
         - Problemas con hasta 7 ciudades (aunque la media son 5 ciudades)
         - muy parecidos entre sí

   >> Análisis de resultados
      > El número de ciudades sí escala medio-bien pero, a pesar de esto, la dificultad escala mal!!
         <<<Al no haber locations, se pueden llevar los paquetes de un sitio a otro usando solo aviones directamente!! (la goal policy solo usa la acción fly, pero no drive (con trucks))>>>
         - Si quiero que la dificultad escale mejor, creo que tiene que aprender a generar problemas
           donde se usen tanto los aviones como los camiones para llevar paquetes 

            >>> Dos opciones:
               - Entrenar en problemas más grandes
               - Probar a aumentar la entropía de la goal_policy

      > A pesar de usar action_entropy_coeffs=1, los problemas son poco diversos!
         - Creo que la action_entropy no es una buena forma de motivar la diversidad:
           dado que el mismo problema se puede generar de distintas formas (añadiendo los átomos
           en distinto orden), una alta entropía puede resultar en problemas con poca diversidad!!
         
            >>> Debería usar diversity reward!!!


>  <logistics>
   planners=[lama-first, lazy-greedy, lazy-greedy]
   max_atoms_init_state=15, max_actions_init_state=1, max_actions_goal_state=2.0
   <init_policy_entropy_coeffs: 0.01, None>
   goal_policy_entropy_coeffs: 0.0, None
   <difficulty_rescale_factor=1>
   <new difficulty normalization>

   > Ejecutándose en casa
   > logs: init_policy/version_203
   > saved_models: both_policies_260

   > Entrenamiento (its=1800)
      - Tiempo (casa): 14h 15min
      - r_diff (goal_policy)= 40 (seguía aumentando)
      - r_eventual=-0.02
      - init_policy_entropy 0.4, goal_policy_entropy 0.2
      - term_cond_prob init_policy y goal_policy ambas convergen a 0
      - num_objs:
         - city: 3
         - airport: 3
         - location: 0.01
         - airplane: 2.1 (y bajando)
         - truck: 3
         - package: 7 
 
   > Problemas (its=1800)
      - max_atoms = 15
         - diff = [51.9 42.3 39.4]
            - todos con 15 átomos
         - diversidad baja
            - Todos los problemas con 3 ciudades
            - Ningún problema con location!
            - Cada ciudad tiene exactamente un airport!!!
              Se generan problemas donde solo se usan aviones,
              pero no camiones!!

      - max_atoms = 20
         - diff = [90.4 73.6 67.]
            - todos con 20 átomos
         - diversidad media-baja
            - Problemas con 3, 4 y 5 ciudades!

      - max_atoms = 30
         - diff = [202.9 155.4 129.3]
            - todos con 30 átomos
         - diversidad media-baja
            - Problemas con hasta 7 ciudades!

      - max_atoms = 40
         - diff = [455.1 306.6 209.1]
            - todos con 40 átomos
         - diversidad media-baja
            - Problemas con hasta 8 ciudades
               - Parece que el num de ciudades ya deja de escalar bien, lo que más aumenta
                 es el número de paquetes

   >> La dificultad escala regular
   >> La diversidad de los problemas es muy baja!
      >>> Necesito aumentar el action entropy coeff
      >>> 0.01 es demasiado bajo!
   >> Quizás necesito entrenar en problemas más grandes
         - Lo que hace que un problema sea difícil puede depender del tamaño!


>  <logistics>
   planners=[lama-first, lazy-greedy, lazy-greedy]
   max_atoms_init_state=15, max_actions_init_state=1, max_actions_goal_state=2.0
   <init_policy_entropy_coeffs: 1, None>
   goal_policy_entropy_coeffs: 0.0, None
   difficulty_rescale_factor=1
   new difficulty normalization

   > Ejecutándose en casa
   > logs: init_policy/version_204
   > saved_models: both_policies_261

   > Entrenamiento (its=9000)
      - Tiempo (casa): 3d
      - r_diff (goal_policy)= 42
      - r_eventual=-0.02
      - init_policy_entropy=0.85, goal_policy_entropy=0.1
      - term_cond_prob init_policy converge a 0 y la goal_policy_entropy a 0.01
      - num_objs:
         - city: 2
         - airport: 5.5
         - location: 1.3
         - airplane: 1.1
         - truck: 2.4
         - package: 4.7   

      >> El número de objetos es muy parecido
         a cuando se usa init_policy_entropy_coeffs=0.1!!
         La mayor diferencia es que ahora sí se generan
         locations

   > Problemas (its=7500)
      - max_atoms = 15
         - diff = [45.4 39.3 38.9] (alta)
            - todos con 15 átomos
         - diversidad media-alta
            - Todos los problemas con dos ciudades
            - El resto de objetos muy diversos

      - max_atoms = 20
         - diff = [75.4 61.1 72.1] (alta)
         - diversidad media-alta
            - Todos los problemas con dos ciudades
            - El resto de objetos muy diversos

      - max_atoms = 30
         - diff = [171.3 132.4 160.1] (media)
         - diversidad media
            - Todos los problemas con dos ciudades
            - El resto de objetos muy diversos

      - max_atoms = 40
         - diff = [373.3 270.5 353.3] (media-baja)
         - diversidad media
            - Todos los problemas con dos ciudades
            - El resto de objetos muy diversos

   >> Se usa drive para generar los objetivos!!!

   >> La dificultad de los problemas y generalización es peor
      que al usar init_policy_action_entropy=0.1
   >> Ahora, al aumentar el tamaño de problema, no aumenta
      el número de ciudades

   >> init_policy action_coeff es mejor 0.1 que 1!!!
      - No obstante, creo que no es un buen acercamiento para generar problemas diversos
         - Debería usar diversity reward!!!


---- Experimentos para mejorar generalización

>  <logistics>
   planners=[lama-first, lazy-greedy, lazy-greedy]
   max_atoms_init_state=15, max_actions_init_state=1, max_actions_goal_state=2.0
   <init_policy_entropy_coeffs: 0.1, None>
   <goal_policy_entropy_coeffs: 0.1, None>
   difficulty_rescale_factor=1
   new difficulty normalization

   > Ejecutándose en DaSCI
   > logs: init_policy/version_207
   > saved_models: both_policies_262

   > Paré el entrenamiento a mitad, tras 1.6k train its
      - La init y goal policies entropy caen muy rápido!!
      - Se generan problemas con varias ciudades, pero donde cada ciudad
        suele tener un único camión y airport, sin locations! -> Poca diversidad!!


>  <logistics>
   planners=[lama-first, lazy-greedy, lazy-greedy]
   <max_atoms_init_state=20>, max_actions_init_state=1, max_actions_goal_state=2.0
   <init_policy_entropy_coeffs: 0.1, None>
   goal_policy_entropy_coeffs: 0, None
   difficulty_rescale_factor=1
   new difficulty normalization
   <minibatch_size=50>

   > Ejecutándose en casa
   > logs: init_policy/version_205, version_206
   > saved_models: both_policies_263, 265

   > Entrenamiento (its=3400, paré el entrenamiento a mitad)
      - Tiempo (casa): 1d 15h
      - r_diff (goal_policy)= 85 (seguía aumentando)
      - r_eventual=-0.02
      - init_policy_entropy=0.7, goal_policy_entropy=0.1
      - term_cond_prob init_policy y goal_policy convergen a 0
      - num_objs:
         - city: 2.5 (bajando)
         - airport: 5.4
         - location: 0.1
         - airplane: 1
         - truck: 4.4
         - package: 8.8  

      >>> Básicamente los mismos objetos que entrenando en problemas de
          tamaño 15!!!!
          La principal diferencia es que hay más camiones (aunque si hubiera
          entrenado durante más probablemente habría disminuido) y que
          hay más paquetes
          Se generan problemas con pocas ciudades (para el tamaño) y
          sin locations!! 

   > Problemas (its=3400)
      - max_atoms = 15
         - diff = [53.9 42.9 46.6] (alta)
         - diversidad media-alta
            - Problemas con 2, 3 y 4 ciudades
            - Sin locations
            - Los num objs varían bastante

      - max_atoms = 20
         - diff = [95.8 77.4 87.9] (alta)
         - Problemas con 2 y 3 ciudades

      - max_atoms = 30
         - diff = [220.4 170.5 191.6] (media)
         - Problemas con 2, 3 y 4 ciudades

      - max_atoms = 40
         - diff = [389.4 298.7 341.] (media-baja)
         - Problemas con solo 2 y 3 ciudades!
         - Solo escala el número de paquetes

   >>> Los resultados no mejoran al entrenar en problemas de 20 átomos!!!
      > Se siguen generando problemas sin locations y pocas ciudades
         - Parece que el "tipo" de problema difícil es igual para 15 que
           20 átomos (con pocas ciudades y sin locations)
      > La generalización no mejora (de hecho empeora, pero creo que es por
        haber parado el entrenamiento a mitad)
         - La dificultad para problemas grandes es menor que al entrenar
           sobre 15 átomos
         - No escala el número de ciudades     

   << Debería entrenar sobre 15 átomos! >>


>  <logistics>
   planners=[lama-first, lazy-greedy, lazy-greedy]
   max_atoms_init_state=15, max_actions_init_state=1, max_actions_goal_state=2.0
   <init_policy_entropy_coeffs: 0.2, None>
   <goal_policy_entropy_coeffs: 0.2, None>
   difficulty_rescale_factor=1
   new difficulty normalization

   > Ejecutándose en DaSCI
   > logs: init_policy/version_208
   > saved_models: both_policies_264

   > Entrenamiento (its=2800, paré el entrenamiento a mitad)
      - Tiempo (casa): 1d 14h
      - r_diff (goal_policy)= 30
      - r_eventual=-0.02
      - init_policy_entropy=0.6, goal_policy_entropy=0.75
      - term_cond_prob init_policy y goal_policy convergen a 0
      - num_objs:
         - city: 3.6
         - airport: 3.7
         - location: 0.2
         - airplane: 1
         - truck: 3.7
         - package: 6.4  

   > Problemas (its=2800)
      - max_atoms = 15
         - diff = [28. 27.2 31.7] (baja)
         - diversidad baja
            - Problemas con 2, 3 y 4 ciudades
            - Cada ciudad tiene un camión y, la mayoría
              de las veces, un solo airport
            - No hay locations

      - max_atoms = 20
         - diff = [44. 39.7 43.6] (baja)
         - diversidad baja
            - Problemas con hasta 6 ciudades

      - max_atoms = 30
         - diff = [94.3  90.2 113.2] (baja)
         - diversidad baja
            - Problemas con hasta 6 ciudades

   >>> Usar una goal_entropy alta no ayuda a que se añadan
       locations!
       Se generan problemas donde cada ciudad solo tiene un
       airport


>  <logistics>
   planners=[lama-first, lazy-greedy, lazy-greedy]
   max_atoms_init_state=15, max_actions_init_state=1, max_actions_goal_state=2.0
   <init_policy_entropy_coeffs: 0.0, None>
   <goal_policy_entropy_coeffs: 0.0, None>
   <diversity_rescale_factor=10.0>

   > Ejecutándose en casa
   > logs: init_policy/version_207
   > saved_models: deleted

   > Entrenamiento (its=7000)
      - Tiempo (casa): 2d 12h
      - r_diff (goal_policy)= 40
      - r_eventual=-0.05
      - init_policy_entropy=0.3, goal_policy_entropy=0.2
      - term_cond_prob init_policy = 0.015, goal_policy = 0.05
      - num_objs:
         - city: 3.2
         - airport: 3.2
         - location: 0.8
         - airplane: 1.1
         - truck: 3.2
         - package: 6.5  

   > Problemas (its=6000)
      - max_atoms = 15
         - diff = [40.7 41.3 42.2] (medio-alta)
         - diversidad media-baja
            - Problemas con 2, 3 y 4 ciudades
            - Cada ciudad tiene un solo camión y, la mayoría
              de las veces, un solo airport
            - Hay pocas locations

      - max_atoms = 20
         - diff = [82. 71.4 73.5] (alta)
         - Problemas con 2 hasta 6 ciudades

      - max_atoms = 30
         - diff = [170. 138.8 153.3] (medio-baja)
         - Problemas con hasta 6 ciudades
         - Se generan problemas con varias locations!!!

      - max_atoms = 40
         - diff = [238.5 215.8 282.] (baja)
         - Problemas con hasta 9 ciudades
         - Se generan problemas con varias locations!!! 

   >>> Análisis de resultados
      - El número de ciudades escala bien
      - En los problemas de cierto tamaño, se añaden un número moderado de locations!
      - No obstante, la dificultad escala mal
      >>> Aunque se generan problemas con locations, la goal-policy
        no usa la acción drive!!!
        2 posibles causas:
         - El número de locations en problemas de 15 átomos
           es muy pequeño como para que aprenda -> aumentar
           el número de locations generados
         - La goal_policy converge prematuramente a solo usar
           acciones fly,load y unload pero no drive -> usar
           goal_policy entropy 


>  <logistics>
   planners=[lama-first, lazy-greedy, lazy-greedy]
   max_atoms_init_state=15, max_actions_init_state=1, max_actions_goal_state=2.0
   <init_policy_entropy_coeffs: 0.0, None>
   <goal_policy_entropy_coeffs: 0.0, None>
   <diversity_rescale_factor=1.0>

   > Ejecutándose en DaSCI
   > logs: init_policy/version_209
   > saved_models: both_policies_266

   > Entrenamiento (its=3800)
      - Tiempo (casa): 2d 12h
      - r_diff (goal_policy)= 50
      - r_eventual=0
      - init_policy_entropy=0.1, goal_policy_entropy=0.14
      - term_cond_prob init_policy y goal_policy convergen a 0
      - num_objs:
         - city: 3
         - airport: 3
         - location: 0
         - airplane: 1
         - truck: 3
         - package: 8 

   > Problemas (its=3500)
      - max_atoms = 15
         - diff = [53.  47.9 49.5] (medio-alta)
         - diversidad muy baja!!!
            - De los 10 problemas, 9 son idénticos

      - max_atoms = 20
         - diff = [85.4  81.5 145.2] (alta)
         - Problemas con 3 y 4 ciudades

      - max_atoms = 30
         - diff = [174, 174, 451274] (alta)
         - Problemas con 5 y 6 ciudades

      - max_atoms = 40
         - diff = [268 268 8.925558e+05] (media alta)
         - Problemas con 7 y 8 ciudades

   >>> diversity_rescale_factor=1 es demasiado bajo!
      - Se generan problemas casi idénticos


>  <logistics>
   planners=[lama-first, lazy-greedy, lazy-greedy]
   max_atoms_init_state=15, max_actions_init_state=1, max_actions_goal_state=2.0
   init_policy_entropy_coeffs: 0.0, None
   goal_policy_entropy_coeffs: 0.0, None
   <new_diversity_reward>
   <no feature weights>
   <diversity_rescale_factor=100.0>

   > Ejecutándose en casa
   > logs: init_policy/version_208
   > saved_models: both_policies_267

   > Entrenamiento (its=9200)
      - Tiempo (casa): 2d 20h
      - r_diff (goal_policy)= 29
      - r_eventual=-0.15
      - init_policy_entropy=0.4, goal_policy_entropy=0.2
      - term_cond_prob init_policy converge a 0, goal_policy converge a 0.05
      - num_objs:
         - city: 2
         - airport: 2.4
         - location: 1.3
         - airplane: 2.3
         - truck: 3
         - package: 6 

   > Problemas (its=9000)
      - max_atoms = 15
         - diff = [30.4 26.7 29.1] (baja)
         - diversidad alta
            - Todos los problemas con dos ciudades
            - Excepto eso, todos los demás objetos varían mucho

      - max_atoms = 20
         - diff = [38.8 34.9 37.9]
         - Todos los problemas con dos ciudades

      - max_atoms = 30
         - diff = [96.1 71.4 85.7]
         - Todos los problemas con dos ciudades

      - max_atoms = 40
         - diff = [115.3  82.9 102. ]
         - Todos los problemas con dos ciudades  

   > Análisis
      >> Se generan problemas con locations y donde la goal_policy usa tanto
         drive como fly!!
      > No obstante, la dificultad es muy baja y solo se generan problemas con
        dos ciudades!


>  <logistics>
   planners=[lama-first, lazy-greedy, lazy-greedy]
   max_atoms_init_state=15, max_actions_init_state=1, max_actions_goal_state=2.0
   init_policy_entropy_coeffs: 0.0, None
   goal_policy_entropy_coeffs: 0.0, None
   <new_diversity_reward>
   <no feature weights>
   <diversity_rescale_factor=10.0>

   > Ejecutándose en DaSCI
   > logs: init_policy/version_210
   > saved_models: both_policies_268

   > Entrenamiento (its=3800)
      - Tiempo (dasci): 2d 8h
      - r_diff (goal_policy)= 42
      - r_eventual=0
      - init_policy_entropy=0.3, goal_policy_entropy=0.2
      - term_cond_prob init_policy converge a 0, goal_policy converge a 0.035
      - num_objs:
         - city: 2.8
         - airport: 2.9
         - location: 0.4
         - airplane: 1
         - truck: 3.2
         - package: 7.5

   > Problemas (its=3500)
      - max_atoms = 15
         - diff = [42.6 40.1 41.4] (medio-alto)
         - diversidad media-alta
            - Problemas con 2 y 3 ciudades
            - La mayoría de las ciudades tienen un solo airport y truck,
              pero algunas tienen dos

      - max_atoms = 20
         - diff = [79.1 62.9 74.1]
         - Problemas con 2-4 ciudades

      - max_atoms = 30
         - diff = [136.6 129. 138.1]
         - Problemas con 5-6 ciudades

      - max_atoms = 40
         - diff = [256.1 228.1 300.3]
         - Problemas con 6-9 ciudades    

   >> Análisis
      >> La goal_policy usa acciones tanto drive como fly
      >> El número de ciudades escala
      > La dificultad escala regular
      > La mayoría de ciudades solo tienen un airport y truck
        y hay pocas locations

      >>> Nota: para problemas grandes, la goal_policy a veces
          entra en bucle (ej.: un camión va de la location A a B
          y de vuelta repetidas veces antes de hacer unload)


>  <logistics>
   planners=[lama-first, lazy-greedy, lazy-greedy]
   max_atoms_init_state=15, max_actions_init_state=1, max_actions_goal_state=2.0
   init_policy_entropy_coeffs: 0.0, None
   goal_policy_entropy_coeffs: 0.0, None
   new_diversity_reward
   no feature weights
   <diversity_rescale_factor=10.0>
   <max_actions_goal_state=10.0>

   > Ejecutándose en casa
   > logs: init_policy/version_209, 210
   > saved_models: both_policies_269, 271

   > Entrenamiento (its=7100)
      - Tiempo (dasci): 2d 18h
      - r_diff (goal_policy)= 42
      - r_eventual=0
      - init_policy_entropy=0.22, goal_policy_entropy=0.22
      - term_cond_prob init_policy converge a 0, goal_policy converge a 0.045
      - num_objs:
         - city: 2.6
         - airport: 2.6
         - location: 0.4
         - airplane: 1
         - truck: 3
         - package: 7.8

   > Problemas (its=3500)
      - max_atoms = 15
         - diff = [43.3 43.3 40.1] (medio-alto)
         - diversidad media-baja
            - Problemas con 2 y 3 ciudades
            - La mayoría de las ciudades tienen un solo airport y truck,
              pero algunas tienen dos 
            - Pocas locations

      - max_atoms = 20
         - diff = [72.5 68.2 60.3]
         - Problemas con 3 y 4 ciudades

      - max_atoms = 30
         - diff = [73.2 60.1 63.1]
         - Problemas con 3-6 ciudades

      - max_atoms = 40
         - diff = [1 1 1]
            - Se samplea la termination condition justo al empezar a generar
              el goal!!!
         - Problemas con 2-6 ciudades

   > Análisis
      > Resultados muy parecidos (e incluso peores) a cuando uso max_actions_goal_state=2.0!
      >>> CREO QUE EL PROBLEMA ES EL disc_factor_difficulty!!! (si no vale 1,
          la r_diff se descuenta, por lo que a la goal_policy se la motiva a
          generar el goal usando el mínimo número de acciones posible)



>  <logistics>
   planners=[lama-first, lazy-greedy, lazy-greedy]
   max_atoms_init_state=15, max_actions_init_state=1
   init_policy_entropy_coeffs: 0.0, None
   goal_policy_entropy_coeffs: 0.0, None
   new_diversity_reward
   no feature weights
   <diversity_rescale_factor=10.0>
   <max_actions_goal_state=4.0>

   > Ejecutándose en DaSCI
   > logs: init_policy/version_211
   > saved_models: both_policies_270

   > Entrenamiento (its=8000)
      - Tiempo (dasci): 5d 16h
      - r_diff (goal_policy)= 70 (la r_diff deja casi de aumentar en its=6000)
      - r_eventual=0
      - init_policy_entropy=0.22, goal_policy_entropy=0.35
      - term_cond_prob init_policy converge a 0, goal_policy converge a 0.045
      - num_objs:
         - city: 2.1
         - airport: 2.1
         - location: 1.8
         - airplane: 2
         - truck: 2.1
         - package: 7

   > Problemas (its=8000)
      - max_atoms = 15
         - diff = [71.3 51.2 55.6] (alta)
         - diversidad media-baja
            - Problemas con 2 y 3 ciudades
            - La mayoría de los problemas son idénticos o muy
              parecidos!!!

      - max_atoms = 20
         - diff = [131.6  87.1 100.6]
         - Problemas con 2 y 3 ciudades

      - max_atoms = 30
         - diff = [13.5 11.5 11.6]
         - Problemas con 3 y 4 ciudades
         - En algunos problemas la goal_policy samplea la
           termination condition antes de tiempo!!!

      - max_atoms = 40
         - diff = [1. 1. 1.]
         - Problemas con 4-6 ciudades
         - En todos los problemas la goal_policy samplea la
           termination condition al empezar!!!

   > Análisis
      - Se generan problemas tanto con locations como con varias ciudades
        y donde la goal_policy usa tanto "fly" como "drive"!!!
      - La dificultad para los problemas con 15 y 20 átomos
        aumenta respecto a cuando se usaba max_actions_goal_state=2
      - No obstante, la diversidad es bastante baja
      - Además, la goal_policy no generaliza bien a problemas de mayor
        tamaño (ejecuta la termination condition antes de tiempo) 



------ Pruebas NeSIG ablations

>  <logistics>
   <only lama-first for training>
   max_atoms_init_state=15
   init_policy_entropy_coeffs: 0.0, None
   goal_policy_entropy_coeffs: 0.0, None
   diversity_rescale_factor=10.0
   <max_actions_goal_state=2.0>
   <disc_factor_difficulty=1>
   <train both policies>

   > Ejecutándose en casa
   > logs: init_policy/version_211
   > saved_models: both_policies_272

   > Entrenamiento(its=1700, lo paré a mitad)
      - Tiempo: 11h
      - Resultados muy parecidos a experimento con
        tres planners en entrenamiento y disc_factor_difficulty=0.995
        No obstante, el tiempo de entrenamiento es bastante menos!

   > Problemas (its=1600)
      - max_atoms = 15
         - diff = [42.  33.8 38.1]

      - max_atoms = 20
         - diff = [72.  56.9 69.3]

      - max_atoms = 30
         - diff = [138.3 104.2 132.9]

      - max_atoms = 40
         - diff = [236.3 190.3 250.1]
         
   > Análisis
      - Parece que todo funciona correctamente
      - Parece que la dificultad evaluada en training con un solo planner
        después generaliza a los otros dos planners! 


>  <logistics>
   <only lama-first for training>
   max_atoms_init_state=15
   init_policy_entropy_coeffs: 0.0, None
   goal_policy_entropy_coeffs: 0.0, None
   diversity_rescale_factor=10.0
   <max_actions_goal_state=2.0>
   <disc_factor_difficulty=1>
   <train only init state policy>

   > Ejecutándose en casa
   > logs: init_policy/version_212 (no hay goal_policy logs)
   > saved_models: both_policies_273

   > Entrenamiento (its=1150, parado a mitad)
      - Tiempo: 3h 30min
      - Aprende, aunque la r_diff sube mucho más lentamente
        que si entrenamos ambas generative policies

   > Problemas (its=1100)
      - max_atoms = 15
         - diff = [3.4 3.2 3.2]

      - max_atoms = 20
         - diff = [5.9 4.7 4.8]

      - max_atoms = 30
         - diff = [8.3 7.4 7.4]

      - max_atoms = 40
         - diff = [11.3 9.4 9.1]

   > Análisis:
      - Aprende aunque mucho peor que con ambas generative policies


>  <logistics>
   <only lama-first for training>
   max_atoms_init_state=15
   init_policy_entropy_coeffs: 0.0, None
   goal_policy_entropy_coeffs: 0.0, None
   diversity_rescale_factor=10.0
   <max_actions_goal_state=2.0>
   <disc_factor_difficulty=1>
   <train only goal policy>

   > Ejecutándose en casa
   > logs: goal_policy/version_190 (no hay init_policy logs)
   > saved_models: both_policies_274

   > Entrenamiento (its=1150, parado a mitad)
      - Tiempo: 1h 47min
      - Aprende, aunque la r_diff sube más lentamente
        que si entrenamos ambas generative policies
      - La r_diff sube más rápidamente que cuando solo
        entrenamos la init policy -> parece que la goal
        policy es más importante que la init policy!!

   > Problemas (its=1100)
      - max_atoms = 15
         - diff = [11.8 11. 11.3]

      - max_atoms = 20
         - diff = [11.7 10.1 10.4]

      - max_atoms = 30
         - diff = [26.8 20.2 20.7]

      - max_atoms = 40
         - diff = [43.4 32.5 34.2]

   > Análisis
      - Aprende peor que con ambas generative policies pero mejor
        que solo entrenando la init_policy


>  <logistics>
   <random generator (no generative policies)>

   > Problemas
      - max_atoms = 15
         - diff = [7.3 5.9 6.5]

      - max_atoms = 20
         - diff = [5.6 4.8 5. ] 

      - max_atoms = 30
         - diff = [8.8 7.8 7.8] 

      - max_atoms = 40
         - diff = [8.  6.8 6.8]

   > Análisis
      - Dificultad muy baja! (más baja que entrenando solo una
        de las dos generative policies)


>  <sokoban>
   <max_atoms_init_state=40+15, max_actions_init_state=1, max_actions_goal_state=1>
   init_policy_entropy_coeffs: 0.0, None
   goal_policy_entropy_coeffs: 0.0, None
   diversity_rescale_factor=10.0

   > Ejecutándose en Casa
   > logs: init_policy/version_0
   > saved_models: both_policies_275

   > Entrenamiento (its=1800)
      - Tiempo: 2d 14h
      - r_diff (goal_policy)= 2.5e+6
      - r_eventual=0
      - init_policy_entropy=0.12, goal_policy_entropy=0.35
      - term_cond_prob init_policy converge a 0, goal_policy oscila alrededor de 0.4
      - num_atoms:
         - at-wall: 0
         - at-box: 7

   > Problemas (its=1800)
      - max_atoms = 40+15, map_size=5x5
         - diff = [2410691 1935919 1051775]
         - diversidad muy baja
            - Todos los problemas con 7 boxes y sin walls
            - Hay problemas idénticos y en muchos las cajas están en los mismos sitios!!!

   > Análisis
      - Los problema son muy difíciles, por lo que el planner tarda mucho en resolverlos
        y el entrenamiento se ralentiza muchísimo!!!
         >>> Por lo pronto, debería intentar resolver los problemas en paralelo!

      - La diversidad de los problemas es muy baja!!!
         >>> Debo aumentar la diversity reward y, quizás, aplicar np.log a r_diff!


>  <logistics>
   max_atoms_init_state=15, max_actions_init_state=1, <max_actions_goal_state=4.0>
   init_policy_entropy_coeffs: 0.0, None
   goal_policy_entropy_coeffs: 0.0, None
   <diversity_rescale_factor=50.0>

   --- Segunda parte de la ejecución con solo logs cada 5 its y midiendo la dificultad
       en paralelo

   > Ejecutándose en DaSCI
   > logs: init_policy/version_212, 213
   > saved_models: both_policies_276, 278

   > Entrenamiento (its=8000)
      - Tiempo (casa): 7d
      - r_diff (goal_policy)= 70
      - r_eventual=0
      - init_policy_entropy=0.1, goal_policy_entropy=0.2
      - term_cond_prob init_policy converge a 0, goal_policy oscila alrededor de 0.002
      - num_objs:
         - city: 2.4
         - airport: 2.4
         - location: 0.6
         - airplane: 1.4
         - truck: 3
         - package: 7.6

    > Problemas (its=8000)
      - max_atoms = 15
         - diff = [73. 65.2 66.9]
         - diversidad media
            - la goal_policy usa drive y fly!!!
            - problemas con 2 y 3 ciudades
            - problemas con 0 o 1 location
            - problemas bastante parecidos entre sí

      - max_atoms = 20
         - diff = [84.5 76.2 85.5]
         - problemas con 2 y 3 ciudades

      - max_atoms = 30
         - diff = [124.5 120.7 163.3]
         - problemas con 3 y 4 ciudades

      - max_atoms = 40
         - diff = [285.7 230.2 287.7]
         - problemas con 2-5 ciudades

   > La dificultad escala regular


>  <logistics>
   max_atoms_init_state=15, max_actions_init_state=1, <max_actions_goal_state=10.0>
   init_policy_entropy_coeffs: 0.0, None
   goal_policy_entropy_coeffs: 0.0, None
   disc_factor_diff=1
   <diversity_rescale_factor=50.0>

   > Ejecutándose en Casa
   > logs: init_policy/version_1
   > saved_models: both_policies_277

   > Entrenamiento (its=8000)
      - Tiempo (casa): 9d
      - r_diff (goal_policy)= 70
      - r_eventual=0
      - init_policy_entropy=0.13, goal_policy_entropy=0.35
      - term_cond_prob init_policy converge a 0, goal_policy oscila alrededor de 0.003
      - num_objs:
         - city: 2.3
         - airport: 2.3
         - location: 0.7
         - airplane: 1.3
         - truck: 2.8
         - package: 7.8

   > Problemas (its=9000)
      - max_atoms = 15
         - diff = [68.3 58.6 67.8]
         - diversidad media
            - la goal_policy usa drive y fly!!!
            - problemas con 2 y 3 ciudades
            - problemas con 0 o 1 location

      - max_atoms = 20
         - diff = [73.5  73.3 101.7]
         - 2 y 3 ciudades

      - max_atoms = 30
         - diff = [96.4  94.3 147.6]
         - 3 a 5 ciudades

      - max_atoms = 40
         - diff = [216.2 198.1 396.]
         - 4 a 6 ciudades

   > Análisis
      - Los resultados son muy parecidos a cuando se usa max_actions_goal_state=4!!!
      - La goal policy usa tanto drive como fly, pero los problemas tienen muy pocos
        locations!
      - La dificultad escala bastante mal
      - Parece como si la goal policy no "aprovechara" la gran cantidad de acciones
        que tiene a su alcance (de ahí que la goal_policy entropy sea tan alta)!!
      - Creo que debería aumentar la entropía de las políticas, para evitar que converjan
        rápido


>  <logistics>
   max_atoms_init_state=15, max_actions_init_state=1, <max_actions_goal_state=5.0>
   init_policy_entropy_coeffs: 0.0, None
   goal_policy_entropy_coeffs: 0.0, None
   disc_factor_diff=1
   <diversity_rescale_factor=100>

   > Ejecutándose en Casa
   > logs: init_policy/version_2
   > saved_models: both_policies_279





   >>> CREO QUE LAS POLICIES CONVERGEN DEMASIADO PRONTO!!!
      - Debería probar a aumentar diversity reward o usar policy entropy y repetir
        experimentos con max_actions_goal_state=10!!!



