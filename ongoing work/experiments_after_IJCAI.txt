>  <logistics>
   <planners=[lama-first, lazy-greedy, lazy-greedy]>
   max_atoms_init_state=15, max_actions_init_state=1, max_actions_goal_state=2.0

   > logs: init_policy\version_204
   > saved_models: both_policies_257

   > Entrenamiento (its=1.5k)
      - Tiempo: 16h (en DaSCI)
      - r_diff (init_policy)= 3 (seguía aumentando, lo paré a mitad)
      - r_eventual=-0.05
      - init_policy_entropy 0.6, goal_policy_entropy en 0.3 (seguía bajando)
      - term_cond_prob init_policy y goal_policy ambas convergen a 0
      - num_objs:
         - city: 2.8 (y subiendo)
         - airport: 3
         - location: 0.2 (y bajando)
         - airplane: 2.3
         - truck: 3
         - package: 6.3


   > Problemas (its=1.5)
      - max_atoms = 20
         - diff = [68.2 52.9 58.1]
            - problemas con 20 átomos
         - diversidad media-alta
            - la mayoría de problemas no tienen locations
            - problemas con 2, 3, 4 y 5 ciudades
            - el número de paquetes varía moderadamente

      - max_atoms = 30
         - diff = [145.6 96.1 87.4]
            - problemas con 30 átomos
         - diversidad media
            - no aumenta el número de ciudades!!! (siguen estando entre 2 y 5)
            - aumenta sobretodo el número de packages

      - max_atoms = 40
         - diff = [233.2 154.6 153.4]
            - problemas con 40 átomos
         - diversidad media
            - se generan problemas con hasta 7 ciudades
            - aumenta sobretodo el número de packages 

   >> Aunque no terminé el entrenamiento, se puede ver que NeSIG no generaliza bien en logistics a problemas más
      grandes:
         - La dificultad escala mal
         - El número de ciudades escala mal


>  <logistics>
   planners=[lama-first, lazy-greedy, lazy-greedy]
   max_atoms_init_state=15, max_actions_init_state=1, max_actions_goal_state=2.0
   init_policy_entropy_coeffs: 0.1, None
   goal_policy_entropy_coeffs: 0.0, None
   <difficulty_rescale_factor=1>
   <new difficulty normalization>

   > Ejecutándose en DaSCI
   > logs: init_policy/version_206
   > saved_models: both_policies_259

   > Entrenamiento (its=5900)
      - Tiempo (DaSCI): 3d 20h
      - r_diff (goal_policy) = 60
      - r_eventual=-0.01
      - init_policy_entropy 0.6, goal_policy_entropy 0.1
      - term_cond_prob init_policy y goal_policy ambas convergen a 0
      - num_objs:
         - city: 2.1
         - airport: 5.5
         - location: 0.005
         - airplane: 1
         - truck: 2.1 (igual que city)
         - package: 6.2

         - Resumen:
            - Se generan problemas con 2 ciudades, un avión, muchos airports,
              sin locations, un camión por cada ciudad y muchos packages

   > Problemas (its=5000)
      - max_atoms = 15
         - diff = [63.9 53.3 59.9] (muy alta)
            - todos con 15 átomos
         - diversidad baja!!
            - Problemas con 2 y 3 ciudades
            - Entre distintos problemas solo suele variar el número de airports y packages (y esto solo varía un poco)

      - max_atoms = 20
         - diff = [125. 98.6 116.4] (muy alta)
         - Problemas con 2 y 3 ciudades

      - max_atoms = 30  
         - diff = [257.6 221.2 244. ] (medio-alta)
         - Problemas con 3, 4 y 5 ciudades

      - max_atoms = 40
         - diff = [450.7 388.8 454.7] (media)
         - Problemas con hasta 7 ciudades (aunque la media son 5 ciudades)
         - muy parecidos entre sí

   >> Análisis de resultados
      > El número de ciudades sí escala medio-bien pero, a pesar de esto, la dificultad escala mal!!
         <<<Al no haber locations, se pueden llevar los paquetes de un sitio a otro usando solo aviones directamente!! (la goal policy solo usa la acción fly, pero no drive (con trucks))>>>
         - Si quiero que la dificultad escale mejor, creo que tiene que aprender a generar problemas
           donde se usen tanto los aviones como los camiones para llevar paquetes 

            >>> Dos opciones:
               - Entrenar en problemas más grandes
               - Probar a aumentar la entropía de la goal_policy

      > A pesar de usar action_entropy_coeffs=1, los problemas son poco diversos!
         - Creo que la action_entropy no es una buena forma de motivar la diversidad:
           dado que el mismo problema se puede generar de distintas formas (añadiendo los átomos
           en distinto orden), una alta entropía puede resultar en problemas con poca diversidad!!
         
            >>> Debería usar diversity reward!!!


>  <logistics>
   planners=[lama-first, lazy-greedy, lazy-greedy]
   max_atoms_init_state=15, max_actions_init_state=1, max_actions_goal_state=2.0
   <init_policy_entropy_coeffs: 0.01, None>
   goal_policy_entropy_coeffs: 0.0, None
   <difficulty_rescale_factor=1>
   <new difficulty normalization>

   > Ejecutándose en casa
   > logs: init_policy/version_203
   > saved_models: both_policies_260

   > Entrenamiento (its=1800)
      - Tiempo (casa): 14h 15min
      - r_diff (goal_policy)= 40 (seguía aumentando)
      - r_eventual=-0.02
      - init_policy_entropy 0.4, goal_policy_entropy 0.2
      - term_cond_prob init_policy y goal_policy ambas convergen a 0
      - num_objs:
         - city: 3
         - airport: 3
         - location: 0.01
         - airplane: 2.1 (y bajando)
         - truck: 3
         - package: 7 
 
   > Problemas (its=1800)
      - max_atoms = 15
         - diff = [51.9 42.3 39.4]
            - todos con 15 átomos
         - diversidad baja
            - Todos los problemas con 3 ciudades
            - Ningún problema con location!
            - Cada ciudad tiene exactamente un airport!!!
              Se generan problemas donde solo se usan aviones,
              pero no camiones!!

      - max_atoms = 20
         - diff = [90.4 73.6 67.]
            - todos con 20 átomos
         - diversidad media-baja
            - Problemas con 3, 4 y 5 ciudades!

      - max_atoms = 30
         - diff = [202.9 155.4 129.3]
            - todos con 30 átomos
         - diversidad media-baja
            - Problemas con hasta 7 ciudades!

      - max_atoms = 40
         - diff = [455.1 306.6 209.1]
            - todos con 40 átomos
         - diversidad media-baja
            - Problemas con hasta 8 ciudades
               - Parece que el num de ciudades ya deja de escalar bien, lo que más aumenta
                 es el número de paquetes

   >> La dificultad escala regular
   >> La diversidad de los problemas es muy baja!
      >>> Necesito aumentar el action entropy coeff
      >>> 0.01 es demasiado bajo!
   >> Quizás necesito entrenar en problemas más grandes
         - Lo que hace que un problema sea difícil puede depender del tamaño!


>  <logistics>
   planners=[lama-first, lazy-greedy, lazy-greedy]
   max_atoms_init_state=15, max_actions_init_state=1, max_actions_goal_state=2.0
   <init_policy_entropy_coeffs: 1, None>
   goal_policy_entropy_coeffs: 0.0, None
   difficulty_rescale_factor=1
   new difficulty normalization

   > Ejecutándose en casa
   > logs: init_policy/version_204
   > saved_models: both_policies_261

   > Entrenamiento (its=9000)
      - Tiempo (casa): 3d
      - r_diff (goal_policy)= 42
      - r_eventual=-0.02
      - init_policy_entropy=0.85, goal_policy_entropy=0.1
      - term_cond_prob init_policy converge a 0 y la goal_policy_entropy a 0.01
      - num_objs:
         - city: 2
         - airport: 5.5
         - location: 1.3
         - airplane: 1.1
         - truck: 2.4
         - package: 4.7   

      >> El número de objetos es muy parecido
         a cuando se usa init_policy_entropy_coeffs=0.1!!
         La mayor diferencia es que ahora sí se generan
         locations

   > Problemas (its=7500)
      - max_atoms = 15
         - diff = [45.4 39.3 38.9] (alta)
            - todos con 15 átomos
         - diversidad media-alta
            - Todos los problemas con dos ciudades
            - El resto de objetos muy diversos

      - max_atoms = 20
         - diff = [75.4 61.1 72.1] (alta)
         - diversidad media-alta
            - Todos los problemas con dos ciudades
            - El resto de objetos muy diversos

      - max_atoms = 30
         - diff = [171.3 132.4 160.1] (media)
         - diversidad media
            - Todos los problemas con dos ciudades
            - El resto de objetos muy diversos

      - max_atoms = 40
         - diff = [373.3 270.5 353.3] (media-baja)
         - diversidad media
            - Todos los problemas con dos ciudades
            - El resto de objetos muy diversos

   >> Se usa drive para generar los objetivos!!!

   >> La dificultad de los problemas y generalización es peor
      que al usar init_policy_action_entropy=0.1
   >> Ahora, al aumentar el tamaño de problema, no aumenta
      el número de ciudades

   >> init_policy action_coeff es mejor 0.1 que 1!!!
      - No obstante, creo que no es un buen acercamiento para generar problemas diversos
         - Debería usar diversity reward!!!


---- Experimentos para mejorar generalización

>  <logistics>
   planners=[lama-first, lazy-greedy, lazy-greedy]
   max_atoms_init_state=15, max_actions_init_state=1, max_actions_goal_state=2.0
   <init_policy_entropy_coeffs: 0.1, None>
   <goal_policy_entropy_coeffs: 0.1, None>
   difficulty_rescale_factor=1
   new difficulty normalization

   > Ejecutándose en DaSCI
   > logs: init_policy/version_207
   > saved_models: both_policies_262

   > Paré el entrenamiento a mitad, tras 1.6k train its
      - La init y goal policies entropy caen muy rápido!!
      - Se generan problemas con varias ciudades, pero donde cada ciudad
        suele tener un único camión y airport, sin locations! -> Poca diversidad!!


>  <logistics>
   planners=[lama-first, lazy-greedy, lazy-greedy]
   <max_atoms_init_state=20>, max_actions_init_state=1, max_actions_goal_state=2.0
   <init_policy_entropy_coeffs: 0.1, None>
   goal_policy_entropy_coeffs: 0, None
   difficulty_rescale_factor=1
   new difficulty normalization
   <minibatch_size=50>

   > Ejecutándose en casa
   > logs: init_policy/version_205, version_206
   > saved_models: both_policies_263, 265

   > Entrenamiento (its=3400, paré el entrenamiento a mitad)
      - Tiempo (casa): 1d 15h
      - r_diff (goal_policy)= 85 (seguía aumentando)
      - r_eventual=-0.02
      - init_policy_entropy=0.7, goal_policy_entropy=0.1
      - term_cond_prob init_policy y goal_policy convergen a 0
      - num_objs:
         - city: 2.5 (bajando)
         - airport: 5.4
         - location: 0.1
         - airplane: 1
         - truck: 4.4
         - package: 8.8  

      >>> Básicamente los mismos objetos que entrenando en problemas de
          tamaño 15!!!!
          La principal diferencia es que hay más camiones (aunque si hubiera
          entrenado durante más probablemente habría disminuido) y que
          hay más paquetes
          Se generan problemas con pocas ciudades (para el tamaño) y
          sin locations!! 

   > Problemas (its=3400)
      - max_atoms = 15
         - diff = [53.9 42.9 46.6] (alta)
         - diversidad media-alta
            - Problemas con 2, 3 y 4 ciudades
            - Sin locations
            - Los num objs varían bastante

      - max_atoms = 20
         - diff = [95.8 77.4 87.9] (alta)
         - Problemas con 2 y 3 ciudades

      - max_atoms = 30
         - diff = [220.4 170.5 191.6] (media)
         - Problemas con 2, 3 y 4 ciudades

      - max_atoms = 40
         - diff = [389.4 298.7 341.] (media-baja)
         - Problemas con solo 2 y 3 ciudades!
         - Solo escala el número de paquetes

   >>> Los resultados no mejoran al entrenar en problemas de 20 átomos!!!
      > Se siguen generando problemas sin locations y pocas ciudades
         - Parece que el "tipo" de problema difícil es igual para 15 que
           20 átomos (con pocas ciudades y sin locations)
      > La generalización no mejora (de hecho empeora, pero creo que es por
        haber parado el entrenamiento a mitad)
         - La dificultad para problemas grandes es menor que al entrenar
           sobre 15 átomos
         - No escala el número de ciudades     

   << Debería entrenar sobre 15 átomos! >>


>  <logistics>
   planners=[lama-first, lazy-greedy, lazy-greedy]
   max_atoms_init_state=15, max_actions_init_state=1, max_actions_goal_state=2.0
   <init_policy_entropy_coeffs: 0.2, None>
   <goal_policy_entropy_coeffs: 0.2, None>
   difficulty_rescale_factor=1
   new difficulty normalization

   > Ejecutándose en DaSCI
   > logs: init_policy/version_208
   > saved_models: both_policies_264

   > Entrenamiento (its=2800, paré el entrenamiento a mitad)
      - Tiempo (casa): 1d 14h
      - r_diff (goal_policy)= 30
      - r_eventual=-0.02
      - init_policy_entropy=0.6, goal_policy_entropy=0.75
      - term_cond_prob init_policy y goal_policy convergen a 0
      - num_objs:
         - city: 3.6
         - airport: 3.7
         - location: 0.2
         - airplane: 1
         - truck: 3.7
         - package: 6.4  

   > Problemas (its=2800)
      - max_atoms = 15
         - diff = [28. 27.2 31.7] (baja)
         - diversidad baja
            - Problemas con 2, 3 y 4 ciudades
            - Cada ciudad tiene un camión y, la mayoría
              de las veces, un solo airport
            - No hay locations

      - max_atoms = 20
         - diff = [44. 39.7 43.6] (baja)
         - diversidad baja
            - Problemas con hasta 6 ciudades

      - max_atoms = 30
         - diff = [94.3  90.2 113.2] (baja)
         - diversidad baja
            - Problemas con hasta 6 ciudades

   >>> Usar una goal_entropy alta no ayuda a que se añadan
       locations!
       Se generan problemas donde cada ciudad solo tiene un
       airport


>  <logistics>
   planners=[lama-first, lazy-greedy, lazy-greedy]
   max_atoms_init_state=15, max_actions_init_state=1, max_actions_goal_state=2.0
   <init_policy_entropy_coeffs: 0.0, None>
   <goal_policy_entropy_coeffs: 0.0, None>
   <diversity_rescale_factor=10.0>

   > Ejecutándose en casa
   > logs: init_policy/version_207
   > saved_models: both_policies_266