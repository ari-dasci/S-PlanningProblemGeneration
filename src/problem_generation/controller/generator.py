"""
This module implements functionality for generating a set of planning problems. They can be generated either with the generative policies or
at random. It also provides functionality for training the generative policies and loading the models in order to generate the problems.
"""

import sys
import time
import random
import numpy as np
import math
import torch
import pytorch_lightning as pl
from pytorch_lightning.loggers.tensorboard import TensorBoardLogger
from joblib import Parallel, delayed
import itertools
import glob
import warnings
from pathlib import Path

from problem_generation.environment.problem_state import ProblemState
from lifted_pddl import Parser
from problem_generation.environment.planner import Planner
from problem_generation.environment.relational_state import RelationalState
from problem_generation.models.nlm import NLM
from problem_generation.models.reinforce import ReinforceDataset
from problem_generation.models.generative_policy import GenerativePolicy
from problem_generation.models.reinforce import TransformReinforceDatasetSample

class Generator():

	"""
	Constructor for the directed generator.

	@use_initial_state_policy If True, the initial state of the problems is generated with the init state policy (based on NLMs). If False,
							  it is generated by adding consistent atoms at random.
	@use_goal_policy If True, the goal of the problems is generated with the goal policy (based on NLMs). If False,
					 it is generated by executing actions at random.
	@diversity_rescale_factor Rescale factor for the diversity reward
	@disc_factor_cont_consistency  Discount factor (gamma) for continuous consistency reward
	@disc_factor_event_consistency Discount factor (gamma) for eventual consistency reward
	@disc_factor_difficulty  Discount factor (gamma) for difficulty reward
	@device Either 'cuda' or 'cpu'. Determines whether the models are trained on GPU or CPU.
	@max_objs_cache_reduce_masks The maximum number of objects for which to store in memory (cache) the reduced_maskes used by the NLMs
								 when exclude_self=True. If 0, we don't store the masks: they are calculated every time they are needed.
	@initial_state_info Information used to initialize the state s0 from which the initial state generation phase starts.
	@num_preds_inner_layers_initial_state_nlm This corresponds to the number of predicates of the NLM layers EXCEPT FOR THE INPUT AND OUTPUT LAYERS,
											  since the shapes of these two layers are calculated from the information about the predicates/actions in the domain.
											  If the NLM has no hidden layers, @num_preds_inner_layers_initial_state_nlm must be an empty list [].
											  Otherwise, the inner layers shapes must be passed as a list of lists, e.g., [[1,1,1,1]] (for only one hidden layer)
	@load_init_state_policy_checkpoint_name If not None, we load the initial state policy checkpoint given by @load_checkpoint_name instead of initializing
											the initial state policy (actor and critic NLMs) weights from scratch.
	@io_residual_initial_state_nlm If True, append the input tensors of the NLM to the input of each intermediate layer.
								   <Note>: io_residual_initial_state_nlm and res_connections_initial_state_nlm cannot be both True!

	"""
	def __init__(self, parser, planner, 
				 predicates_to_consider_for_goal=None, initial_state_info=None, consistency_validator=None,
				 allowed_virtual_objects=None,
				 penalization_continuous_consistency=-1, penalization_eventual_consistency=-1,
				 diversity_rescale_factor=10.0,
				 disc_factor_cont_consistency=0, disc_factor_event_consistency=0.9, disc_factor_difficulty=1, 
				 device='cuda', max_objs_cache_reduce_masks=0,
				
				 use_initial_state_policy=True,
				 num_preds_inner_layers_initial_state_nlm=[[4,4,4,4]], mlp_hidden_layers_initial_state_nlm=[0,0], 
				 io_residual_initial_state_nlm=True, res_connections_initial_state_nlm=False, exclude_self_inital_state_nlm=True,
				 lr_initial_state_nlm=5e-4, entropy_coeff_init_state_policy=1.0,
				 entropy_annealing_coeffs_init_state_policy = None, epsilon_init_state_policy=0.2, load_init_state_policy_checkpoint_name=None,
				 
				 use_goal_policy=True,
				 num_preds_inner_layers_goal_nlm=[[4,4,4,4]], mlp_hidden_layers_goal_nlm=[0,0], 
				 io_residual_goal_nlm=True, res_connections_goal_nlm=False, exclude_self_goal_nlm=True,
				 lr_goal_nlm=5e-4, entropy_coeff_goal_policy = 1.0,
				 entropy_annealing_coeffs_goal_policy = None, epsilon_goal_policy=0.2, load_goal_policy_checkpoint_name=None):
		
		# Ignore warnings
		warnings.filterwarnings('ignore', category=FutureWarning) # Numpy warning
		warnings.filterwarnings("ignore", ".*Consider increasing the value of the `num_workers` argument*") # Pytorch warning about increasing number of workers for dataloader

		if (io_residual_initial_state_nlm and res_connections_initial_state_nlm) or \
		   (io_residual_goal_nlm and res_connections_goal_nlm):
			raise Exception("The NLM cannot use io_residual and residual_connections at the same time.")

		self._parser = parser
		self._planner = planner
		self._initial_state_info = initial_state_info
		self._consistency_validator = consistency_validator
		self._allowed_virtual_objects = allowed_virtual_objects
		self._diversity_rescale_factor = diversity_rescale_factor
		self._disc_factor_cont_consistency = disc_factor_cont_consistency
		self._disc_factor_event_consistency = disc_factor_event_consistency
		self._disc_factor_difficulty = disc_factor_difficulty
		self._penalization_continuous_consistency = penalization_continuous_consistency
		self._penalization_eventual_consistency = penalization_eventual_consistency
		self._use_initial_state_policy = use_initial_state_policy
		self._use_goal_policy = use_goal_policy

		# Device
		assert device in ('cuda', 'cpu'), "Device must be either 'cuda' or 'cpu'"
	
		if device == 'cuda':
			if torch.cuda.is_available():
				self.device = torch.device('cuda:0')
			else:
				raise Exception("No GPU available (torch.cuda.is_available() returned False). Either solve the issue or set device to 'cpu'")
		else:
			self.device = torch.device('cpu')
			
		# <Relational State which contains the object types, type_hierarchy and actions in the domain>
		# Used to convert from action name to index and vice versa (e.g.: "stack" <-> 1)

		# Represent the parser actions in a suitable form for RelationalState
		# (action_name, action_params) -> action_params correspond to the subset of action variables with class=='param'
		parser_actions = set([(action[0], tuple([var for var, var_class in zip(action[1][0], action[1][1]) if var_class=='param'])) \
							  for action in self._parser.actions])

		self._dummy_rel_state_actions = RelationalState(self._parser.types, self._parser.type_hierarchy,
														parser_actions) 
		dummy_rel_state_predicates = RelationalState(self._parser.types, self._parser.type_hierarchy, self._parser.predicates)

		# <Goal predicates, as a list of predicates (with name and parameters)>
		if predicates_to_consider_for_goal is None: # Consider every predicate for the goal
			self._predicates_to_consider_for_goal = self._parser.predicates
		else:
			# Make sure every predicate name only appears at most once
			pred_names = set([pred[0] for pred in predicates_to_consider_for_goal])

			if len(pred_names) != len(predicates_to_consider_for_goal):
				raise ValueError("The parameter predicates_to_consider_for_goal contains at least one duplicate predicate")

			self._predicates_to_consider_for_goal = set(predicates_to_consider_for_goal)

		# <Parameters used to normalize the rewards>

		# Initial state policy
		self._reward_moving_mean_init_policy = 0 # 0
		self._reward_moving_std_init_policy  = 1 # 1
		self._initialize_reward_moving_mean_and_std_init_policy  = True # As initialization, we set self._reward_moving_mean and std to the
														   # mean and std reward of the first group of trajectories

		# Goal policy
		self._reward_moving_mean_goal_policy = 0 # 0
		self._reward_moving_std_goal_policy  = 1 # 1
		self._initialize_reward_moving_mean_and_std_goal_policy  = True

		# <Parameter used to normalize the difficulties obtained with the different planners>
		self._moving_mean_diff_each_planner = None
		

		# <Generative policies>

		# Initial state generation policy
		if self._use_initial_state_policy:
			num_preds_all_layers_initial_state_nlm = self._num_preds_all_layers_init_nlm(num_preds_inner_layers_initial_state_nlm)

			if load_init_state_policy_checkpoint_name is None:
				self._initial_state_policy = GenerativePolicy(num_preds_all_layers_initial_state_nlm, mlp_hidden_layers_initial_state_nlm, 
															io_residual_initial_state_nlm,
															res_connections_initial_state_nlm, exclude_self_inital_state_nlm,
															lr_initial_state_nlm, entropy_coeff_init_state_policy,
															entropy_annealing_coeffs_init_state_policy, epsilon_init_state_policy,
															dummy_rel_state_predicates,
															self.device, max_objs_cache_reduce_masks)
			else: # Load initial state policy from checkpoint
				self._initial_state_policy = GenerativePolicy.load_from_checkpoint(checkpoint_path=load_init_state_policy_checkpoint_name,
																					num_preds_layers_nlm=num_preds_all_layers_initial_state_nlm, 
																					mlp_hidden_sizes_nlm=mlp_hidden_layers_initial_state_nlm,
																					nlm_io_residual=io_residual_initial_state_nlm,
																					nlm_residual_connections=res_connections_initial_state_nlm, 
																					nlm_exclude_self=exclude_self_inital_state_nlm,
																					lr=lr_initial_state_nlm,
																					action_entropy_coeff=entropy_coeff_init_state_policy,
																					entropy_annealing_coeffs=entropy_annealing_coeffs_init_state_policy, 
																					epsilon=epsilon_init_state_policy,
																					dummy_rel_state=dummy_rel_state_predicates,
																					device=self.device,
																					max_objs_cache_reduce_masks=max_objs_cache_reduce_masks)
		else: # No init state policy -> initial states are generated at random
			self._initial_state_policy = None


		# Goal generation policy
		if self._use_goal_policy:
			num_preds_all_layers_goal_nlm = self._num_preds_all_layers_goal_nlm(num_preds_inner_layers_goal_nlm)

			if load_goal_policy_checkpoint_name is None:
				self._goal_policy = GenerativePolicy(num_preds_all_layers_goal_nlm, mlp_hidden_layers_goal_nlm,
															io_residual_goal_nlm,
															res_connections_goal_nlm, exclude_self_goal_nlm,
															lr_goal_nlm, entropy_coeff_goal_policy,
															entropy_annealing_coeffs_goal_policy, epsilon_goal_policy,
															dummy_rel_state_predicates,
															self.device, max_objs_cache_reduce_masks)
			else: # Load initial state policy from checkpoint
				self._goal_policy = GenerativePolicy.load_from_checkpoint(checkpoint_path=load_goal_policy_checkpoint_name,
																					num_preds_layers_nlm=num_preds_all_layers_goal_nlm, 
																					mlp_hidden_sizes_nlm=mlp_hidden_layers_goal_nlm,
																					nlm_io_residual=io_residual_goal_nlm,
																					nlm_residual_connections=res_connections_goal_nlm, 
																					nlm_exclude_self=exclude_self_goal_nlm,
																					lr=lr_goal_nlm,
																					action_entropy_coeff=entropy_coeff_goal_policy,
																					entropy_annealing_coeffs=entropy_annealing_coeffs_goal_policy, 
																					epsilon=epsilon_goal_policy,
																					dummy_rel_state=dummy_rel_state_predicates,
																					device=self.device,
																					max_objs_cache_reduce_masks=max_objs_cache_reduce_masks)
		else: # No goal policy -> goals are generated at random
			self._goal_policy = None


# ------- Getters and Setters --------
		

	@property
	def initial_state_policy(self):
		return self._initial_state_policy

	@property
	def goal_policy(self):
		return self._goal_policy

	@property
	def diversity_rescale_factor(self):
		return self._diversity_rescale_factor
	
	@property
	def disc_factor_cont_consistency(self):
		return self._disc_factor_cont_consistency
	
	@property
	def disc_factor_event_consistency(self):
		return self._disc_factor_event_consistency
	
	@property
	def disc_factor_difficulty(self):
		return self._disc_factor_difficulty

	@property
	def predicates_to_consider_for_goal(self):
		return self._predicates_to_consider_for_goal

	@property
	def penalization_continuous_consistency(self):
		return self._penalization_continuous_consistency

	@property
	def penalization_eventual_consistency(self):
		return self._penalization_eventual_consistency

	@property
	def allowed_virtual_objects(self):
		return self._allowed_virtual_objects

	@property
	def dummy_rel_state_actions(self):
		return self._dummy_rel_state_actions


	# ------- Auxiliary Methods --------


	"""
	Receives @num_preds_inner_layers_initial_state_nlm and returns the number of predicates of ALL the layers in the NLM 
	(it adds the shapes corresponding to the input and output layers).

	This function also adds the extra input nullary predicate corresponding to the number of atoms already added to the initial state,
	the extra input unary predicates encoding object types and the extra output nullary predicate (in the last position) corresponding 
	to the termination condition.
	"""
	def _num_preds_all_layers_init_nlm(self, num_preds_inner_layers_initial_state_nlm):
		num_preds_inner_layers_initial_state_nlm = np.array(num_preds_inner_layers_initial_state_nlm, dtype=np.int) # Convert to np array in case it was a list
		
		# Get domain predicates
		domain_types = self._parser.types
		domain_type_hierarchy = self._parser.type_hierarchy
		domain_preds = self._parser.predicates
		
		dummy_rel_state = RelationalState(domain_types, domain_type_hierarchy, domain_preds)

		if len(num_preds_inner_layers_initial_state_nlm) == 0: # Don't use inner layers
			input_nlm_layer_shape = np.array(dummy_rel_state.num_preds_each_arity_for_nlm(-1)).reshape(1,-1)
			output_nlm_layer_shape = np.array(dummy_rel_state.num_preds_each_arity_for_nlm(-1)).reshape(1,-1)

			# Input predicates
			input_nlm_layer_shape[0][0] += 2 # Add two extra nullary predicates for problem_size and perc_actions_executed
			input_nlm_layer_shape[0][0] += len(domain_types) # Add extra nullary predicates to represent the number
			input_nlm_layer_shape[0][0] += len(domain_preds) # of objects and atoms of each type				
			input_nlm_layer_shape[0][1] += len(domain_types)+1 # Add extra unary predicates to represent the object types (and one more to represent whether an object is virtual)

			# Output predicates
			output_nlm_layer_shape[0][0] += 1 # Add one extra nullary predicate for the termination condition probability

			num_preds_all_layers_initial_state_nlm = np.concatenate((input_nlm_layer_shape, output_nlm_layer_shape))

		else: # Use inner layers, as given by @num_preds_inner_layers_initial_state_nlm
			max_nlm_arity = len(num_preds_inner_layers_initial_state_nlm[0])-1
			
			input_nlm_layer_shape = np.array(dummy_rel_state.num_preds_each_arity_for_nlm(max_nlm_arity)).reshape(1,-1)
			output_nlm_layer_shape = np.array(dummy_rel_state.num_preds_each_arity_for_nlm(max_nlm_arity)).reshape(1,-1)

			# Input predicates
			input_nlm_layer_shape[0][0] += 2 # Add two extra nullary predicates for problem_size and perc_actions_executed
			input_nlm_layer_shape[0][0] += len(domain_types) # Add extra nullary predicates to represent the number
			input_nlm_layer_shape[0][0] += len(domain_preds) # of objects and atoms of each type
			input_nlm_layer_shape[0][1] += len(domain_types)+1 # Add extra unary predicates to represent the object types (and one more to represent whether an object is virtual)

			# Output predicates
			output_nlm_layer_shape[0][0] += 1 # Add one extra nullary predicate for the termination condition probability

			num_preds_all_layers_initial_state_nlm = np.concatenate((input_nlm_layer_shape, num_preds_inner_layers_initial_state_nlm,
																	 output_nlm_layer_shape))

		return num_preds_all_layers_initial_state_nlm


	"""
	Receives @num_preds_inner_layers_goal_nlm and returns the number of predicates of ALL the layers in the NLM 
	(it adds the shapes corresponding to the input and output layers).

	This function also adds the extra input nullary predicate corresponding to the number of actions already executed to obtain
	the current goal state, the extra input unary predicates encoding object types and the extra output nullary predicate 
	(in the last position) corresponding to the termination condition.
	"""
	def _num_preds_all_layers_goal_nlm(self, num_preds_inner_layers_goal_nlm):
		num_preds_inner_layers_goal_nlm = np.array(num_preds_inner_layers_goal_nlm, dtype=np.int) # Convert to np array in case it was a list
		
		# Get domain types and actions (with their parameters types) -> e.g.: ['stack', ['block', 'block']]
		domain_types = self._parser.types
		domain_type_hierarchy = self._parser.type_hierarchy
		domain_preds = self._parser.predicates
		
		dummy_rel_state_input = RelationalState(domain_types, domain_type_hierarchy, domain_preds)
		dummy_rel_state_output = self._dummy_rel_state_actions

		if len(num_preds_inner_layers_goal_nlm) == 0: # Don't use inner layers
			input_nlm_layer_shape = np.array(dummy_rel_state_input.num_preds_each_arity_for_nlm(-1)).reshape(1,-1)
			input_nlm_layer_shape *= 2 # The number of input predicates is actually twice, as it corresponds to both the predicates of the initial and goal states	
			input_nlm_layer_shape[0][0] += 2 # Add two extra nullary predicates for problem_size and perc_actions_executed
			input_nlm_layer_shape[0][0] += len(domain_types)   # Add extra nullary predicates to represent the number
			input_nlm_layer_shape[0][0] += 2*len(domain_preds) # of objects and atoms (for the init and goal states) of each type
			input_nlm_layer_shape[0][1] += len(domain_types) # Add extra unary predicates to represent the object types

			output_nlm_layer_shape = np.array(dummy_rel_state_output.num_preds_each_arity_for_nlm(-1)).reshape(1,-1)
			output_nlm_layer_shape[0][0] += 1 # Add one extra nullary predicate representing the termination condition

			num_preds_all_layers_goal_nlm = np.concatenate((input_nlm_layer_shape, output_nlm_layer_shape)) # Both the input and output layers have the same shape, as they correspond to state predicates

		else: # Use inner layers, as given by @num_preds_inner_layers_initial_state_nlm
			max_nlm_arity = len(num_preds_inner_layers_goal_nlm[0])-1

			input_nlm_layer_shape = np.array(dummy_rel_state_input.num_preds_each_arity_for_nlm(max_nlm_arity)).reshape(1,-1)
			input_nlm_layer_shape *= 2 # The number of input predicates is actually twice, as it corresponds to both the predicates of the initial and goal states	
			input_nlm_layer_shape[0][0] += 2 # Add two extra nullary predicates for problem_size and perc_actions_executed
			input_nlm_layer_shape[0][0] += len(domain_types)   # Add extra nullary predicates to represent the number
			input_nlm_layer_shape[0][0] += 2*len(domain_preds) # of objects and atoms (for the init and goal states) of each type
			input_nlm_layer_shape[0][1] += len(domain_types) # Add extra unary predicates to represent the object types

			output_nlm_layer_shape = np.array(dummy_rel_state_output.num_preds_each_arity_for_nlm(max_nlm_arity)).reshape(1,-1)
			output_nlm_layer_shape[0][0] += 1 # Add one extra nullary predicate representing the termination condition

			num_preds_all_layers_goal_nlm = np.concatenate((input_nlm_layer_shape, num_preds_inner_layers_goal_nlm,
																	 output_nlm_layer_shape))

		return num_preds_all_layers_goal_nlm	


	"""
	Returns the mask tensors used to mask (i.e., set to -inf) the probabilities of inconsistent atoms.
	An atom is inconsistent if:
		- It is instantiated on objects of incorrect type
		- The state resulting from adding the atom to it, it is not continuous-consistent  
		-  In case of the termination condition, it can only be sampled once all the required predicates
		  have been added to the initial state
	
	The mask tensors are returned as a list of tensors (or None, if there are no tensors to mask
	for that arity), with the same shape as the NLM output.
	In order to mask the NLM output, simply sum the mask tensor values with the output NLM tensors.

	<Note>: if no actions are valid, i.e., no atom can be added to the init state, we <never> mask the
			termination condition, even if the current state does not contain all the required predicates.

	@nlm_output_shape Shape of the last NLM layer, as a list of num_preds, e.g., [1,2,3,0]. Note: @nlm_output_shape must
					  take into account the extra nullary predicate added for the termination condition (in case it is added).
	@problem Instance of ProblemState containing the initial state the NLM is applied to.
	@allowed_predicates Predicates which can be added to the state in the next action.
	"""
	def _get_mask_tensors_init_policy(self, nlm_output_shape, problem, allowed_predicates):  
		# Get the objects (including virtuals)
		# Example: ['truck', 'airplane', 'package']
		rel_state = problem.initial_state
		objs_with_virtuals = rel_state.objects + rel_state.virtual_objs_with_type(allowed_predicates, self._allowed_virtual_objects)
		num_objs_with_virtuals = len(objs_with_virtuals)
		predicates = rel_state.predicates # Get the state predicates
		pred_to_index_dict = rel_state.pred_names_to_indices_dict_each_arity

		# Get the allowed predicates, i.e., those which can be added to rel_state -> We no longer use predicate_order
		# allowed_preds = self._consistency_validator.predicates_in_current_phase(rel_state)

		# Check if we can sample the termination condition (else it needs to be masked)
		# It can be sampled iff rel_state contains every required_pred
		state_atoms = rel_state.atoms
		preds_in_curr_state = set([a[0] for a in state_atoms])
		required_preds = set(self._consistency_validator.required_pred_names())
		term_cond_allowed = required_preds.issubset(preds_in_curr_state)

		# Initialize mask tensors full of -inf (mask all the tensor positions)
		mask_tensors = [torch.full( (num_objs_with_virtuals,)*r + (num_preds,), fill_value=-float("inf"), dtype=torch.float32, device=self.device) \
						if num_preds != 0 else None for r, num_preds in enumerate(nlm_output_shape)]

		# Obtain list of atoms which can be added to the state (i.e., those that preserve continuous consistency)
		consistent_atoms = problem.get_continuous_consistent_init_state_actions(allowed_predicates, self._allowed_virtual_objects)

		# Unmask those tensor positions corresponding to the atoms in consistent_atoms
		for atom in consistent_atoms:
			pred_ind = pred_to_index_dict[atom[0]]
			pred_arity = len(atom[1])
			atom_obj_inds = tuple(atom[1])

			# Unmask the tensor position associated with the atom
			mask_tensors[pred_arity][atom_obj_inds + (pred_ind,)] = 0.0

		# Unmask the termination condition if it can be sampled
		if term_cond_allowed:
			mask_tensors[0][-1] = 0.0

		# If no action is valid, we unmask the termination condition
		# (set to 0)
		all_values_masked = all(torch.all(tensor==-float("inf")) for tensor in mask_tensors if tensor is not None)

		if all_values_masked:
			mask_tensors[0][-1] = 0.0

		return mask_tensors


	"""
	Returns the mask tensors used to mask the goal policy's NLM output. It masks (sets to -inf) the tensor positions
	corresponding to invalid actions, i.e., those with parameters of invalid type (according to type hierarchy) 
	or those for which their preconditions are not met.

	@nlm_output_shape Shape of the last NLM layer, as a list of num_preds, e.g., [1,2,3,0]. Note: @nlm_output_shape must
					  take into account the extra nullary predicate added for the termination condition (in case it is added).
	@problem The current problem (s_i, s_gc), used to obtain the applicable actions at the current goal state.
	@termination_condition Whether the NLM output contains an extra nullary predicate representing the termination condition.
						   If True, we must also unmask that predicate (since the termination_condition can always be executed).
	"""
	def _get_mask_tensors_goal_policy(self, nlm_output_shape, problem, termination_condition=True):
		num_objs = problem.goal_state.num_objects # Number of objects in the goal state (and also in the initial state)
		
		# Get applicable ground actions at the current goal state
		applicable_ground_actions = problem.applicable_ground_actions()

		# Convert from the relational encoding ( ('stack', (1, 2)) ) to the encoding
		# used by the NLM ( [action_arity, obj_1_ind, obj_2_ind, ..., action_ind] -> [2, 1, 2, 0] )
		action_name_to_ind_dict = self._dummy_rel_state_actions.pred_names_to_indices_dict_each_arity

		applicable_ground_actions_nlm_format = [ [len(a[1])] + list(a[1]) + [action_name_to_ind_dict[a[0]]] \
												for a in applicable_ground_actions ]

		# We mask all the NLM output positions except the ones corresponding to applicable_ground_actions_nlm_format
		
		# Initialize mask tensors full of -inf (all values are masked to -inf)
		mask_tensors = [torch.full( (num_objs,)*r + (num_preds,), -float("inf"), dtype=torch.float32, device=self.device) \
						if num_preds != 0 else None for r, num_preds in enumerate(nlm_output_shape)]

		# Unmask (set to 0) positions corresponding to applicable actions
		for a in applicable_ground_actions_nlm_format:
			mask_tensors[a[0]][tuple(a[1:])] = 0.0

		# If the NLM output has a termination condition, also unmask it
		if termination_condition:
			mask_tensors[0][nlm_output_shape[0]-1] = 0.0
			
		return mask_tensors


	"""
	In the initial state policy, used to transform from the NLM encoding of the atom to add to the one used by RelationalState.
	Returns a list with the objects to add to @rel_state. These objects correspond to those virtual objects @atom_to_add is instantiated on,
	i.e., those objects that are not present in @rel_state.

	<Note:> The atom must be represented as a list since tuples can't be modified (are immutable).

	Returns the atom to add with the changed indexes (so that they index objs in the state after adding the list objs_to_add) and the objects 
	to add to the state as a list like ['block', 'block'].
	After adding the objs_to_add to the state, the obj indexes in atom_to_add corresponding to virtual objects now index objects in the state.
	Example: if the state contains two objects and we are going to add a new virtual object, we need to change @atom_to_add from
			 ['on', [3, 0]] to ['on', [2,0]]
	"""
	def get_objs_to_add_and_atom_with_correct_indexes(self, rel_state, atom_to_add, allowed_predicates=None):
		state_preds = rel_state.predicates
		objs_without_virtuals = rel_state.objects
		num_objs_without_virtuals = len(objs_without_virtuals)
		objs_with_virtuals = objs_without_virtuals + rel_state.virtual_objs_with_type(allowed_predicates, self._allowed_virtual_objects)
		num_objs_with_virtuals = len(objs_with_virtuals)

		# <Obtain the types of the objects atom_to_add is instantiated on>
		# To do so, we use the object indexes returned by the NLM, which can index both
		# objects in rel_state and virtual objects

		obj_types = [objs_with_virtuals[obj_ind] for obj_ind in atom_to_add[1]]

		# <Obtain the new objects to add to rel_state (corresponding to the virtual objs
		# atom_to_add is instantiated on) and change the obj inds of atom_to_add[1] so that
		# they index objects in the state after these new objects are added to it>

		objs_to_add = []
		ind_next_state_obj = num_objs_without_virtuals # Index associated with the next object to add to the state (Example: if there are 2 objs in the state, ind_next_state_obj = 2)
		dict_old_inds_to_new_inds = dict() # This dictionary is used to transform the obj inds of atom_to_add
										   # Example: (on 3 1) -> (on 2 1), (on 3 3) -> (on 2 2)
		virtual_obj_indexes_used = set() # Contains the indexes corresponding to virtual objects that we have already processed (so that we don't process them again)
										 # For example, for the atom (on 1 1) (on a state with a single object) we only need to add an object of type "block", and not two

		for obj_ind in atom_to_add[1]:
			if obj_ind >= num_objs_without_virtuals and obj_ind not in virtual_obj_indexes_used: # the obj given by obj_ind is a virtual object (i.e., it is not in the state, so it must be added)
				# Add an object of type given by the corresponding virtual object in rel_state
				objs_to_add.append(objs_with_virtuals[obj_ind])

				# Change atom index corresponding to virtual object
				dict_old_inds_to_new_inds[obj_ind] = ind_next_state_obj
				ind_next_state_obj += 1

				virtual_obj_indexes_used.add(obj_ind)

		# Change virtual obj indexes according to the values stored in the dict
		for param_position, obj_ind in enumerate(atom_to_add[1]):
			if obj_ind in dict_old_inds_to_new_inds: # If the index is not in the dictionary, it does not need to be changed
				atom_to_add[1][param_position] = dict_old_inds_to_new_inds[obj_ind]

		return atom_to_add, objs_to_add, obj_types


	"""
	This method receives a trajectory as input and, for each element, it obtains the discounted sum of rewards, accounting
	for the continuous consistency, eventual consistency, problem difficulty and the sum of these three discounted rewards.
	We assume the first element of the trajectory corresponds to the state at t=0 and the last element
	to the state t=T.

	<Note>: this method is inplace (does not return the trajectory but transforms it inplace)
	"""
	# <CAMBIADO>
	def _sum_rewards_trajectory(self, trajectory):
		
		r_continuous_sum = 0
		r_eventual_sum = 0
		r_difficulty_new_sum = 0
		r_difficulty_old_sum = 0

		# Iterate over the trajectory in reverse (from the end to the beginning)
		for i in range(len(trajectory)-1,-1,-1):
			curr_r_continuous = trajectory[i][-4]
			curr_r_eventual = trajectory[i][-3]
			curr_r_difficulty_new = trajectory[i][-2]
			curr_r_difficulty_old = trajectory[i][-1]

			sum_r_continuous = curr_r_continuous + r_continuous_sum
			sum_r_eventual = curr_r_eventual + r_eventual_sum
			sum_r_difficulty_new = curr_r_difficulty_new + r_difficulty_new_sum
			sum_r_difficulty_old = curr_r_difficulty_old + r_difficulty_old_sum

			# The total sum of reward, used for training, uses the new r_difficulty
			sum_r_total = sum_r_continuous + sum_r_eventual + sum_r_difficulty_new

			trajectory[i] = trajectory[i][:-4] + [sum_r_continuous, sum_r_eventual, sum_r_difficulty_new, sum_r_difficulty_old, sum_r_total]

			r_continuous_sum += curr_r_continuous # Sum the current reward to the sum of disc rewards R
			r_continuous_sum *= self._disc_factor_cont_consistency # Apply disc factor to all the rewards in the sum
			r_eventual_sum += curr_r_eventual
			r_eventual_sum *= self._disc_factor_event_consistency
			r_difficulty_new_sum += curr_r_difficulty_new
			r_difficulty_new_sum *= self._disc_factor_difficulty
			r_difficulty_old_sum += curr_r_difficulty_old
			r_difficulty_old_sum *= self._disc_factor_difficulty


	"""
	This method adds the state value for each element in the trajectory corresponding
	to the initial state or goal generation phase.
	This is needed by the PPO algorithm to train the NLMs.
	Note: this method is in-place.

	@trajectory A list with the samples of the trajectory. Each sample must be in the form:
				[curr_state_tensors, num_objs_with_virtuals, mask_tensors, chosen_action_index, chosen_action_prob,
				 r_continuous_sum, r_eventual_sum, r_difficulty_sum, r_total_sum]
	@policy If 'initial_state_policy', we calculate the state values with the initial_state_policy's critic.
			If 'goal_policy', we use the goal_policy's critic.
	"""
	def _calculate_state_values_trajectory(self, trajectory, policy):
		
		# < Represent the data in a suitable form for the calculations >

		# Represent the trajectory as a numpy array. The row are the samples and the columns the different elements of each sample.
		trajectory_len = len(trajectory)
		# trajectory_np = np.array(trajectory, dtype=object) 
		#list_num_objs_with_virtuals = trajectory_np[:, 2].tolist()
		list_num_objs_with_virtuals = [sample[2] for sample in trajectory]


		# Represent the state tensors in a suitable encoding for the NLMs
		num_preds_state_tensors = len(trajectory[0][1]) # The number of elements in state_tensors (equal to the max predicate arity - 1)
		list_state_tensors_nlm_encoding = [[sample[1][r] for sample in trajectory] for r in range(num_preds_state_tensors)]

		# < Estimate State Value V(s) >

		# Estimate state-value V(s) with the Critic NLM of the initial state policy
		if policy == 'initial_state_policy':
			critic_output = self._initial_state_policy.critic_nlm(list_state_tensors_nlm_encoding, \
																  list_num_objs_with_virtuals)[0] # [0] to obtain the tensors for the nullary predicates
		elif policy == 'goal_policy':
			critic_output = self._goal_policy.critic_nlm(list_state_tensors_nlm_encoding, \
														 list_num_objs_with_virtuals)[0] # [0] to obtain the tensors for the nullary predicates
		else:
			raise ValueError("Policy parameter must be either 'initial_state_policy' or 'goal_policy'")
		
		state_values = [tensor[0].detach().item() for tensor in critic_output] # [0] to obtain the first predicate of the nullary predicates (corresponding to the state_value)

		# < Add new information to the trajectory >

		for i in range(trajectory_len):
			trajectory[i].append(state_values[i])


	"""
	This method receives a completely-generated problem (s_i, s_g) and returns its difficulty.
	This difficulty corresponds to the number of nodes expanded by the planner.
	It returns both the real difficulty and the scaled difficulty.
	If the planner uses more than @max_planning_time seconds to solve the problem, we assume
	the difficulty of the problem is equal to @max_difficulty.

	@phase If "train", we only use the first planner (lama-first) to measure the problem difficulty.
	       If "test", we measure the problem difficulty with the three planners.
	@max_difficulty The difficulty we consider a problem has when it could not be solved by the corresponding planner
	                (outofmemory error).

	<Note>: This method also selects the goal atoms corresponding to the goal predicates given by the user
	<Note2>: We no longer use a rescale factor for the difficulty. It is no longer needed as we normalize difficulties
	         to be close to 1.
	"""
	# <CAMBIADO>
	def get_problem_difficulty(self, problem, phase, max_difficulty=(1e6,1e6,1e6)):
		# Encode the problem in PDDL
		# > This method also selects the goal atoms corresponding to the goal predicates given by the user
		pddl_problem = problem.obtain_pddl_problem()

		# Write the problem to the temporary file
		self._fd_temp_problem.seek(0)
		self._fd_temp_problem.write(pddl_problem)
		self._fd_temp_problem.truncate()

		# Obtain its difficulty

		if phase == 'train':
			planners_to_use = (0,)
		elif phase == 'test':
			planners_to_use = None # Equivalent to (0,1,2)
		else:
			raise Exception("'phase' arguments needs to be either 'train' or 'test'")

		# Obtain its difficulty with several planners (number of nodes expanded by the planner or -1 if it couldn't solve it under
		# max_planning_time)
		# Note: if the planner does not find a solution, it also returns -1, but this situation should not happen
		#       as every problem is solvable.
		problem_difficulty_list = self._planner.get_problem_difficulty(self._temp_problem_path, planners_to_use)		

		# If the difficulty was -1, then there was an outofmemory error. We substitute it for max_difficulty
		problem_difficulty_list = [max_difficulty[ind] if diff == -1.0 else diff for ind, diff in enumerate(problem_difficulty_list)]

		# Return both the scaled and real difficulty
		return problem_difficulty_list


	"""
	This method normalizes the rewards in a trajectory (or set of trajectories) obtained by the initial policy so that they aproximately
	distribute normally (according to N(0,1)).
	Since the scale of rewards can vary a lot during training, we use a moving average to calculate the mean (\mu)
	and std (\sigma) used to normalize the rewards.

	<Note1>: we assume the rewards are in the position -2 of each trajectory sample. We insert in the -2 position
			 the normalized reward.
	<Note2>: this method modifies the trajectory in-place.
	<Note3>: I think this method doesn't work if called in parallel! (as we would be accessing the self._reward_moving_mean and self._reward_moving_std
			 variables in parallel!)
	"""
	def _normalize_rewards_init_policy(self, trajectory, moving_avg_coeff=0.8, moving_std_coeff=0.8):

		# <Calculate the mean and std of the trajectory rewards (the discounted sum of total rewards)>
		trajectory_rewards_np = np.array([sample[-2] for sample in trajectory], dtype=np.float32)
		trajectory_rewards_mean = np.mean(trajectory_rewards_np)
		trajectory_rewards_std = np.std(trajectory_rewards_np)

		# <Update the mu and sigma parameters using a moving average>
		# First iteration -> initialize moving averages
		if self._initialize_reward_moving_mean_and_std_init_policy:			
			self._reward_moving_mean_init_policy = trajectory_rewards_mean
			self._reward_moving_std_init_policy = trajectory_rewards_std
			self._initialize_reward_moving_mean_and_std_init_policy = False

		# <Normalize the trajectory rewards>
		# We store both the reward before normalization and after it
		for i in range(len(trajectory)):
			norm_reward = (trajectory[i][-2] - self._reward_moving_mean_init_policy) / (self._reward_moving_std_init_policy + 1e-10) # z-score normalization
			trajectory[i] = trajectory[i][:-1] + [norm_reward] + trajectory[i][-1:] # Store the normalized reward in the trajectory[i][-2] position

		# Update the moving average after the current update, unless this is the first iteration
		if not self._initialize_reward_moving_mean_and_std_init_policy:
			self._reward_moving_mean_init_policy = self._reward_moving_mean_init_policy*moving_avg_coeff + trajectory_rewards_mean*(1-moving_avg_coeff)
			self._reward_moving_std_init_policy = self._reward_moving_std_init_policy*moving_std_coeff + trajectory_rewards_std*(1-moving_std_coeff)


	"""
	This method normalizes the rewards in a trajectory (or set of trajectories) obtained by the goal policy so that they aproximately
	distribute normally (according to N(0,1)).
	Since the scale of rewards can vary a lot during training, we use a moving average to calculate the mean (\mu)
	and std (\sigma) used to normalize the rewards.

	<Note1>: we assume the rewards are in the position -2 of each trajectory sample. We insert in the -2 position
			 the normalized reward.
	<Note2>: this method modifies the trajectory in-place.
	<Note3>: I think this method doesn't work if called in parallel! (as we would be accessing the self._reward_moving_mean and self._reward_moving_std
			 variables in parallel!)
	"""
	def _normalize_rewards_goal_policy(self, trajectory, moving_avg_coeff=0.8, moving_std_coeff=0.8):

		# <Calculate the mean and std of the trajectory rewards>
		trajectory_rewards_np = np.array([sample[-2] for sample in trajectory], dtype=np.float32)
		trajectory_rewards_mean = np.mean(trajectory_rewards_np)
		trajectory_rewards_std = np.std(trajectory_rewards_np)

		# <Update the mu and sigma parameters using a moving average>
		# First iteration -> initialize moving averages
		if self._initialize_reward_moving_mean_and_std_goal_policy:		
			self._reward_moving_mean_goal_policy = trajectory_rewards_mean
			self._reward_moving_std_goal_policy = trajectory_rewards_std
			self._initialize_reward_moving_mean_and_std_goal_policy = False
			
		# <Normalize the trajectory rewards>
		# We store both the reward before normalization and after it
		for i in range(len(trajectory)):
			norm_reward = (trajectory[i][-2] - self._reward_moving_mean_goal_policy) / (self._reward_moving_std_goal_policy + 1e-10) # z-score normalization
			trajectory[i] = trajectory[i][:-1] + [norm_reward] + trajectory[i][-1:] # Store the normalized reward in the trajectory[i][-2] position

		# Update the moving average after the current update, unless this is the first iteration
		if not self._initialize_reward_moving_mean_and_std_goal_policy:
			self._reward_moving_mean_goal_policy = self._reward_moving_mean_goal_policy*moving_avg_coeff + trajectory_rewards_mean*(1-moving_avg_coeff)
			self._reward_moving_std_goal_policy = self._reward_moving_std_goal_policy*moving_std_coeff + trajectory_rewards_std*(1-moving_std_coeff)


	"""
	This method receives a list @goal_trajectories of goal_policy trajectories. For each sample in each trajectory, it then calculates
	the normalized mean of the different planner difficulties. The difficulties are normalized so that they all have a similar mean.
	It calculates both the "new" normalized mean, which oscillates around 1, and the "old" normalized mean, which increases during training and thus is
	used for logging the difficulty.
	"""
	def _calculate_normalized_mean_planner_diffs(self, goal_trajectories, moving_avg_coeff=0.8):
		# <Calculate the mean difficulty for each planner for the goal trajectories>

		# trajectory[-1][-2] is the second-to-last element (r_diff_list) for the last sample of the trajectory
		diff_non_empty_trajectories = [trajectory[-1][-2] for trajectory in goal_trajectories if len(trajectory)>0]

		# If all the goal trajectories are empty, we do not need to calculate the normalized difficulty for any of them
		if len(diff_non_empty_trajectories) == 0:
			return

		mean_diffs_curr_trajectory = np.array(diff_non_empty_trajectories).mean(axis=0) 

		# First call to this method
		# Initialize the (moving) means to the curr trajectory means
		first_call = self._moving_mean_diff_each_planner is None

		if first_call:
			self._moving_mean_diff_each_planner = mean_diffs_curr_trajectory
			
		# <Calculate the normalized difficulty mean>

		# OLD
		# Rescale all the planner difficulties so that they all have the same mean as the first planner
		# rescale_coeffs[0]=1, meaning that we don't need to rescale the difficulty for the first planner
		rescale_coeffs_old = self._moving_mean_diff_each_planner / self._moving_mean_diff_each_planner[0]

		# New -> rescale planner difficulties so that they all oscilate around 1 for the whole training
		rescale_coeffs_new = self._moving_mean_diff_each_planner

		for i in range(len(goal_trajectories)):
			if len(goal_trajectories[i]) > 0:
				diff_list = goal_trajectories[i][-1][-2]

				# Old normalized mean
				normalized_diffs_old = diff_list / rescale_coeffs_old
				goal_trajectories[i][-1][-1] = np.prod(normalized_diffs_old)**(1/normalized_diffs_old.size)

				# New normalized mean
				normalized_diffs_new = diff_list / rescale_coeffs_new
				goal_trajectories[i][-1][-2] = np.prod(normalized_diffs_new)**(1/normalized_diffs_new.size)

		# If this is NOT the first call to this method, update the moving means after updating the current trajectories
		if not first_call:
			self._moving_mean_diff_each_planner = self._moving_mean_diff_each_planner*moving_avg_coeff + mean_diffs_curr_trajectory*(1-moving_avg_coeff)


	"""
	Auxiliary method used by _add_diversity_reward(), in order to obtain the feature vector associated with each state.
	It returns both the feature vector and a feature of weights, used to calculate the weighted average of feature differences.

	@allowed_object_types A list with the object types that we can find in the initial state. If None, we assume
	                      it is equal to @init_state.types
	"""
	def _obtain_dist_features(self, init_state, allowed_object_types=None):
		types = init_state.types if allowed_object_types is None else allowed_object_types # We only care about the allowed object types
		predicates = init_state.predicates
		pred_names = init_state.predicate_names
		objs = tuple(init_state.objects) # Tuple with the type of each object in the state
		atoms = init_state.atoms
		atom_names = tuple([a[0] for a in atoms])
		num_types = len(types)
		num_preds = len(predicates)
		num_objs = len(objs)
		num_atoms = len(atoms)
	
		# <Number of objects and atoms of each type>
		
		# Count percentage of objects of each type
		num_objs_each_type = np.array([objs.count(t)/num_objs for t in types], dtype=float)

		# Count percentage of atoms of each predicate type
		num_atoms_each_pred = np.array([atom_names.count(p)/num_atoms for p in pred_names], dtype=float)

		# <Mean and std of the number of objects each object relates to>

		# Obtain dictionaries that assign a number to every type and predicate
		# Pred names to indices
		pred_to_ind_dict = init_state.pred_names_to_indices_dict

		# (allowed) object types to indices
		type_indices = list(range(len(types)))
		type_to_ind_dict = dict(zip(types, type_indices)) # 'airplane' : 0, 'location' : 1
		ind_to_type_dict = dict(zip(type_indices, types)) # 0 : 'airplane', 1 : 'location'

		# Obtain list l[obj_ind][pred_type][obj_type]:
		# For each object "obj_ind", measure with how many objects of each type "obj_type" it relates according
		# to atoms of type "pred_type". An object "relates" to another one if they are instantiated on the same
		# atom (regardless of position)
		l = np.zeros(shape=(num_objs, num_preds, num_types), dtype=np.uint16) # Initialized to zeros

		for curr_atom in atoms:
			curr_pred_type = curr_atom[0] # Predicate type of the current atom

			# <Calculate how many objects of each type the atom is instantiated on>
			# Convert from obj indexes to obj_types
			types_objs_in_atom = [objs[obj_ind] for obj_ind in curr_atom[1]]

			# list_num_objs_each_type[i]=n -> curr_atom is instantiated on "n" objects of type associated with index "i"
			list_num_objs_each_type = [types_objs_in_atom.count(ind_to_type_dict[type_ind]) for type_ind in type_indices]

			# <For each object curr_atom is instantiated on, sum the number of objects of each type it is instantiated on>
			curr_pred_ind = pred_to_ind_dict[curr_pred_type]

			for curr_obj in curr_atom[1]:
				l[curr_obj][curr_pred_ind] += np.array(list_num_objs_each_type, dtype=np.uint16)

				# Substract one to the type of curr_obj (else, we would be counting one extra type for curr_obj)
				l[curr_obj][curr_pred_ind][ type_to_ind_dict[objs[curr_obj]] ] -= 1

		# Obtain list l2[obj_type_i][pred_type][obj_type_j]:
		# l2[obj_type_i][pred_type][obj_type_j] is equal to the np array [l[a][pred_type][obj_type_j], l[b][pred_type][obj_type_j], ...]
		# where a, b, ... are all the objects of type obj_type_i
		l2 = np.empty(shape=(num_types, num_preds, num_types), dtype=object)
		l2.fill(np.array([0], dtype=np.uint16)) # We fill every position with np.array([0]) in case the init state does not contain any object
		                                        # of the type associated with that position

		for curr_type in types:
			curr_type_ind = type_to_ind_dict[curr_type]

			# Obtain the indices of all the objects of type curr_type
			obj_inds_curr_type = [i for i, t in enumerate(objs) if t == curr_type]

			# We do this loop because we can't simply do l2[curr_type_ind,:,:]=l[obj_inds_curr_type,:,:]
			# I know there should be a more efficient way :(
			if len(obj_inds_curr_type) > 0: # If there are no objects of curr_type in the state, then skip this step
				for p_ind in range(num_preds):
					for t_ind in range(num_types):
						l2[curr_type_ind, p_ind, t_ind] = l[obj_inds_curr_type, p_ind, t_ind]

		# Obtain a list with the averages and a list with the standard deviations
		# for every sublist in l2[i,j,k]
		l_mean = np.empty(shape=(num_types, num_preds, num_types), dtype=float)
		l_std = np.empty(shape=(num_types, num_preds, num_types), dtype=float)

		for i in range(num_types):
			for j in range(num_preds):
				for k in range(num_types):
					l_mean[i,j,k] = np.mean(l2[i,j,k])
					l_std[i,j,k] = np.std(l2[i,j,k])

		# Flatten the two arrays
		l_mean = l_mean.flatten()
		l_std = l_std.flatten()

		# Concatenate all the features
		state_features = np.concatenate((num_objs_each_type, num_atoms_each_pred, l_mean, l_std))

		# Obtain the weight vector for doing the weighted average of feature differences
		# The more features in each one of the four previous arrays -> the less weight each feature has
		# Note: this weith vector is the same for all the initial states of the same domain
		weights_num_objs_each_type = [1/num_objs_each_type.size]*num_objs_each_type.size
		weights_num_atoms_each_pred = [1/num_atoms_each_pred.size]*num_atoms_each_pred.size
		weights_l_mean = [1/l_mean.size]*l_mean.size
		weights_l_std = [1/l_std.size]*l_std.size

		weights = weights_num_objs_each_type + weights_num_atoms_each_pred + weights_l_mean + weights_l_std

		return state_features, weights	

		
	"""
	Auxiliary method used by _add_diversity_reward(), in order to obtain the distances between init_states.

	@list_consistent_inds List with the indexes (rows) of @feature_matrix corresponding to eventual-consistent problems.
	                      If a problem is inconsistent, we assign a distance of 0 from it to any other problem.
	"""
	def _get_distance_matrix(self, feature_matrix, feature_weights, list_consistent_inds):
		epsilon = 1e-6
		num_states, num_features = feature_matrix.shape
		distance_matrix = np.zeros((num_states,num_states), dtype=np.float32)

		for i in range(num_states):
			for j in range(i+1, num_states):
				curr_features_diff = np.abs(feature_matrix[i] - feature_matrix[j]) / (feature_matrix[i] + feature_matrix[j] + epsilon)

				# Don't use feature weights
				distance_matrix[i, j] = distance_matrix[j, i] = np.average(curr_features_diff, weights=None) \
																if (i in list_consistent_inds and j in list_consistent_inds) else 0.0

				# --- Use feature weights ---
				#distance_matrix[i, j] = distance_matrix[j, i] = np.average(curr_features_diff, weights=feature_weights) \
				#												if (i in list_consistent_inds and j in list_consistent_inds) else 0.0

				# OLD
				#distance_matrix[i, j] = distance_matrix[j, i] = np.mean(np.abs(feature_matrix[i] - feature_matrix[j]) / (feature_matrix[i] + feature_matrix[j] + epsilon)) \
				#												if (i in list_consistent_inds and j in list_consistent_inds) else 0.0

		return distance_matrix


	"""
	This method adds an additional reward to init_policy_trajectories, given by their diversity.
	For each trajectory in @init_policy_trajectories, it calculates its diversity, i.e., average distance,
	to the other trajectories in @init_policy_trajectories. This distance is then added as an auxiliary
	reward to every sample in the trajectory (this is equivalent to adding a reward to the last sample
	of the trajectory and obtaining the total reward with a discount_factor of 1).
	In order to calculate the distance, we obtain a set of features (corresponding to the
	number and type of objects and atoms) for the last sample of each trajectory.
	These features are then used to compute the distances between trajectories.

	@init_policy_trajectories_lens List containing, at each position i, the length of the trajectory number i
								   in @init_policy_trajectories
	@allowed_object_types A list with the object types that we can find in the initial state. If None, we assume
						it is equal to @init_state.types 

	<Note>: init_policy_trajectories is modified in-place
	"""
	# <CAMBIADO>
	def _add_diversity_reward(self, init_policy_trajectories, init_policy_trajectories_lens, allowed_object_types=None):
		# Obtain the indexes which delimit each individual trajectory in init_policy_trajectories
		list_delims = [sum(init_policy_trajectories_lens[:i+1]) for i in range(len(init_policy_trajectories_lens))]

		# Obtain the initial_state of the last sample of each trajectory
		last_sample_list = [init_policy_trajectories[i-1] for i in list_delims]
		init_state_list = [sample[0] for sample in last_sample_list]

		# Obtain which trajectories are eventual-consistent (the last sample in the trajectory has r_eventual=0)
		
		# OLD -> only use consistent trajectories
		# consistent_inds = [i for i, sample in enumerate(last_sample_list) if sample[-5] == 0]
		# NEW -> use all trajectories
		consistent_inds = list(range(len(last_sample_list))) # A list from 0 to num_trajectories-1		
		num_consistent_trajectories = len(consistent_inds)

		# For each init_state in init_state_list, obtain its associated features
		init_state_features_and_weights = [self._obtain_dist_features(init_state, allowed_object_types) for init_state in init_state_list]
		feature_matrix = np.array([x[0] for x in init_state_features_and_weights], dtype=np.float32)
		feature_weights = init_state_features_and_weights[0][1] # We obtain the feature weights for the same init_state, since it is the same for all of them	
		
		# Obtain the distance matrix between pairs of init_states, according to init_state_feature_vectors
		# --- Right now, we don't use featur weights since the features associated with how objects of each type
		# relate to each other is a lot sparser than the other features ---
		distance_matrix = self._get_distance_matrix(feature_matrix, feature_weights, consistent_inds)

		# Given the distance_matrix, obtain the diversity score of each state when compared to the rest
		# Diversity_score = mean distance of the state with the rest
		# Also, rescale the diversity scores according to self._diversity_rescale_factor
		if num_consistent_trajectories > 0:
			diversity_scores = [ (np.sum(distance_matrix[i,:])/num_consistent_trajectories)*self._diversity_rescale_factor for i in range(len(init_state_list))]
		else:
			diversity_scores = [0.0 for i in range(len(init_state_list))]

		# <For each init_state trajectory, add to each sample of the trajectory the corresponding diversity_score>

		l_begin = [0] + list_delims[:-1]
		l_end = list_delims

		# Iterate over the intervals (begin, end) which split the trajectories in init_policy_trajectories
		# and the associated diversity_scores (score)
		for begin, end, score in zip(l_begin, l_end, diversity_scores): 
			for i in range(begin, end): # Iterate over all the samples associated with the current init_state_policy trajectory
				init_policy_trajectories[i][-2] += score


	# ------- Main Methods --------


	"""
	Obtains a set of trajectories correponding to the generation of a set of problem initial states. To obtain these trajectories,
	either the initial state generation policies is used or they are obtained at random.
	This method returns a tuple (problem, trajectory), where "problem" contains the problem corresponding to the state in the last
	trajectory sample.
	"""
	# <CAMBIADO>
	def _obtain_trajectories_init_policy(self, num_trajectories, list_max_atoms_init_state, list_max_actions_init_state, verbose=False):
		pass




	# TODO
	"""
	This method trains both the initial and goal generation policies end-to-end.

	@training_iterations The number of PPO iterations
	@start_it At which iteration start training. Used when we load a model checkpoint and resume training. In this case,
			  @start_it should be set to the training_it the model was at when it was saved. Otherwise, it should be set to 0.
	@epochs_per_train_it For each PPO iteration, how many training epochs to use over the dataset of collected trajectories
	@trajectories_per_train_it For each PPO iteration, how many trajectories to collect
	@minibatch_size Minibatch size to use when training the model over the collected trajectories
	@its_per_model_checkpoint Every this number of train its, the current model (Actor and Critic NLMs) weights are saved to the folder 
							  given by @checkpoint_folder. If it is -1, we do not save checkpoint.
	Note: We add an index to the folder name given by @checkpoint_folder. Example: saved_models/both_policies_2
		  (in case there are two other experiments ids=0, 1 before it).
	@max_atoms_init_state The maximum number of atoms the initial state can have. If we reach this number and the termination condition hasn't
						  been executed, we end the initial state generation phase and check the eventual consistency rules.
	@max_actions_init_state The maximum number of actions (atoms) (invalid or not) that can be tried in the current trajectory. 
							If we reach this number of actions and the initial state hasn't been generated yet, we check the eventual consistency
							rules and apply the penalization (if needed).
	@max_actions_goal_state The maximum number of actions the goal policy can apply from @initial_state. If we reach this
							number of actions and the goal policy hasn't chosen the termination condition, we assume
							the current state corresponds to the completely-generated goal state.
	Note: The real value of @max_actions_init_state and @max_actions_goal_state is obtained by multiplying each parameter by @max_atoms_init_state.
	                        For example, if max_atoms_init_state=15 and max_actions_goal_state=2, then we execute a maximum of 30 actions to generate
				            problem goals.
	"""
	def train_generative_policies(self, training_iterations, start_it=0, epochs_per_train_it=1, trajectories_per_train_it=25, minibatch_size=75,
								  its_per_model_checkpoint=10, checkpoint_folder="saved_models/both_policies", logs_name="both_policies",
								  max_atoms_init_state=15, max_actions_init_state=1.0, max_actions_goal_state=2.0):
		pass