> <lr = 1e-3> (antes usaba lr=5e-4), max_atoms_init_state=20, max_actions_init_state=60, max_actions_goal_state=20:
	Aprende bien (el entrenamiento no es inestable por aumentar el lr).
	La r_continuous y r_eventual convergen a 0, mientras que la r_difficulty (de la goal_policy y rescale_coeff=0.2)
	llega a 2.5. La r_difficulty sigue aumentando, pero el entrenamiento se ralentiza muchísimo debido
	a que calcular la dificultad de los problemas (con el planner), se vuelve muy lento.
	La entropía de la init_policy es alta (los problemas deberían ser diversos).

	------ Problemas generados (both_policies_49):
		> its=240 -> avg_diff = <1.063.733> (los problemas son muy difíciles) y la diversidad es media


> lr=1e-3, <max_atoms_init_state=10, max_actions_init_state=30, max_actions_goal_state=10>:
	Aprende bien (r_continuous y r_eventual convergen a 0). R_difficulty (goal_policy, rescale_factor=0.2)
	llega hasta 1.5. La entropía de la política está bien aunque quizás disminuye un poco demasiado rápido.
	
	------ Problemas generados (both_policies_50):

		>>> max_atoms_init_state=10, max_actions_init_state=30, max_actions_goal_state=10:
			> its=450 -> avg. diff = 718, los problemas son muy diversos

		>>> max_atoms_init_state=20, max_actions_init_state=60, max_actions_goal_state=20:
			> its=450 -> avg. diff = <2.292.061>, los problemas son bastante diversos

	> PARECE QUE ES CAPAZ DE GENERALIZAR A UN NÚMERO DE ÁTOMOS Y GOAL_ACTIONS DISTINTO DEL USADO DURANTE
	EL ENTRENAMIENTO!! Aunque haya entrenado el modelo con max_atoms_init_state=10, max_actions_init_state=30, max_actions_goal_state=10,
	al generar problemas con max_atoms_init_state=20, max_actions_init_state=60, max_actions_goal_state=20,
	la dificultad es muy alta!!! (de hecho es el doble que cuando entreno al modelo directamente sobre este tamaño
	de problemas!! (aunque creo que esto pasa porque el entrenamiento se ralentiza cuando los problemas empiezan a ser muy grandes)).

> lr = 1e-3, max_atoms_init_state=10, max_actions_init_state=30, max_actions_goal_state=10,
  <planner_search_options = 'astar(lmcut())'>:
	Aprende bien (la r_continuous y r_eventual convergen a 0) y la r_difficulty (de la goal_policy, con
	rescale_factor=0.2 y usando search_options='astar(lmcut())') llega a 0.7. La gráfica de la policy_entropy
	es igual que usando planner_search_options = 'astar(blind())'.

	------ Problemas generados (both_policies_51):
		> its=400 -> avg diff. 19, la diversidad es bastante baja
		> its=500 -> avg diff. 28, la diversidad es bastante baja

	Parece que puede generar problemas más o menos difíciles, pero la diversidad es baja.


> lr = 1e-3, max_atoms_init_state=10, max_actions_init_state=30, max_actions_goal_state=10,
  planner_search_options = 'astar(lmcut())', <parallel_NLM>:
	Mismos resultados que con la NLM que recibía cada sample por separado en vez del batch entero.
	Reduce los tiempos de ejecución respecto a la NLM secuencial!
	No obstante, conforme aumenta el núm de its se reduce la diferencia (creo que es porque,
	al generar problemas grandes, el planificador tarda más en resolverlos). -> <el bottleneck
    es el método para calcular la dificultad de los problemas!!!!>

	------ Problemas generados (both_policies_52):
		> its=400 -> avg diff. 35.5, la diversidad es media
	

> lr = 1e-3, max_atoms_init_state=10, max_actions_init_state=30, max_actions_goal_state=10,
  planner_search_options = 'astar(lmcut())', parallel_NLM,
  <state_validator with handempty in state>:
	Las gráficas de entrenamiento son muy parecidas a cuando permito (en el state_validator)
	que el estado inicial tenga un átomo de tipo handempty() o holding(X).
	
	------ Problemas generados (both_policies_53):
		> its=260 -> avg diff. 9.8, la diversidad es media tirando a baja
		<Nota>: paré el entrenamiento a mitad (si el entrenamiento continua, la dificultad sigue aumentando)

	Es capaz de generar problemas con el átomo handempty() en el estado inicial! Creo que cuando se le da
	la posibilidad de generar problemas con handempty() o holding(X) prefiere generar problemas con holding(X)
	ya que eso permite que el estado inicial tenga un objeto más (X), lo que permite generar problemas más complejos.

	<Quizás debería cambiar cómo calculo la entropía de la política para que la política seleccione átomos de distintos
	tipos (en este caso, que algunas veces escoja handempty() y otras holding(X)). Para ello, debería usar una fórmula
	parecida a la lifted_action_entropy, pero esta vez calculando las probabilidades por cada acción (y no cada ariedad)
	por separado.>


> lr = 1e-3, <max_atoms_init_state=30, max_actions_init_state=90, max_actions_goal_state=30>,
  planner_search_options = 'astar(lmcut())', parallel_NLM,
  <state_validator normal>:
	Las gráficas de recompensa son idénticas a cuando uso max_atoms_init_state=10, max_actions_init_state=30, max_actions_goal_state=10.
	La dificultad va aumentando pero, conforme aumenta, el entrenamiento es cada vez más lento (al tardar el planner cada vez más
	en resolver los problemas para calcular su dificultad).

	------ Problemas generados (both_policies_54, its=260):
	Evalúo la diversidad y dificultad de los problemas de distinto tamaño.

	> max_atoms_init_state=30, max_actions_init_state=90, max_actions_goal_state=30:
		Avg. diff = 4453, diversidad muy alta

	> max_atoms_init_state=50, max_actions_init_state=150, max_actions_goal_state=50:
		Avg. diff = 44344, diversidad alta

	Es capaz de generar problemas con un mayor número de objetos/átomos sobre los que fue entrenado!!!!
	No obstante, parece que nunca suele generar problemas con el máximo número de átomos posibles (ejecuta la condición
	de parada antes). Creo que esto es porque el entrenamiento no terminó (la dificultad seguía aumentando pero tuve que pararlo
	porque ya tardaba mucho en obtener la dificultad y el entrenamiento se ralentizó). Si no, podría probar a quitar
	la condición de parada.





> ---- Pruebas lifted action entropy

# CAMBIAR EN _policy_entropy "return tensor_lifted_entropy" POR "return tensor_ground_entropy + tensor_lifted_entropy"



> lr = 1e-3, <max_atoms_init_state=10, max_actions_init_state=30, max_actions_goal_state=10>,
  planner_search_options = 'astar(lmcut())', parallel_NLM: 


	
# LA ÚLTIMA EJECUCIÓN ES TENIENDO EN CUENTA LA TERMINATION CONDITION PARA CALCULAR LA GROUND ENTROPY
# PROBAR AHORA A REPETIR LA EJECUCIÓN PERO SIN TENER EN CUENTA LA TERMINATION CONDITION PARA CALCULAR LA GROUND ENTROPY -> Deja de funcionar

# Ejecución -> solo tensor lifted entropy -> La consistencia continua aumenta pero la eventual no (la termination condition prob es demasiado alta)

# Ejecución -> ignoramos la termination condition prob para calcular la lifted action entropy
  
# Ejecución -> solo ignoramos la termination condition para la lifted action entropy. Usamos lifted+ground action entropy
  Logs: \init_policy\version_9
  La termination condition prob es del 15% aprox. con lo que se generan problemas muy fáciles!!!

# Ejecución -> solo ground action entropy, sin ignorar termination condition
  La reward_consistency aumenta y la termination condition prob. se mantiene alrededor de 0.04.

# Ejecución -> return 0.9*tensor_ground_entropy + 0.1*tensor_lifted_entropy
	La termination condition probability primero sube a 0.12 y después termina bajando a 0.08, pero tarda 9 horas!
	Problemas generados de alta dificultad! (66.6 de media mientras que el modelo anterior con 10 átomos era de 35.5)
	No obstante, tienen muy poca diversidad y nunca generan un problema con handempty() en el estado inicial!
	Al principio del entrenamiento (100 its) sí genera problemas con handempty() (aunque de muy poca dificultad), pero
	parece que aprende a solo generar problemas con holding.

# Ejecución -> 0.5*lifted+0.5*action ground entropy sin ignorar termination condition para ninguna de las dos entropías.
	Termination cond prob alrededor del 15%, los problemas generados son muy sencillos y todos usan handempty() en vez de holding()!!
	La diversidad de los problemas es también muy baja.

# Ejecución -> 0.9*tensor_ground_entropy + 0.1*tensor_lifted_entropy sin ignorar term condition prob.
	Mismas gráficas que cuando se ignora el term condition prob.

# Ejecución -> 0.99*tensor_ground_entropy + 0.01*tensor_lifted_entropy sin ignorar term condition prob.
	Mismas gráficas que con 0.9*ground + 0.1*lifted

# Ejecución -> 0.999*tensor_ground_entropy + 0.001*tensor_lifted_entropy sin ignorar term condition prob.
	Mismas gráficas que con 0.99*ground + 0.01*lifted

# Ejecución -> 1*tensor_ground_entropy + 0*tensor_lifted_entropy sin ignorar term condition prob.
	Mismas gráficas que con 0.999*ground + 0.011*lifted! (la term cond prob sube a 0.14 y después baja a 0.1)

# Ejecución -> solo tensor_ground_entropy sin ignorar term cond prob.
	Mismas gráficas que con 1*tensor_ground_entropy + 0*tensor_lifted_entropy

# Ejecución -> <IMPLEMENTACIÓN PREVIA> solo tensor_ground_entropy sin ignorar term cond prob.
	La term cond prob llega hasta 0.14 tras 5h de ejecución y después va disminuyendo

	Diff (its=420) - avg. diff 28.9 - diversidad media -> Funciona! (la avg. diff random es 18.7)
	<<Probar a generar problemas de distinto tamaño>> -> both_policies_75
	>>> Problemas generados
	<max_atoms_init_state, max_actions_init_state, max_actions_goal_state>
	
	- <10,30,10> -> avg. diff 42 - diversidad media
	- <20,60,20> -> avg. diff 516.3 -> NOTA: LOS PROBLEMAS SON MUY SENCILLOS (esto es porque la mayoría tienen mucho menos de 20 átomos -> deja de generar el problema demasiado rápido!!!)

# Ejecución -> <NUEVA IMPLEMENTACIÓN> solo tensor_ground_entropy sin ignorar term cond prob., 20 atoms&actions
	El entrenamiento tarda 3 veces más que usando 10atoms (tuvimos que pararlo a mitad)	
	
	>>> Problemas generados -> both_policies_76
	<max_atoms_init_state, max_actions_init_state, max_actions_goal_state>

	- <10, 30, 10> -> avg. diff 20 - diversidad muy alta
	- <20, 60, 20> -> avg. diff 1151.8 - diversidad media
	- <30, 90, 30> -> avg. diff 6460.4 - diversidad alta -> NOTA: PARA MUCHOS PROBLEMAS DEJA DE GENERAR EL ESTADO INICIAL ANTES DE LA CUENTA (USA MUCHOS MENOS DE 30 ÁTOMOS)


# Ejecución -> <NUEVA IMPLEMENTACIÓN> solo tensor_ground_entropy sin ignorar term cond prob.
  <entropy_coeff_init_state_policy = 2, entropy_annealing_coeffs_init_state_policy = (<150>, 0.1)>
	La recompensa tarda lo mismo en empezar a aumentar y la r_difficulty no pasa de 0.4 (es menor
	que cuando uso entropy_annealing_coeffs_init_state_policy = (300, 0.1)).
	BAJAR LA ENTROPÍA NO AYUDA A ENTRENAR MÁS RÁPIDO!!!

	Si genero problemas, la dificultad media es 10 -> Tienen muy poca dificultad!

# Ejecución -> <NUEVA IMPLEMENTACIÓN> solo tensor_ground_entropy sin ignorar term cond prob.
	Valores previous de entropía (2, 300, 0.1)
	10 átomos y goal actions
	<planner_search_options='ehc(ff())'>
	
	Tarda más, al menos al principio del entrenamiento (paré el entrenamiento cuando llevaba 30 steps porque tardaba más que con astar(lmcut()))

# Ejecución -> <NUEVA IMPLEMENTACIÓN> solo tensor_ground_entropy sin ignorar term cond prob.
	10 átomos y goal actions
	<planner_search_options='lazy_greedy([lmcut()])'>

	Tarda más, al menos al principio del entrenamiento (paré el entrenamiento cuando llevaba 10 steps porque tardaba más que con astar(lmcut()))

# Ejecución -> <NUEVA IMPLEMENTACIÓN> solo tensor_ground_entropy sin ignorar term cond prob.
	10 átomos y goal actions
	<planner_search_options='eager(single(lmcut()))'>

	<El entrenamiento tarda lo mismo que usando astar(lmcut())!!>
	Paré el entrenamiento antes de que terminara (la recompensa aún seguía aumentando)

	>> Generación de problemas con el modelo entrenado -> diff calculada con eager(single(lmcut())
		- 10 atoms&actions - avg. diff - 19.4 -> los problemas son pequeños (la term cond se ejecuta antes de tiempo) y todos tienen holding!
												 <Paré el entrenamiento antes de tiempo -> 5h no son suficientes>
	
	>> Generación de problemas con el random generator -> diff calculada con eager(single(lmcut())
		- 10 atoms&actions - avg. diff - 17.2

# Ejecución -> <NUEVA IMPLEMENTACIÓN> solo tensor_ground_entropy sin ignorar term cond prob.
	10 átomos y goal actions
	<planner_search_options= --alias lama-first>

	> Entrenamiento:
		- No es más rápido que usando planner=astar(lmcut())! -> creo que al ser los problemas pequeños da igual el algoritmo de búsqueda
		- La recompensa tarda unas 11h en llegar a su punto más alto
		- La term cond prob se estabiliza a 0.1 (sube al principio del entrenamiento y después baja)

	> Problemas (diff medida con planner_search_options=--alias lama-first)
		> directed generator (modelo entrenado) -> its=620 (aunque use un modelo con its menor la diversidad de los problema sigue siendo baja)
			- 10 atoms&actions - avg. diff = 40.6 - diversidad muy baja
			- 30 atoms&actions - avg. diff = 154.8 - diversidad media -> genera problemas con más de 10 átomos, pero lo máximo son 16 (no genera con 30 átomos en ningún momento)

		> random generator
			- 10 atoms&actions - avg. diff = 12.2
			- 30 atoms&actions - avg. diff = 55.8

# Ejecución -> <NUEVA IMPLEMENTACIÓN> solo tensor_ground_entropy sin ignorar term cond prob.
	<20 átomos y goal actions>
	<planner_search_options= --alias lama-first>

	> Entrenamiento:
		- Conforme se van generando problemas de mayor dificultad, <el entrenamiento se va ralentizando, pero no tanto como cuando usaba astar(lmcut())>!
		  -> de la it 0 a 100 se tardan 210 min, de 100-200 unos 250 min y de 200-300 unos 388 min -> el entrenamiento se ralentiza casi a la mitad, pero no más
		- La term cond prob sube hasta alcanzar un pico en 0.11 y después baja poco a poco hasta 0.06
		- Las gráficas de recompensa (r_eventual, r_continous y r_difficulty) van subiendo progresivamente desde el inicio del entrenamiento!!!
		  -> son unas buenas gráficas, que muestran que la NLM aprende sin problemas
		- La init_state_ policy entropy disminuye hasta 0.2, mientras que la goal_policy_entropy disminuye hasta 0.2!!! -> Creo que
		  <la policy entropy termina siendo demasiado baja, con lo que debería aumentar los entropy_coeffs!!>

	> Problemas (diff medida con planner_search_options=--alias lama-first)
		> directed generator (modelo entrenado) -> its=300 (si uso 360 its los problemas son casi todos idénticos!!)
			- 10 atoms&actions - diff = 29.1 - diversidad media-baja
			- 20 atoms&actions - diff = 135.7 - diversidad media
			- 30 atoms&actions - diff = 414.6 - diversidad media-baja -> NINGÚN PROBLEMA TIENE MÁS DE 20 ÁTOMOS (a pesar de que pueden tener hasta 30!)

		> random generator
			- 10 atoms&actions - diff = 12.2
			- 20 atoms&actions - diff = 36.1
			- 30 atoms&actions - diff = 55.8 

# Ejecución -> ground_entropy*0.5 + lifted_entropy*0.5, sin ignorar term cond prob
	20 átomos y goal actions
	planner_search_options= --alias lama-first

	- Carpeta: both_policies_83 - logs: init_policy\ version_33, goal_policy\ version_16

	> Entrenamiento:
		Le cuesta aprender al principio, ya que la term cond prob aumenta hasta 0.2. No obstante, en poco tiempo empieza disminuir
		y aprende a generar problemas con r_eventual=0 y r_consistency=0. Al final del entrenamiento la term cond prob
		es 0.05, menor que cuando solo uso ground_entropy (en el experimento anterior termina en 0.07)!

	> Problemas
		<Genera problemas más difíciles y diversos que cuando solo uso ground_entropy!!! También generaliza mejor a problemas más grandes!
		(los problemas tienen más átomos)>
		> directed generator -> its=600
			- 10 atoms&actions - diff = 25.5 - diversidad media-alta
			- 20 atoms&actions - diff = 217.4 - diversidad media-alta -> Genera problemas con un número alto de átomos!!
			- 30 atoms&actions - diff = 810.2 - diversidad media-alta -> Genera problemas con hasta 27 átomos, aunque la mayoría rondan los 20.
		<AHORA LOS PROBLEMAS SOLO TIENEN HANDEMPTY, Y NINGUNO TIENE HOLDING!!>

	<ES MEJOR USAR LIFTED Y GROUND ENTROPY QUE SOLO GROUND ENTROPY!!>


# Ejecución -> ground_entropy*0.5 + lifted_entropy*0.5, sin ignorar term cond prob
	20 átomos y goal actions
	planner_search_options= --alias lama-first
	<NLM without preds arity 3>

	- Logs: los eliminé, así como los modelos guardados

	> Entrenamiento:
		- << El tiempo de entrenamiento es la mitad que usando predicados de ariedad 3!!! >>
		- La term cond prob primero sube, después cae en picado y después vuelve a subir y se estabiliza en 0.07
		- La r_difficulty llega hasta casi la misma que al usar NLM con predicados de ariedad 3
		- La init_policy_entropy cae demasiado rápido y se estabiliza en un valor demasiado bajo, 0.15!


# Ejecución -> ground_entropy*0.5 + lifted_entropy*0.5, sin ignorar term cond prob
	20 átomos y goal actions
	<difficulty = initial heuristic value for ff()>
	NLM without preds arity 3

	- Logs: init_policy/ version_34

	> Entrenamiento:
		- Alcanza el pico en r_difficulty tras 5h de entrenamiento, y a partir de ahí se mantiene constante
		- <<Respecto al tiempo de entrenamiento, parece que casi no hay diferencia entre usar lama-first y calcular la heurística ff()!!!>>

	> Problemas
		<Los problemas generados son muy sencillos!!! (aún más que siendo generados al azar) ->
		 Solo medir la dificultad con la heurística ff() no es un buen método!!!>
		> directed generator -> its=380 
			- 20 atoms&actions - diff = 17.8


# Ejecución -> ground_entropy*0.5 + lifted_entropy*0.5, sin ignorar term cond prob
	20 átomos y goal actions
	difficulty = initial heuristic value for ff()
	NLM without preds arity 3
	write_logs every 10 training its (calls to trainer.fit())

	El tiempo de entrenamiento es el mismo.
	<Escribir los logs en cada train it no es el bottleneck!>


# Ejecución ->ground_entropy*0.5 + lifted_entropy*0.5, sin ignorar term cond prob
	20 átomos y goal actions
	difficulty = initial heuristic value for ff()
	NLM without preds arity 3
	<planner on RAM disk and don't open PDDL problem each time>

	Los tiempos de entrenamiento mejoran, aunque es difícil decir cuánto exactamente.


# Ejecución -> ground_entropy*0.5 + lifted_entropy*0.5, sin ignorar term cond prob
	20 átomos y goal actions
	difficulty = initial heuristic value for ff()
	NLM without preds arity 3
	<planner on SDD and don't open PDDL problem each time>
	(same as last experiment, but now we don't use the RAM disk)

	Creo que los tiempos son muy parecidos, use un RAM disk o no. -> no es necesario usar RAM disk

# Ejecución -> ground_entropy*0.5 + lifted_entropy*0.5, sin ignorar term cond prob
	20 átomos y goal actions
	difficulty = initial heuristic value for ff()
	NLM without preds arity 3
	planner on SDD and don't open PDDL problem each time
	<evaluate three heuristics for each problem -> planner_search_options='eager_greedy([ff(), lmcut(), hm(m=1)], bound=0)'>

	Se ralentiza un poco el entrenamiento. En problemas difíciles, se pasa de tardar 4h en realizar 100 train its,
	a tardar 4h 40 min en hacer 100 train its. El entrenamiento se vuelve un 15% más lento aprox.
	
# Ejecución -> ground_entropy*0.5 + lifted_entropy*0.5, sin ignorar term cond prob
	20 átomos y goal actions
	<planner_search_options= --alias lama-first>
	NLM without preds arity 3

	- logs: init_policy\ version_38

	> Entrenamiento
		- Term cond prob sube al principio hasta 0.2 y después baja hasta 0.07
		- r_eventual y r_continuous convergen a 0 tras hora y media de entrenamiento
		- La r_difficulty (con np.log y rescale_factor=0.2) llega hasta 0.67 tras 11h de entrenamiento (seguía aumentando pero paré el entrenamiento)
		- La init_state_policy_entropy baja hasta 0.22

	> Problemas (its=680)
		- 20 atoms&actions - diff = 98.1 - diversidad media-baja - problemas con 15 átomos de media

# Ejecución -> ground_entropy*0.5 + lifted_entropy*0.5, sin ignorar term cond prob
	20 átomos y goal actions
	planner_search_options= --alias lama-first
	NLM without preds arity 3
	<no np.log() to rescale problem difficulty>

	- logs: init_policy\ version_39

	> Entrenamiento
		- Term con prob sube al principio hasta 0.2 y después baja hasta 0.02 (mucho menor que cuando uso np.log() para la dificultad (ver experimento anterior))
		- r_eventual converge a 0 pero r_continuous converge a -0.17 (peor r_continuous que cuando uso np.log())
		- r_difficulty llega hasta 26 (sin np.log y con rescale_factor=0.2) tras 13h de entrenamiento -> MEJOR DIFFICULTY QUE CUANDO USO NP.LOG!!
		  (si uso np.log() la r_difficulty sería equivalente a 0.97, mayor que la 0.67 del experimento anterior)
		- La init_state_policy_entropy baja hasta 0.35 -> TIENE MEJOR (MAYOR) ENTROPÍA QUE CUANDO USO NP.LOG!!

	<Las gráficas de entrenamiento son bastante mejores que cuando uso np.log, excepto por la r_continuous que no converge a 0>

	> Problemas (its=650)
		- 20 atoms&actions - diff = 162.7 - diversidad muy baja (solo un átomo on() por problema) - muchos problems tienen 20 átomos!!

	<CREO QUE ES MEJOR <NO> USAR NP.LOG, YA QUE LOS PROBLEMAS TIENEN MAYOR DIFICULTAD Y MÁS ÁTOMOS DE MEDIA.>
	No obstante, su diversidad es bastante menor -> hay que disminuir el rescale_factor para la dificultad y/o aumentar los entropy coeffs.


# Ejecución -> ground_entropy*0.5 + lifted_entropy*0.5, sin ignorar term cond prob
	20 átomos y goal actions
	planner_search_options= --alias lama-first
	NLM without preds arity 3
	no np.log() to rescale problem difficulty
	<NLM with object types>
	(Mismo experimento que el anterior pero ahora la NLM codifica los object types)

	Funciona perfectamente! (se obtienen las mismas gráficas de entrenamiento que en el experimento anterior)


# Ejecución -> ground_entropy*0.5 + lifted_entropy*0.5, sin ignorar term cond prob
	20 átomos y goal actions
	planner_search_options= --alias lama-first
	NLM without preds arity 3
	no np.log() to rescale problem difficulty
	<rescale_factor=0.02 for difficulty>

	- logs: init_policy\ version_40

	> Entrenamiento:
		- Term cond prob es más inestable que cuando uso rescale_factor=0.2: sube hasta 0.25, baja en picado hasta 0.02, sube otra vez
		  y después baja lentamente hasta 0.04 (la term cond prob termina más alta que con rescale_factor=0.2)
		- r_eventual y r_continuous convergen a 0 (mientras que con rescale_factor=0.2 r_continuous converge a -0.17)
		- r_difficulty llega hasta 2 -> si uso rescale_factor=0.2 sería igual a 20, mientras que en el experimento con rescale_factor=0.2
		  llegaba hasta 26 
		- la init_state_policy_entropy baja hasta 0.17 (menos que con rescale_factor=0.2)
		- El tiempo de entrenamiento es el mismo que usando rescale_factor=0.2

	> Problemas (its=580):
		- 20 atoms&actions - diff = 126.0 - diversidad media-baja (aunque mucho mejor que usando rescale_factor=0.2)
		                                    Los problemas tienen un alto número de átomos!

	<Es mejor usar rescale_factor=0.02 (como en este experimento) que rescale_factor=0.2>


# >> Cambiamos _calculate_state_values_trajectory para que no se vuelva a llamar a la NLM -> se reduce el tiempo de entrenamiento!







# >>>>> Mejor hasta la fecha

# Ejecución -> ground_entropy*0.5 + lifted_entropy*0.5, sin ignorar term cond prob
	20 átomos y goal actions
	planner_search_options= --alias lama-first
	NLM without preds arity 3
	no np.log() to rescale problem difficulty
	rescale_factor=0.02 for difficulty
	<entropy_annealing_coeffs_init_state_policy = (600, 0.2), entropy_annealing_coeffs_goal_policy = (300, 0.2)>

	- logs: init_policy\ version_41, goal_policy\ version_23
	- saved_models_96

	> Entrenamiento (comparación con experimento anterior con menores entropy coeffs)
		- Term cond prob baja hasta 0.06 (un valor un poco más alto que el experimento anterior) y la gráfica es más estable
		- La r_eventual y r_continuous convergen a 0
		- La r_difficulty llega hasta 2 (al igual que en el experimento anterior), aunque tarda 100 its más
		- La init_state_policy_entropy baja más lentamente, hasta 0.26 (en el experimento anterior baja hasta 0.17) ->
		  Tiene más entropía!
		- La goal_policy_entropy baja hasta 0.09 (en el experimento anterior baja hasta 0.05) -> Tiene más entropía, pero sigue siendo poca
		- Tiempo de entrenamiento: 9h

	> Problemas (its=680):
		- 10 atoms&actions - diff = 36.5
		- 20 atoms&actions - diff = 160 - diversidad media - los problemas tienen un número de átomos variable, algunos tienen casi 20!
		                                  La mayoría de problemas tienen una sola torre, aunque hay algunos con dos torres
									      Todos los estados iniciales tienen holding() y ninguno handempty()
										  Los objetivos tienen diversidad media (y algunos tienen holding como handempty)
		- 30 atoms&actions - diff = 331 - Ningún problema generado se acerca a 30 átomos: el que más tiene es 25 y la mayoría tienen alrededor
		                                  de 20 átomos
		- 50 atoms&actions - diff = 688.2 - Ningún problema generado se acerca a 50 átomos, pero sí hay problemas con alrededor de 30 átomos!
		- 70 atoms&actions - diff = 1177.7 - Se generan problemas de hasta 44 átomos (la mayoría rondan los 30)
		
		
	<Genera problemas más difíciles y diversos que en el experimento anterior -> es mejor usar un alto valor de entropía!>
	<No obstante, no generaliza bien a problemas más grandes y la diversidad de los problemas, sobretodo de los objetivos debe mejorar aún.
	Además, ningún init state tiene handempty, sino que todos tienen holding>

	<<ES POSIBLE GENERAR PROBLEMAS CON UN MAYOR NÚMERO DE ÁTOMOS SI max_atoms_init_state LO PONGO MAYOR AL NÚMERO DE ÁTOMOS QUE QUIERO GENERAR
	  (ej.: para generar problemas con 50 átomos, ponerlo a 70)>>







# Ejecución -> ground_entropy*0.5 + lifted_entropy*0.5, sin ignorar term cond prob
	20 átomos y goal actions
	planner_search_options= --alias lama-first
	NLM without preds arity 3
	no np.log() to rescale problem difficulty
	rescale_factor=0.02 for difficulty
	<entropy_annealing_coeffs_init_state_policy = (600, 0.5), entropy_annealing_coeffs_goal_policy = (400, 0.5)>

	> Entrenamiento (comparación con experimento anterior)
		- Term cond prob baja más lentamente y hasta 0.08 (en el experimento anterior hasta 0.06)
		- La r_eventual y r_continuous convergen a 0
		- La r_difficulty llega hasta 0.35 tras 9h de entrenamiento! (en el experimento anterior llegaba hasta 2)!!
		- La init_state_policy entropy baja hasta 0.3 (en el experimento anterior bajaba hasta 0.25)
		- La goal_policy entropy casi no disminuye nada durante el entrenamiento!!! -> baja hasta 0.32 mientras que en el experimento
		  anterior bajaba hasta 0.09 -> LA ENTROPÍA DE LA GOAL POLICY ES DEMASIADO ALTA!!!

	> Problemas (its=810)
		- 20 atoms&actions - diff = 26.5 - diversidad alta - LA DIVERSIDAD DE LOS PROBLEMAS ES MUY BUENA PERO LA DIFICULTAD MUY BAJA!!
															 No obstante, todos los problemas tienen holding y ninguno handempty.
					
	<Mejora la diversidad al aumentar la entropía pero los problemas son demasiado sencillos, y tienen unos pocos átomos menos de media
	 que en el experimento anterior>
	<Creo que el problema es la goal_policy, que tiene una entropía demasiado elevada!!!>


# Ejecución -> ground_entropy*0.5 + lifted_entropy*0.5, sin ignorar term cond prob
	20 átomos y goal actions
	planner_search_options= --alias lama-first
	NLM without preds arity 3
	no np.log() to rescale problem difficulty
	rescale_factor=0.02 for difficulty
	<entropy_annealing_coeffs_init_state_policy = (600, 0.5), entropy_annealing_coeffs_goal_policy = (300, 0.3)>

	- logs: init_policy\ version_43, saved_models_98

	> Entrenamiento
		- Mismos resultados que en el experimento anterior menos:
			- La r_difficulty ahora llega hasta 1 (más alta, pero sigue siendo baja)
			- La goal_policy entropy baja hasta 0.18

	> Problemas (its=820)
		- 20 atoms&actions - diff = 79.5 - diversidad alta, aunque todos los problemas tienen holding en el init state y ninguno handempty
										   No obstante, en el estado inicial todos los problemas tienen o 1 o dos torres.
		
	<Mejora la dificultad de los problemas, pero sigue sin ser suficiente>


# Ejecución -> ground_entropy*0.5 + lifted_entropy*0.5, sin ignorar term cond prob
	20 átomos y goal actions
	planner_search_options= --alias lama-first
	NLM without preds arity 3
	no np.log() to rescale problem difficulty
	rescale_factor=0.02 for difficulty
	<entropy_annealing_coeffs_init_state_policy = (600, 0.5), entropy_annealing_coeffs_goal_policy = (500, 0.2)>

	> Entrenamiento
		- La r_difficulty ahora llega hasta 1.3 (más alta, pero sigue sin llegar a 2)
		- La goal_policy entropy baja hasta 0.12 (es más baja que en el experimento anterior)

	> Problemas (its=810)
		- 20 atoms&actions - diff = 70.7 - diversidad alta
		                                  Los problemas tienen pocos átomos!
										  Los objetivos son muy poco diversos! (casi todos tienen ontable(x) donde x era el átomo
										  que estaba en holding(X) en el estado inicial y, a veces, otro ontable, aunque la mayoría
										  de veces el goal es solo apilar todos los bloques en X)

	<La dificultad sigue siendo baja y ahora los goals son muy poco diversos!>

# Ejecución -> ground_entropy*0.5 + lifted_entropy*0.5, sin ignorar term cond prob
	20 átomos y goal actions
	planner_search_options= --alias lama-first
	NLM without preds arity 3
	no np.log() to rescale problem difficulty
	rescale_factor=0.02 for difficulty
	<entropy_annealing_coeffs_init_state_policy = (600, 0.3), entropy_annealing_coeffs_goal_policy = (300, 0.3)>

	- logs: init_policy\ version_45, saved_models_100

	> Entrenamiento
		(comparación con entropy_annealing_coeffs_init_state_policy = (600, 0.2), entropy_annealing_coeffs_goal_policy = (300, 0.2))
		- La r_difficulty sube hasta 1.4, mientras que en el experimento anterior sube hasta 2
		- La entropía de la init_state_policy es ligeramente mayor (0.3 vs 0.25) y la de la goal_policy es bastante mayor
		  (0.15 vs 0.08)
		- El entrenamiento tarda un poco más (11h vs 8h30)
		- El resto de cosas son muy similares entre ambos experimentos

	<Parece que a cambio de aumentar bastante la goal_policy entropy y un poco la init_policy entropy, la dificultad disminuye bastante
	 y el entrenamiento se ralentiza un poco>

	> Problemas (its=780)
	    - 10 atoms&actions - diff = 30
		- 20 atoms&actions - diff = 90 -    diversidad media (casi todos los problemas tienen una sola torre en el estado inicial)
											(casi ninguno tiene dos y ninguno tiene tres)
											El número de átomos ronda los 12
											Los objetivos son bastante diversos

# Ejecución -> ground_entropy*0.5 + lifted_entropy*0.5, sin ignorar term cond prob
	20 átomos y goal actions
	planner_search_options= --alias lama-first
	NLM without preds arity 3
	no np.log() to rescale problem difficulty
	rescale_factor=0.02 for difficulty
	<entropy_annealing_coeffs_init_state_policy = (600, 1.0), entropy_annealing_coeffs_goal_policy = (300, 0.25)>

	> Entrenamiento
		- La r difficulty no pasa de 0.4!
		- Tanto la init_state_policy como la goal_policy tienen una mayor entropía que en el experimento anterior
		  (a pesar de solo haber aumentado la entropía de la init_policy)

	> Problemas (its=950)
		- 20 atoms&actions - diff = 28 - diversidad muy alta 
										 Casi todos los problemas tienen holding en el estado inicial (solo dos tienen handempty)
										 Los problemas son muy sencillos, ya que la mayoría tienen muy pocos átomos en el estado inicial!!

	<Se consigue aumentar mucho la diversidad de los problemas, a costa de reducir mucho la dificultad. No obstante, ni así se consiguen
	 problemas con handempty en el estado inicial (solo dos de diez) ni con tres torres (solo tienen o una o dos)>

# Ejecución -> ground_entropy*0.5 + lifted_entropy*0.5, sin ignorar term cond prob
	20 átomos y goal actions
	planner_search_options= --alias lama-first
	NLM without preds arity 3
	no np.log() to rescale problem difficulty
	rescale_factor=0.02 for difficulty
	<entropy_annealing_coeffs_init_state_policy = (600, 0.2), entropy_annealing_coeffs_goal_policy = (300, 0.2)>
	<trajectories_per_train_it=25, minibatch_size=70>

	- logs: init_policy\ version_47

	> Entrenamiento
		- Tarda más (en alcanzar la misma dificultad) que cuando uso un mayor número de trajectories_per_train_it!

	> Problemas (its=1500)
		- 20 atoms&actions - diff = 150.8 - diversidad baja (todos los problemas tienen una única torre en el init_state)

	<Es mejor usar trajectories_per_train_it=50>



-----------------------------

> Comparación directed_generator con random_generator en blocksworld
	- Mi método genera problemas mucho más difíciles (CON EL MISMO NÚMERO DE ÁTOMOS)
		- Ej.: para 50 átomos, el random_generator obtiene dificulty 100 y mi método 1100

	- No obstante, mi método no es más rápido que el random generator
		- Esto es porque las comprobaciones de consistencia son muy rápidas de hacer y lo costoso
		  es generar el objetivo (lo que hacen igual tanto el random como directed generator)

> Consigo generar problemas más o menos diversos y difíciles con las generative policies
  También consigo que generalicen a problemas más grandes aumentando el max_actions_init_state por encima del número
  de átomos que quiero obtener
  No obstante, el método se ralentiza cuando tiene que generar problemas con un gran número de átomos (debido al pddl_parser)

> Cosas a mejorar:
	- La diversidad de los problemas debería ser un poco mayor (el init state generalmente tiene una sola torre y nunca tiene handempty)
	- La generalización a problemas más grandes (tengo que poner max_actions_init_state más alto de lo necesario para que generalice)
	- El tiempo en generar el goal para problemas grandes (el método groundify del pddl_parser es muy lento)