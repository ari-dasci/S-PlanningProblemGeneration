>  <logistics>
   <planners=[lama-first, lazy-greedy, lazy-greedy]>
   max_atoms_init_state=15, max_actions_init_state=1, max_actions_goal_state=2.0

   > logs: init_policy\version_204
   > saved_models: both_policies_257

   > Entrenamiento (its=1.5k)
      - Tiempo: 16h (en DaSCI)
      - r_diff (init_policy)= 3 (seguía aumentando, lo paré a mitad)
      - r_eventual=-0.05
      - init_policy_entropy 0.6, goal_policy_entropy en 0.3 (seguía bajando)
      - term_cond_prob init_policy y goal_policy ambas convergen a 0
      - num_objs:
         - city: 2.8 (y subiendo)
         - airport: 3
         - location: 0.2 (y bajando)
         - airplane: 2.3
         - truck: 3
         - package: 6.3


   > Problemas (its=1.5)
      - max_atoms = 20
         - diff = [68.2 52.9 58.1]
            - problemas con 20 átomos
         - diversidad media-alta
            - la mayoría de problemas no tienen locations
            - problemas con 2, 3, 4 y 5 ciudades
            - el número de paquetes varía moderadamente

      - max_atoms = 30
         - diff = [145.6 96.1 87.4]
            - problemas con 30 átomos
         - diversidad media
            - no aumenta el número de ciudades!!! (siguen estando entre 2 y 5)
            - aumenta sobretodo el número de packages

      - max_atoms = 40
         - diff = [233.2 154.6 153.4]
            - problemas con 40 átomos
         - diversidad media
            - se generan problemas con hasta 7 ciudades
            - aumenta sobretodo el número de packages 

   >> Aunque no terminé el entrenamiento, se puede ver que NeSIG no generaliza bien en logistics a problemas más
      grandes:
         - La dificultad escala mal
         - El número de ciudades escala mal


>  <logistics>
   planners=[lama-first, lazy-greedy, lazy-greedy]
   max_atoms_init_state=15, max_actions_init_state=1, max_actions_goal_state=2.0
   init_policy_entropy_coeffs: 0.1, None
   goal_policy_entropy_coeffs: 0.0, None
   <difficulty_rescale_factor=1>
   <new difficulty normalization>

   > Ejecutándose en DaSCI
   > logs: init_policy/version_206
   > saved_models: both_policies_259

   > Entrenamiento (its=5900)
      - Tiempo (DaSCI): 3d 20h
      - r_diff (goal_policy) = 60
      - r_eventual=-0.01
      - init_policy_entropy 0.6, goal_policy_entropy 0.1
      - term_cond_prob init_policy y goal_policy ambas convergen a 0
      - num_objs:
         - city: 2.1
         - airport: 5.5
         - location: 0.005
         - airplane: 1
         - truck: 2.1 (igual que city)
         - package: 6.2

         - Resumen:
            - Se generan problemas con 2 ciudades, un avión, muchos airports,
              sin locations, un camión por cada ciudad y muchos packages

   > Problemas (its=5000)
      - max_atoms = 15
         - diff = [63.9 53.3 59.9] (muy alta)
            - todos con 15 átomos
         - diversidad baja!!
            - Problemas con 2 y 3 ciudades
            - Entre distintos problemas solo suele variar el número de airports y packages (y esto solo varía un poco)

      - max_atoms = 20
         - diff = [125. 98.6 116.4] (muy alta)
         - Problemas con 2 y 3 ciudades

      - max_atoms = 30  
         - diff = [257.6 221.2 244. ] (medio-alta)
         - Problemas con 3, 4 y 5 ciudades

      - max_atoms = 40
         - diff = [450.7 388.8 454.7] (media)
         - Problemas con hasta 7 ciudades (aunque la media son 5 ciudades)
         - muy parecidos entre sí

   >> Análisis de resultados
      > El número de ciudades sí escala medio-bien pero, a pesar de esto, la dificultad escala mal!!
         <<<Al no haber locations, se pueden llevar los paquetes de un sitio a otro usando solo aviones directamente!! (la goal policy solo usa la acción fly, pero no drive (con trucks))>>>
         - Si quiero que la dificultad escale mejor, creo que tiene que aprender a generar problemas
           donde se usen tanto los aviones como los camiones para llevar paquetes 

            >>> Dos opciones:
               - Entrenar en problemas más grandes
               - Probar a aumentar la entropía de la goal_policy

      > A pesar de usar action_entropy_coeffs=1, los problemas son poco diversos!
         - Creo que la action_entropy no es una buena forma de motivar la diversidad:
           dado que el mismo problema se puede generar de distintas formas (añadiendo los átomos
           en distinto orden), una alta entropía puede resultar en problemas con poca diversidad!!
         
            >>> Debería usar diversity reward!!!


>  <logistics>
   planners=[lama-first, lazy-greedy, lazy-greedy]
   max_atoms_init_state=15, max_actions_init_state=1, max_actions_goal_state=2.0
   <init_policy_entropy_coeffs: 0.01, None>
   goal_policy_entropy_coeffs: 0.0, None
   <difficulty_rescale_factor=1>
   <new difficulty normalization>

   > Ejecutándose en casa
   > logs: init_policy/version_203
   > saved_models: both_policies_260

   > Entrenamiento (its=1800)
      - Tiempo (casa): 14h 15min
      - r_diff (goal_policy)= 40 (seguía aumentando)
      - r_eventual=-0.02
      - init_policy_entropy 0.4, goal_policy_entropy 0.2
      - term_cond_prob init_policy y goal_policy ambas convergen a 0
      - num_objs:
         - city: 3
         - airport: 3
         - location: 0.01
         - airplane: 2.1 (y bajando)
         - truck: 3
         - package: 7 
 
   > Problemas (its=1800)
      - max_atoms = 15
         - diff = [51.9 42.3 39.4]
            - todos con 15 átomos
         - diversidad baja
            - Todos los problemas con 3 ciudades
            - Ningún problema con location!
            - Cada ciudad tiene exactamente un airport!!!
              Se generan problemas donde solo se usan aviones,
              pero no camiones!!

      - max_atoms = 20
         - diff = [90.4 73.6 67.]
            - todos con 20 átomos
         - diversidad media-baja
            - Problemas con 3, 4 y 5 ciudades!

      - max_atoms = 30
         - diff = [202.9 155.4 129.3]
            - todos con 30 átomos
         - diversidad media-baja
            - Problemas con hasta 7 ciudades!

      - max_atoms = 40
         - diff = [455.1 306.6 209.1]
            - todos con 40 átomos
         - diversidad media-baja
            - Problemas con hasta 8 ciudades
               - Parece que el num de ciudades ya deja de escalar bien, lo que más aumenta
                 es el número de paquetes

   >> La dificultad escala regular
   >> La diversidad de los problemas es muy baja!
      >>> Necesito aumentar el action entropy coeff
      >>> 0.01 es demasiado bajo!
   >> Quizás necesito entrenar en problemas más grandes
         - Lo que hace que un problema sea difícil puede depender del tamaño!


>  <logistics>
   planners=[lama-first, lazy-greedy, lazy-greedy]
   max_atoms_init_state=15, max_actions_init_state=1, max_actions_goal_state=2.0
   <init_policy_entropy_coeffs: 1, None>
   goal_policy_entropy_coeffs: 0.0, None
   difficulty_rescale_factor=1
   new difficulty normalization

   > Ejecutándose en casa
   > logs: init_policy/version_204
   > saved_models: both_policies_261

   > Entrenamiento (its=9000)
      - Tiempo (casa): 3d
      - r_diff (goal_policy)= 42
      - r_eventual=-0.02
      - init_policy_entropy=0.85, goal_policy_entropy=0.1
      - term_cond_prob init_policy converge a 0 y la goal_policy_entropy a 0.01
      - num_objs:
         - city: 2
         - airport: 5.5
         - location: 1.3
         - airplane: 1.1
         - truck: 2.4
         - package: 4.7   

      >> El número de objetos es muy parecido
         a cuando se usa init_policy_entropy_coeffs=0.1!!
         La mayor diferencia es que ahora sí se generan
         locations

   > Problemas (its=7500)
      - max_atoms = 15
         - diff = [45.4 39.3 38.9] (alta)
            - todos con 15 átomos
         - diversidad media-alta
            - Todos los problemas con dos ciudades
            - El resto de objetos muy diversos

      - max_atoms = 20
         - diff = [75.4 61.1 72.1] (alta)
         - diversidad media-alta
            - Todos los problemas con dos ciudades
            - El resto de objetos muy diversos

      - max_atoms = 30
         - diff = [171.3 132.4 160.1] (media)
         - diversidad media
            - Todos los problemas con dos ciudades
            - El resto de objetos muy diversos

      - max_atoms = 40
         - diff = [373.3 270.5 353.3] (media-baja)
         - diversidad media
            - Todos los problemas con dos ciudades
            - El resto de objetos muy diversos

   >> Se usa drive para generar los objetivos!!!

   >> La dificultad de los problemas y generalización es peor
      que al usar init_policy_action_entropy=0.1
   >> Ahora, al aumentar el tamaño de problema, no aumenta
      el número de ciudades

   >> init_policy action_coeff es mejor 0.1 que 1!!!
      - No obstante, creo que no es un buen acercamiento para generar problemas diversos
         - Debería usar diversity reward!!!


---- Experimentos para mejorar generalización

>  <logistics>
   planners=[lama-first, lazy-greedy, lazy-greedy]
   max_atoms_init_state=15, max_actions_init_state=1, max_actions_goal_state=2.0
   <init_policy_entropy_coeffs: 0.1, None>
   <goal_policy_entropy_coeffs: 0.1, None>
   difficulty_rescale_factor=1
   new difficulty normalization

   > Ejecutándose en DaSCI
   > logs: init_policy/version_207
   > saved_models: both_policies_262

   > Paré el entrenamiento a mitad, tras 1.6k train its
      - La init y goal policies entropy caen muy rápido!!
      - Se generan problemas con varias ciudades, pero donde cada ciudad
        suele tener un único camión y airport, sin locations! -> Poca diversidad!!


>  <logistics>
   planners=[lama-first, lazy-greedy, lazy-greedy]
   <max_atoms_init_state=20>, max_actions_init_state=1, max_actions_goal_state=2.0
   <init_policy_entropy_coeffs: 0.1, None>
   goal_policy_entropy_coeffs: 0, None
   difficulty_rescale_factor=1
   new difficulty normalization
   <minibatch_size=50>

   > Ejecutándose en casa
   > logs: init_policy/version_205, version_206
   > saved_models: both_policies_263, 265

   > Entrenamiento (its=3400, paré el entrenamiento a mitad)
      - Tiempo (casa): 1d 15h
      - r_diff (goal_policy)= 85 (seguía aumentando)
      - r_eventual=-0.02
      - init_policy_entropy=0.7, goal_policy_entropy=0.1
      - term_cond_prob init_policy y goal_policy convergen a 0
      - num_objs:
         - city: 2.5 (bajando)
         - airport: 5.4
         - location: 0.1
         - airplane: 1
         - truck: 4.4
         - package: 8.8  

      >>> Básicamente los mismos objetos que entrenando en problemas de
          tamaño 15!!!!
          La principal diferencia es que hay más camiones (aunque si hubiera
          entrenado durante más probablemente habría disminuido) y que
          hay más paquetes
          Se generan problemas con pocas ciudades (para el tamaño) y
          sin locations!! 

   > Problemas (its=3400)
      - max_atoms = 15
         - diff = [53.9 42.9 46.6] (alta)
         - diversidad media-alta
            - Problemas con 2, 3 y 4 ciudades
            - Sin locations
            - Los num objs varían bastante

      - max_atoms = 20
         - diff = [95.8 77.4 87.9] (alta)
         - Problemas con 2 y 3 ciudades

      - max_atoms = 30
         - diff = [220.4 170.5 191.6] (media)
         - Problemas con 2, 3 y 4 ciudades

      - max_atoms = 40
         - diff = [389.4 298.7 341.] (media-baja)
         - Problemas con solo 2 y 3 ciudades!
         - Solo escala el número de paquetes

   >>> Los resultados no mejoran al entrenar en problemas de 20 átomos!!!
      > Se siguen generando problemas sin locations y pocas ciudades
         - Parece que el "tipo" de problema difícil es igual para 15 que
           20 átomos (con pocas ciudades y sin locations)
      > La generalización no mejora (de hecho empeora, pero creo que es por
        haber parado el entrenamiento a mitad)
         - La dificultad para problemas grandes es menor que al entrenar
           sobre 15 átomos
         - No escala el número de ciudades     

   << Debería entrenar sobre 15 átomos! >>


>  <logistics>
   planners=[lama-first, lazy-greedy, lazy-greedy]
   max_atoms_init_state=15, max_actions_init_state=1, max_actions_goal_state=2.0
   <init_policy_entropy_coeffs: 0.2, None>
   <goal_policy_entropy_coeffs: 0.2, None>
   difficulty_rescale_factor=1
   new difficulty normalization

   > Ejecutándose en DaSCI
   > logs: init_policy/version_208
   > saved_models: both_policies_264

   > Entrenamiento (its=2800, paré el entrenamiento a mitad)
      - Tiempo (casa): 1d 14h
      - r_diff (goal_policy)= 30
      - r_eventual=-0.02
      - init_policy_entropy=0.6, goal_policy_entropy=0.75
      - term_cond_prob init_policy y goal_policy convergen a 0
      - num_objs:
         - city: 3.6
         - airport: 3.7
         - location: 0.2
         - airplane: 1
         - truck: 3.7
         - package: 6.4  

   > Problemas (its=2800)
      - max_atoms = 15
         - diff = [28. 27.2 31.7] (baja)
         - diversidad baja
            - Problemas con 2, 3 y 4 ciudades
            - Cada ciudad tiene un camión y, la mayoría
              de las veces, un solo airport
            - No hay locations

      - max_atoms = 20
         - diff = [44. 39.7 43.6] (baja)
         - diversidad baja
            - Problemas con hasta 6 ciudades

      - max_atoms = 30
         - diff = [94.3  90.2 113.2] (baja)
         - diversidad baja
            - Problemas con hasta 6 ciudades

   >>> Usar una goal_entropy alta no ayuda a que se añadan
       locations!
       Se generan problemas donde cada ciudad solo tiene un
       airport


>  <logistics>
   planners=[lama-first, lazy-greedy, lazy-greedy]
   max_atoms_init_state=15, max_actions_init_state=1, max_actions_goal_state=2.0
   <init_policy_entropy_coeffs: 0.0, None>
   <goal_policy_entropy_coeffs: 0.0, None>
   <diversity_rescale_factor=10.0>

   > Ejecutándose en casa
   > logs: init_policy/version_207
   > saved_models: deleted

   > Entrenamiento (its=7000)
      - Tiempo (casa): 2d 12h
      - r_diff (goal_policy)= 40
      - r_eventual=-0.05
      - init_policy_entropy=0.3, goal_policy_entropy=0.2
      - term_cond_prob init_policy = 0.015, goal_policy = 0.05
      - num_objs:
         - city: 3.2
         - airport: 3.2
         - location: 0.8
         - airplane: 1.1
         - truck: 3.2
         - package: 6.5  

   > Problemas (its=6000)
      - max_atoms = 15
         - diff = [40.7 41.3 42.2] (medio-alta)
         - diversidad media-baja
            - Problemas con 2, 3 y 4 ciudades
            - Cada ciudad tiene un solo camión y, la mayoría
              de las veces, un solo airport
            - Hay pocas locations

      - max_atoms = 20
         - diff = [82. 71.4 73.5] (alta)
         - Problemas con 2 hasta 6 ciudades

      - max_atoms = 30
         - diff = [170. 138.8 153.3] (medio-baja)
         - Problemas con hasta 6 ciudades
         - Se generan problemas con varias locations!!!

      - max_atoms = 40
         - diff = [238.5 215.8 282.] (baja)
         - Problemas con hasta 9 ciudades
         - Se generan problemas con varias locations!!! 

   >>> Análisis de resultados
      - El número de ciudades escala bien
      - En los problemas de cierto tamaño, se añaden un número moderado de locations!
      - No obstante, la dificultad escala mal
      >>> Aunque se generan problemas con locations, la goal-policy
        no usa la acción drive!!!
        2 posibles causas:
         - El número de locations en problemas de 15 átomos
           es muy pequeño como para que aprenda -> aumentar
           el número de locations generados
         - La goal_policy converge prematuramente a solo usar
           acciones fly,load y unload pero no drive -> usar
           goal_policy entropy 


>  <logistics>
   planners=[lama-first, lazy-greedy, lazy-greedy]
   max_atoms_init_state=15, max_actions_init_state=1, max_actions_goal_state=2.0
   <init_policy_entropy_coeffs: 0.0, None>
   <goal_policy_entropy_coeffs: 0.0, None>
   <diversity_rescale_factor=1.0>

   > Ejecutándose en DaSCI
   > logs: init_policy/version_209
   > saved_models: both_policies_266

   > Entrenamiento (its=3800)
      - Tiempo (casa): 2d 12h
      - r_diff (goal_policy)= 50
      - r_eventual=0
      - init_policy_entropy=0.1, goal_policy_entropy=0.14
      - term_cond_prob init_policy y goal_policy convergen a 0
      - num_objs:
         - city: 3
         - airport: 3
         - location: 0
         - airplane: 1
         - truck: 3
         - package: 8 

   > Problemas (its=3500)
      - max_atoms = 15
         - diff = [53.  47.9 49.5] (medio-alta)
         - diversidad muy baja!!!
            - De los 10 problemas, 9 son idénticos

      - max_atoms = 20
         - diff = [85.4  81.5 145.2] (alta)
         - Problemas con 3 y 4 ciudades

      - max_atoms = 30
         - diff = [174, 174, 451274] (alta)
         - Problemas con 5 y 6 ciudades

      - max_atoms = 40
         - diff = [268 268 8.925558e+05] (media alta)
         - Problemas con 7 y 8 ciudades

   >>> diversity_rescale_factor=1 es demasiado bajo!
      - Se generan problemas casi idénticos


>  <logistics>
   planners=[lama-first, lazy-greedy, lazy-greedy]
   max_atoms_init_state=15, max_actions_init_state=1, max_actions_goal_state=2.0
   init_policy_entropy_coeffs: 0.0, None
   goal_policy_entropy_coeffs: 0.0, None
   <new_diversity_reward>
   <no feature weights>
   <diversity_rescale_factor=100.0>

   > Ejecutándose en casa
   > logs: init_policy/version_208
   > saved_models: both_policies_267

   > Entrenamiento (its=9200)
      - Tiempo (casa): 2d 20h
      - r_diff (goal_policy)= 29
      - r_eventual=-0.15
      - init_policy_entropy=0.4, goal_policy_entropy=0.2
      - term_cond_prob init_policy converge a 0, goal_policy converge a 0.05
      - num_objs:
         - city: 2
         - airport: 2.4
         - location: 1.3
         - airplane: 2.3
         - truck: 3
         - package: 6 

   > Problemas (its=9000)
      - max_atoms = 15
         - diff = [30.4 26.7 29.1] (baja)
         - diversidad alta
            - Todos los problemas con dos ciudades
            - Excepto eso, todos los demás objetos varían mucho

      - max_atoms = 20
         - diff = [38.8 34.9 37.9]
         - Todos los problemas con dos ciudades

      - max_atoms = 30
         - diff = [96.1 71.4 85.7]
         - Todos los problemas con dos ciudades

      - max_atoms = 40
         - diff = [115.3  82.9 102. ]
         - Todos los problemas con dos ciudades  

   > Análisis
      >> Se generan problemas con locations y donde la goal_policy usa tanto
         drive como fly!!
      > No obstante, la dificultad es muy baja y solo se generan problemas con
        dos ciudades!


>  <logistics>
   planners=[lama-first, lazy-greedy, lazy-greedy]
   max_atoms_init_state=15, max_actions_init_state=1, max_actions_goal_state=2.0
   init_policy_entropy_coeffs: 0.0, None
   goal_policy_entropy_coeffs: 0.0, None
   <new_diversity_reward>
   <no feature weights>
   <diversity_rescale_factor=10.0>

   > Ejecutándose en DaSCI
   > logs: init_policy/version_210
   > saved_models: both_policies_268

   > Entrenamiento (its=3800)
      - Tiempo (dasci): 2d 8h
      - r_diff (goal_policy)= 42
      - r_eventual=0
      - init_policy_entropy=0.3, goal_policy_entropy=0.2
      - term_cond_prob init_policy converge a 0, goal_policy converge a 0.035
      - num_objs:
         - city: 2.8
         - airport: 2.9
         - location: 0.4
         - airplane: 1
         - truck: 3.2
         - package: 7.5

   > Problemas (its=3500)
      - max_atoms = 15
         - diff = [42.6 40.1 41.4] (medio-alto)
         - diversidad media-alta
            - Problemas con 2 y 3 ciudades
            - La mayoría de las ciudades tienen un solo airport y truck,
              pero algunas tienen dos

      - max_atoms = 20
         - diff = [79.1 62.9 74.1]
         - Problemas con 2-4 ciudades

      - max_atoms = 30
         - diff = [136.6 129. 138.1]
         - Problemas con 5-6 ciudades

      - max_atoms = 40
         - diff = [256.1 228.1 300.3]
         - Problemas con 6-9 ciudades    

   >> Análisis
      >> La goal_policy usa acciones tanto drive como fly
      >> El número de ciudades escala
      > La dificultad escala regular
      > La mayoría de ciudades solo tienen un airport y truck
        y hay pocas locations

      >>> Nota: para problemas grandes, la goal_policy a veces
          entra en bucle (ej.: un camión va de la location A a B
          y de vuelta repetidas veces antes de hacer unload)


>  <logistics>
   planners=[lama-first, lazy-greedy, lazy-greedy]
   max_atoms_init_state=15, max_actions_init_state=1, max_actions_goal_state=2.0
   init_policy_entropy_coeffs: 0.0, None
   goal_policy_entropy_coeffs: 0.0, None
   new_diversity_reward
   no feature weights
   <diversity_rescale_factor=10.0>
   <max_actions_goal_state=10.0>

   > Ejecutándose en casa
   > logs: init_policy/version_209, 210
   > saved_models: both_policies_269, 271

   > Entrenamiento (its=7100)
      - Tiempo (dasci): 2d 18h
      - r_diff (goal_policy)= 42
      - r_eventual=0
      - init_policy_entropy=0.22, goal_policy_entropy=0.22
      - term_cond_prob init_policy converge a 0, goal_policy converge a 0.045
      - num_objs:
         - city: 2.6
         - airport: 2.6
         - location: 0.4
         - airplane: 1
         - truck: 3
         - package: 7.8

   > Problemas (its=3500)
      - max_atoms = 15
         - diff = [43.3 43.3 40.1] (medio-alto)
         - diversidad media-baja
            - Problemas con 2 y 3 ciudades
            - La mayoría de las ciudades tienen un solo airport y truck,
              pero algunas tienen dos 
            - Pocas locations

      - max_atoms = 20
         - diff = [72.5 68.2 60.3]
         - Problemas con 3 y 4 ciudades

      - max_atoms = 30
         - diff = [73.2 60.1 63.1]
         - Problemas con 3-6 ciudades

      - max_atoms = 40
         - diff = [1 1 1]
            - Se samplea la termination condition justo al empezar a generar
              el goal!!!
         - Problemas con 2-6 ciudades

   > Análisis
      > Resultados muy parecidos (e incluso peores) a cuando uso max_actions_goal_state=2.0!
      >>> CREO QUE EL PROBLEMA ES EL disc_factor_difficulty!!! (si no vale 1,
          la r_diff se descuenta, por lo que a la goal_policy se la motiva a
          generar el goal usando el mínimo número de acciones posible)



>  <logistics>
   planners=[lama-first, lazy-greedy, lazy-greedy]
   max_atoms_init_state=15, max_actions_init_state=1
   init_policy_entropy_coeffs: 0.0, None
   goal_policy_entropy_coeffs: 0.0, None
   new_diversity_reward
   no feature weights
   <diversity_rescale_factor=10.0>
   <max_actions_goal_state=4.0>

   > Ejecutándose en DaSCI
   > logs: init_policy/version_211
   > saved_models: both_policies_270

   > Entrenamiento (its=8000)
      - Tiempo (dasci): 5d 16h
      - r_diff (goal_policy)= 70 (la r_diff deja casi de aumentar en its=6000)
      - r_eventual=0
      - init_policy_entropy=0.22, goal_policy_entropy=0.35
      - term_cond_prob init_policy converge a 0, goal_policy converge a 0.045
      - num_objs:
         - city: 2.1
         - airport: 2.1
         - location: 1.8
         - airplane: 2
         - truck: 2.1
         - package: 7

   > Problemas (its=8000)
      - max_atoms = 15
         - diff = [71.3 51.2 55.6] (alta)
         - diversidad media-baja
            - Problemas con 2 y 3 ciudades
            - La mayoría de los problemas son idénticos o muy
              parecidos!!!

      - max_atoms = 20
         - diff = [131.6  87.1 100.6]
         - Problemas con 2 y 3 ciudades

      - max_atoms = 30
         - diff = [13.5 11.5 11.6]
         - Problemas con 3 y 4 ciudades
         - En algunos problemas la goal_policy samplea la
           termination condition antes de tiempo!!!

      - max_atoms = 40
         - diff = [1. 1. 1.]
         - Problemas con 4-6 ciudades
         - En todos los problemas la goal_policy samplea la
           termination condition al empezar!!!

   > Análisis
      - Se generan problemas tanto con locations como con varias ciudades
        y donde la goal_policy usa tanto "fly" como "drive"!!!
      - La dificultad para los problemas con 15 y 20 átomos
        aumenta respecto a cuando se usaba max_actions_goal_state=2
      - No obstante, la diversidad es bastante baja
      - Además, la goal_policy no generaliza bien a problemas de mayor
        tamaño (ejecuta la termination condition antes de tiempo) 



------ Pruebas NeSIG ablations

>  <logistics>
   <only lama-first for training>
   max_atoms_init_state=15
   init_policy_entropy_coeffs: 0.0, None
   goal_policy_entropy_coeffs: 0.0, None
   diversity_rescale_factor=10.0
   <max_actions_goal_state=2.0>
   <disc_factor_difficulty=1>
   <train both policies>

   > Ejecutándose en casa
   > logs: init_policy/version_211
   > saved_models: both_policies_272

   > Entrenamiento(its=1700, lo paré a mitad)
      - Tiempo: 11h
      - Resultados muy parecidos a experimento con
        tres planners en entrenamiento y disc_factor_difficulty=0.995
        No obstante, el tiempo de entrenamiento es bastante menos!

   > Problemas (its=1600)
      - max_atoms = 15
         - diff = [42.  33.8 38.1]

      - max_atoms = 20
         - diff = [72.  56.9 69.3]

      - max_atoms = 30
         - diff = [138.3 104.2 132.9]

      - max_atoms = 40
         - diff = [236.3 190.3 250.1]
         
   > Análisis
      - Parece que todo funciona correctamente
      - Parece que la dificultad evaluada en training con un solo planner
        después generaliza a los otros dos planners! 


>  <logistics>
   <only lama-first for training>
   max_atoms_init_state=15
   init_policy_entropy_coeffs: 0.0, None
   goal_policy_entropy_coeffs: 0.0, None
   diversity_rescale_factor=10.0
   <max_actions_goal_state=2.0>
   <disc_factor_difficulty=1>
   <train only init state policy>

   > Ejecutándose en casa
   > logs: init_policy/version_212 (no hay goal_policy logs)
   > saved_models: both_policies_273

   > Entrenamiento (its=1150, parado a mitad)
      - Tiempo: 3h 30min
      - Aprende, aunque la r_diff sube mucho más lentamente
        que si entrenamos ambas generative policies

   > Problemas (its=1100)
      - max_atoms = 15
         - diff = [3.4 3.2 3.2]

      - max_atoms = 20
         - diff = [5.9 4.7 4.8]

      - max_atoms = 30
         - diff = [8.3 7.4 7.4]

      - max_atoms = 40
         - diff = [11.3 9.4 9.1]

   > Análisis:
      - Aprende aunque mucho peor que con ambas generative policies


>  <logistics>
   <only lama-first for training>
   max_atoms_init_state=15
   init_policy_entropy_coeffs: 0.0, None
   goal_policy_entropy_coeffs: 0.0, None
   diversity_rescale_factor=10.0
   <max_actions_goal_state=2.0>
   <disc_factor_difficulty=1>
   <train only goal policy>

   > Ejecutándose en casa
   > logs: goal_policy/version_190 (no hay init_policy logs)
   > saved_models: both_policies_274

   > Entrenamiento (its=1150, parado a mitad)
      - Tiempo: 1h 47min
      - Aprende, aunque la r_diff sube más lentamente
        que si entrenamos ambas generative policies
      - La r_diff sube más rápidamente que cuando solo
        entrenamos la init policy -> parece que la goal
        policy es más importante que la init policy!!

   > Problemas (its=1100)
      - max_atoms = 15
         - diff = [11.8 11. 11.3]

      - max_atoms = 20
         - diff = [11.7 10.1 10.4]

      - max_atoms = 30
         - diff = [26.8 20.2 20.7]

      - max_atoms = 40
         - diff = [43.4 32.5 34.2]

   > Análisis
      - Aprende peor que con ambas generative policies pero mejor
        que solo entrenando la init_policy


>  <logistics>
   <random generator (no generative policies)>

   > Problemas
      - max_atoms = 15
         - diff = [7.3 5.9 6.5]

      - max_atoms = 20
         - diff = [5.6 4.8 5. ] 

      - max_atoms = 30
         - diff = [8.8 7.8 7.8] 

      - max_atoms = 40
         - diff = [8.  6.8 6.8]

   > Análisis
      - Dificultad muy baja! (más baja que entrenando solo una
        de las dos generative policies)


>  <sokoban>
   <max_atoms_init_state=40+15, max_actions_init_state=1, max_actions_goal_state=1>
   init_policy_entropy_coeffs: 0.0, None
   goal_policy_entropy_coeffs: 0.0, None
   diversity_rescale_factor=10.0

   > Ejecutándose en Casa
   > logs: init_policy/version_0
   > saved_models: both_policies_275

   > Entrenamiento (its=1800)
      - Tiempo: 2d 14h
      - r_diff (goal_policy)= 2.5e+6
      - r_eventual=0
      - init_policy_entropy=0.12, goal_policy_entropy=0.35
      - term_cond_prob init_policy converge a 0, goal_policy oscila alrededor de 0.4
      - num_atoms:
         - at-wall: 0
         - at-box: 7

   > Problemas (its=1800)
      - max_atoms = 40+15, map_size=5x5
         - diff = [2410691 1935919 1051775]
         - diversidad muy baja
            - Todos los problemas con 7 boxes y sin walls
            - Hay problemas idénticos y en muchos las cajas están en los mismos sitios!!!

   > Análisis
      - Los problema son muy difíciles, por lo que el planner tarda mucho en resolverlos
        y el entrenamiento se ralentiza muchísimo!!!
         >>> Por lo pronto, debería intentar resolver los problemas en paralelo!

      - La diversidad de los problemas es muy baja!!!
         >>> Debo aumentar la diversity reward y, quizás, aplicar np.log a r_diff!


>  <logistics>
   max_atoms_init_state=15, max_actions_init_state=1, <max_actions_goal_state=4.0>
   init_policy_entropy_coeffs: 0.0, None
   goal_policy_entropy_coeffs: 0.0, None
   <diversity_rescale_factor=50.0>

   --- Segunda parte de la ejecución con solo logs cada 5 its y midiendo la dificultad
       en paralelo

   > Ejecutándose en DaSCI
   > logs: init_policy/version_212, 213
   > saved_models: both_policies_276, 278

   > Entrenamiento (its=8000)
      - Tiempo (casa): 7d
      - r_diff (goal_policy)= 70
      - r_eventual=0
      - init_policy_entropy=0.1, goal_policy_entropy=0.2
      - term_cond_prob init_policy converge a 0, goal_policy oscila alrededor de 0.002
      - num_objs:
         - city: 2.4
         - airport: 2.4
         - location: 0.6
         - airplane: 1.4
         - truck: 3
         - package: 7.6

    > Problemas (its=8000)
      - max_atoms = 15
         - diff = [73. 65.2 66.9]
         - diversidad media
            - la goal_policy usa drive y fly!!!
            - problemas con 2 y 3 ciudades
            - problemas con 0 o 1 location
            - problemas bastante parecidos entre sí

      - max_atoms = 20
         - diff = [84.5 76.2 85.5]
         - problemas con 2 y 3 ciudades

      - max_atoms = 30
         - diff = [124.5 120.7 163.3]
         - problemas con 3 y 4 ciudades

      - max_atoms = 40
         - diff = [285.7 230.2 287.7]
         - problemas con 2-5 ciudades

   > La dificultad escala regular


>  <logistics>
   max_atoms_init_state=15, max_actions_init_state=1, <max_actions_goal_state=10.0>
   init_policy_entropy_coeffs: 0.0, None
   goal_policy_entropy_coeffs: 0.0, None
   disc_factor_diff=1
   <diversity_rescale_factor=50.0>

   > Ejecutándose en Casa
   > logs: init_policy/version_1
   > saved_models: both_policies_277

   > Entrenamiento (its=8000)
      - Tiempo (casa): 9d
      - r_diff (goal_policy)= 70
      - r_eventual=0
      - init_policy_entropy=0.13, goal_policy_entropy=0.35
      - term_cond_prob init_policy converge a 0, goal_policy oscila alrededor de 0.003
      - num_objs:
         - city: 2.3
         - airport: 2.3
         - location: 0.7
         - airplane: 1.3
         - truck: 2.8
         - package: 7.8

   > Problemas (its=9000)
      - max_atoms = 15
         - diff = [68.3 58.6 67.8]
         - diversidad media
            - la goal_policy usa drive y fly!!!
            - problemas con 2 y 3 ciudades
            - problemas con 0 o 1 location

      - max_atoms = 20
         - diff = [73.5  73.3 101.7]
         - 2 y 3 ciudades

      - max_atoms = 30
         - diff = [96.4  94.3 147.6]
         - 3 a 5 ciudades

      - max_atoms = 40
         - diff = [216.2 198.1 396.]
         - 4 a 6 ciudades

   > Análisis
      - Los resultados son muy parecidos a cuando se usa max_actions_goal_state=4!!!
      - La goal policy usa tanto drive como fly, pero los problemas tienen muy pocos
        locations!
      - La dificultad escala bastante mal
      - Parece como si la goal policy no "aprovechara" la gran cantidad de acciones
        que tiene a su alcance (de ahí que la goal_policy entropy sea tan alta)!!
      - Creo que debería aumentar la entropía de las políticas, para evitar que converjan
        rápido


>  <logistics>
   max_atoms_init_state=15, max_actions_init_state=1, <max_actions_goal_state=5.0>
   init_policy_entropy_coeffs: 0.0, None
   goal_policy_entropy_coeffs: 0.0, None
   <diversity_rescale_factor=100>

   > Ejecutándose en Casa
   > logs: init_policy/version_2, 3, 4
   > saved_models: both_policies_279, 281, 282

   > Entrenamiento (its=8500)
      - Tiempo (casa): 3d 12h
      - r_diff (goal_policy)= 80
      - r_eventual=0
      - init_policy_entropy=0.2, goal_policy_entropy=0.25
      - term_cond_prob init_policy converge a 0, goal_policy oscila alrededor de 0.0003
      - num_objs:
         - city: 2
         - airport: 3.3
         - location: 0.7
         - airplane: 1
         - truck: 4.2
         - package: 5.9

   > Problemas (its=8500)
      - max_atoms = 15
         - diff = [70.5 57.4 72. ]
         - diversidad media-baja
            - la goal_policy usa drive y fly!!!
            - casi todos los problemas con 2 ciudades, solo uno con 3
            - problemas con 0 o 1 location

      - max_atoms = 20
         - diff = [76.1 68.  77.4]
         - problemas con 2 y 3 ciudades

      - max_atoms = 30
         - diff = [205.5 168.9 256.1]
         - problemas con 3-5 ciudades

      - max_atoms = 40
         - diff = [429.9 338. 417.2]
         - problemas con 3-5 ciudades

   >>> Se generan problemas bastante difíciles y cuya dificultad escala muy
       bien!!! No obstante, su diversidad debería mejorar
      > Creo que debería aumentar el diversity reward o usar también
        policy entropy
  
 
>  <logistics>
   max_atoms_init_state=15, max_actions_init_state=1, <max_actions_goal_state=5.0>
   <init_policy_entropy_coeffs: 1, (5000, 0.01)>
   <goal_policy_entropy_coeffs: 1, (5000, 0.01)>
   <diversity_rescale_factor=50>

   > Ejecutándose en DaSCI
   > logs: init_policy/version_214
   > saved_models: both_policies_280

   > Entrenamiento (its=8000)
      - Tiempo (casa): 6d 10h
      - r_diff (goal_policy)= <150> (muy alto!)
      - r_eventual=0
      - init_policy_entropy=0.3, goal_policy_entropy=0.65
      - term_cond_prob init_policy y goal_policy convergen a alrededor de 0.0005
      - num_objs:
         - city: 2
         - airport: 4.6
         - location: 0.8
         - airplane: 1
         - truck: 2
         - package: 6.5

   > Problemas (its=8000)
      - max_atoms = 15
         - diff =  [64.2 55.3 55.3]
         - diversidad media-baja
            - la goal_policy usa drive y fly!!!
            - problemas con 2 y 3 ciudades
            - problemas con 0 o 1 location
            - muchos problemas casi idénticos

      - max_atoms = 20
         - diff =  [239.1 163.7 227.6]
         - problemas solo con dos ciudades!

      - max_atoms = 30
         - diff =  [733.  485.6 730.3]
         - problemas solo con dos ciudades! 

      - max_atoms = 40
         - diff = [1196.  769.2 1202.4]
         - problemas solo con dos ciudades! 

      >>> Análisis
         - Se generan problemas muy difíciles y la dificultad escala muy bien!!!
         - No obstante, los problemas solo tienen dos ciudades y son muy poco diversos
         - Parece que usar action entropy ayuda a explorar (ya que se generan problemas
           más difíciles que cuando no se usa action entropy)
         - Para evitar que se generen problemas con solo dos ciudades quizás haya que
            - Bajar el goal action entropy (ya que, si no, la goal policy solo ejecuta
              acciones casi al azar)
            - Aumentar el diversity reward

>  <logistics>
   max_atoms_init_state=15, max_actions_init_state=1, max_actions_goal_state=5.0
   init_policy_entropy_coeffs: 0.0, None
   goal_policy_entropy_coeffs: 0.0, None
   <diversity_rescale_factor=500>

   > Ejecutándose en Casa
   > logs: init_policy/version_5
   > saved_models: both_policies_283

   > Entrenamiento (its=25000)
      - Tiempo (casa): 7d 21h
      - r_diff (goal_policy)= 60
      - r_eventual=-0.2
      - init_policy_entropy=0.15, goal_policy_entropy=0.15
      - term_cond_prob init_policy y goal_policy oscilan alrededor de 1e-3
      - num_objs:
         - city: 2
         - airport: 2.1
         - location: 2.8
         - airplane: 1.3
         - truck: 3.1
         - package: 5.5   

   > Problemas (its=25000)
      - max_atoms = 15
         - diff = [35.4 28.7 26.9] (baja)
         - diversidad media-alta
            - la goal_policy usa drive y fly!!!
            - problemas con solo 2 ciudades!

      - max_atoms = 20
         - diff = [77.2 52.1 51.9]
         - problemas con solo 2 ciudades!

      - max_atoms = 30
         - diff = [110.6 85.8 95.7]
         - solo 2 problemas con tres ciudades 

      - max_atoms = 40
         - diff = [161.3 140. 166.9]
         - problemas con solo 2 ciudades!    

   > Análisis:
      - Problemas muy diversos menos por el hecho de que solo tienen dos ciudades
      - Problemas de dificultad baja -> puede que la razón sea que son demasiado
        diversos (debería bajar el diversity_rescale_factor) o que solo tienen dos
        ciudades
      - Parece que la diversity_reward hace que se generen problemas con solo
        dos ciudades!!!

   
>  <logistics>
   max_atoms_init_state=15, max_actions_init_state=1, max_actions_goal_state=5.0
   <init_policy_entropy_coeffs: 1, (5000, 0.01)>
   <goal_policy_entropy_coeffs: 0, None>
   <diversity_rescale_factor=50>

   > Ejecutándose en DaSCI
   > logs: init_policy/version_215
   > saved_models: both_policies_284

   > Entrenamiento (its=9000)
      - Tiempo (dasci): 7d 20h
      - r_diff (goal_policy)= 80
      - r_eventual=0
      - init_policy_entropy=0.1, goal_policy_entropy=0.35
      - term_cond_prob init_policy converge a 0 y la goal_policy oscila alrededor de 5e-4
      - num_objs:
         - city: 2
         - airport: 3.5
         - location: 0.5
         - airplane: 1
         - truck: 2
         - package: 8

   > Problemas (its=9000)
      - max_atoms = 15
         - diff =  [90.9 57.3 65. ]
         - diversidad muy baja!!!
            - la goal_policy usa drive y fly!!!
            - casi todos los problemas son idénticos
            - solo se generan problemas con dos ciudades

      - max_atoms = 20
         - diff = [121.7  81.2  95.6]
         - problemas solo con dos y tres ciudades

      - max_atoms = 30
         - diff = [272.6 192.  250.1]
         - problemas con tres y cuatro ciudades

      - max_atoms = 40
         - diff = [315, 197, 258]
         - problemas con tres y cuatro ciudades

   > Análisis
      > Se generan problemas menos difíciles que cuando también utilizábamos goal_entropy!!
        (ver experimento anterior) -> DEBERÍAMOS USAR GOAL E INIT ENTROPY
      > Problemas muy poco diversos -> una entropía final de 0.01 es muy baja
      > La dificultad escala razonablemente bien (aunque debe mejorar)
      > Puntos a mejorar:
         - Diversidad
         - Escalado dificultad

>  <logistics>
   max_atoms_init_state=15, max_actions_init_state=1, max_actions_goal_state=5.0
   <init_policy_entropy_coeffs: 0.5, (5000, 0.1)>
   <goal_policy_entropy_coeffs: 0.5, (5000, 0)>
   <diversity_rescale_factor=50>

   > Ejecutándose en DaSCI
   > logs: init_policy/version_216
   > saved_models: both_policies_285

      > Entrenamiento (its=7800)
      - Tiempo (dasci): 6d 17
      - r_diff (goal_policy)= 160 <muy alto!!>
      - r_eventual=0
      - init_policy_entropy=0.55, goal_policy_entropy=0.35
      - term_cond_prob init_policy converge a 0 y la goal_policy oscila alrededor de 2e-4
      - num_objs:
         - city: 2.1
         - airport: 5
         - location: 0.85
         - airplane: 1
         - truck: 2.1
         - package: 6 

   > Problemas
      > its=7000
         - max_atoms = 15
            - diff = [97.3 74.3 93.2] (muy alta!!!)
            - diversidad media-baja
               - la goal_policy usa drive y fly!!!
               - Todos los problemas con dos ciudades
               - Problemas bastante parecidos entre sí

         - max_atoms = 20
            - diff = [378.9 220.6 311. ] 
            - todos los problemas con dos ciudades

         - max_atoms = 30
            - diff = [587.7 402.5 588.8] 
            - problemas con dos y tres ciudades

         - max_atoms = 40
            - diff = [701.8 495.  712.8]
            - problemas con dos y tres ciudades

         >>> Se generan problemas muy difíciles pero con pocas ciudades
            - La dificultad escala bien pero escalaría mejor si el número
              de ciudades escalara según el problem size

      > its=5000
         - max_atoms = 15
            - diff = [66.1 51. 57.2] (alta)
            - diversidad alta!
               - la goal_policy usa drive y fly!!!
               - Problemas con dos y tres ciudades

         - max_atoms = 20
            - diff= [108.1  82.5  90.6]
            - problemas con 2, 3 y 4 ciudades

         - max_atoms = 30
            - diff= [282.8 198.9 235.8]
            - problemas con 2, 3 y 4 ciudades

         - max_atoms = 40
            - diff= [344.5 310.2 513.1]
            - problemas con 2-5 ciudades

         >>> Se generan problemas muy diversos y la dificultad y número de ciudades
             escala bien con el problem size!!!

   >>> Este es el mejor modelo hasta la fecha!!!
      - No obstante, tengo que hacer "early stopping" porque si no la entropía
        baja demasiado.
      - Quizás debería aumentar los entropy coeffs!!!

>  <logistics>
   max_atoms_init_state=15, max_actions_init_state=1, max_actions_goal_state=5.0
   init_policy_entropy_coeffs: 0.0, None
   <goal_policy_entropy_coeffs: 0.5, (5000, 0)>
   <diversity_rescale_factor=500>
   <use feature_weights for diversity_reward>

   > Ejecutándose en Casa
   > logs: init_policy/version_6
   > saved_models: both_policies_286

   > Entrenamiento (its=60000)
      - Tiempo (dasci): 6d 16h
      - r_diff (goal_policy)= 20 (muy baja!!)
      - r_eventual=-0.5 (no aprende!!)
      - init_policy_entropy=0.15, goal_policy_entropy=0.5 (fluctúa mucho)
      - term_cond_prob init_policy converge a 0 y la goal_policy oscila mucho alrededor de 2e-4
      - num_objs:
         El número de objetos fluctúa mucho!
         Locations parece que converge a 0
         Num cities y airports convergen a 2

   >>> No aprende!!!
      - Ahora al usar pesos para la diversity reward deja de aprender!!
      - Debería bajar el diversity rescale factor!!!

>  <logistics>
   <max_atoms_init_state=(10,20)>, max_actions_init_state=1, max_actions_goal_state=5.0
   <init_policy_entropy_coeffs: 0.5, (5000, 0.2)>
   <goal_policy_entropy_coeffs: 0.5, (5000, 0.1)>
   <diversity_rescale_factor=0>
   <no feature_weights for diversity_reward>
   <nlm receives problem max size as input>
   <minibatch_size=50>

   > Ejecutándose en Casa
   > logs: init_policy/version_7
   > saved_models: both_policies_287

   >>> La entropía parece ser demasiado baja!!! (num_locations baja demasiado y num_cities converge a 2)

   
>  <logistics>
   <max_atoms_init_state=15>, max_actions_init_state=1, max_actions_goal_state=5.0
   <init_policy_entropy_coeffs: 0.5, (5000, 0.2)>
   <goal_policy_entropy_coeffs: 0.5, (5000, 0.1)>
   <diversity_rescale_factor=0>
   <no feature_weights for diversity_reward>
   <nlm does NOT receive problem max size as input>
   <minibatch_size=50>

   > Ejecutándose en DaSCI
   > logs: init_policy/version_217
   > saved_models: both_policies_288

   >>> La entropía parece ser demasiado baja!!! (num_locations baja demasiado y num_cities converge a 2)


>  <logistics>
   <max_atoms_init_state=15>, max_actions_init_state=1, max_actions_goal_state=5.0
   <init_policy_entropy_coeffs: 0.5, None>
   <goal_policy_entropy_coeffs: 0.2, None>
   <diversity_rescale_factor=0>
   <no feature_weights for diversity_reward>
   <nlm does NOT receive problem max size as input>
   <minibatch_size=50>

   > Ejecutándose en DaSCI
   > logs: init_policy/version_218
   > saved_models: both_policies_289

   > Entrenamiento (its=10000)
      - Tiempo (dasci): 9d
      - r_diff (goal_policy)= 45
      - r_eventual=-0.01
      - init_policy_entropy=0.83, goal_policy_entropy=0.94
      - term_cond_prob init_policy converge a 0 y la goal_policy oscila alrededor de 2e-3
      - num_objs:
         - city: 2.02
         - airport: 5.3
         - location: 0.8
         - airplane: 1.1
         - truck: 2.1
         - package: 5.6

   > Problemas
      > its=10000
         - max_atoms = 15
            - diff = [37.9 31.2 35.7] (media-baja)
            - diversidad media-alta
               - la goal_policy usa drive y fly!!!
               - Todos los problemas con dos ciudades
               - Problemas muy diversos (más allá del hecho de que solo tienen dos ciudades)

         - max_atoms = 20
            - diff = [83.2 65.5 71.8] (media-alta)
            - Todos los problemas con dos ciudades

         - max_atoms = 30
            - diff = [160.9 121.8 113.6] (media-baja)
            - Todos los problemas con dos ciudades

         - max_atoms = 40
            - diff = [322.9 248.9 280.1] (media-baja)
            - Todos los problemas con dos ciudades

   >>> Solo se generan problemas con dos ciudades y la dificultad de los problemas es medio baja
      - Creo que debería aumentar la init_policy_entropy y bajar la goal_policy_entropy


>  <logistics>
   <max_atoms_init_state=15>, max_actions_init_state=1, max_actions_goal_state=5.0
   <init_policy_entropy_coeffs: 1, None>
   <goal_policy_entropy_coeffs: 0.05, None>
   diversity_rescale_factor=0
   <nlm does NOT receive problem max size as input>
   <minibatch_size=50>

   > Ejecutándose en DaSCI
   > logs: init_policy/version_219
   > saved_models: both_policies_290

   > Paré el entrenamiento a mitad a 7000 its
      - Tiempo: 2d 22h
      - num_cities baja hasta 2.02
      - r_diff llega hasta 30
      - goal_policy_entropy=0.91, init_policy_entropy=0.89
      - r_eventual=-0.05
      - num_objs:
         - city: 2.02
         - airport: 4.6
         - location: 2.1
         - airplane: 1.5
         - truck: 2.5
         - package: 4.3


>  <logistics>
   <max_atoms_init_state=15>, max_actions_init_state=1, max_actions_goal_state=5.0
   <init_policy_entropy_coeffs: 1, None>
   <goal_policy_entropy_coeffs: 0.1, None>
   diversity_rescale_factor=0
   <nlm does NOT receive problem max size as input>
   <minibatch_size=50>

   > Ejecutándose en Casa
   > logs: init_policy/version_9
   > saved_models: both_policies_291

   > Paré el entrenamiento a mitad
      - Resultados muy parecidos al experimento anterior (goal_policy_entropy_coeffs=0.05, None)
      - num_cities baja hasta 2.02

>  <logistics>
   max_atoms_init_state=15, max_actions_init_state=1, max_actions_goal_state=5.0
   <init_policy_entropy_coeffs: 0.5, None>
   <goal_policy_entropy_coeffs: 0.1, None>
   <diversity_rescale_factor=50>
   <use feature_weights for diversity_reward>
   nlm does NOT receive problem max size as input

   > Ejecutándose en Casa
   > logs: init_policy/version_10
   > saved_models: both_policies_292 (casa)

   > Paré el entrenamiento a mitad a 50k its
      - Tiempo: 14d
      - num_cities baja hasta 2.075
      - r_diff llega hasta 70 (seguía subiendo)
         - Nota: a mismo num it (27k), la dificultad era la misma que en el siguiente experimento
                 (init_policy_entropy = 0.2)
      - goal_policy_entropy=0.87, init_policy_entropy=0.78 (ambas bajando)
      - r_eventual=-0.23
      - num_objs:
         - city: 2.075
         - airport: 3.2
         - location: 2.9
         - airplane: 1.9
         - truck: 2.9
         - package: 4

>  <logistics>
   max_atoms_init_state=15, max_actions_init_state=1, max_actions_goal_state=5.0
   <init_policy_entropy_coeffs: 0.2, None>
   <goal_policy_entropy_coeffs: 0.1, None>
   <diversity_rescale_factor=50>
   <use feature_weights for diversity_reward>
   nlm does NOT receive problem max size as input

   > Ejecutándose en DaSCI
   > logs: init_policy/version_221
   > saved_models: both_policies_292 (dasci)

   > Paré el entrenamiento a mitad a 27k its
      - Tiempo: 14d
      - num_cities baja hasta 2.16
      - r_diff llega hasta 37 (seguía subiendo)
      - goal_policy_entropy=0.87, init_policy_entropy=0.65
      - r_eventual=-0.3
      - num_objs:
         - city: 2.16
         - airport: 2.37
         - location: 2.23
         - airplane: 1.77
         - truck: 3.1
         - package: 5.4

>  <logistics>
   max_atoms_init_state=15, max_actions_init_state=1, max_actions_goal_state=5.0
   <init_policy_entropy_coeffs: 0.5, None>
   <goal_policy_entropy_coeffs: 0.2, (5000, 0)>
   diversity_rescale_factor=50
   use feature_weights for diversity_reward
   nlm does NOT receive problem max size as input

   > Ejecutándose en Casa
   > logs: init_policy/version_11, 12, 13
   > saved_models: both_policies_293, 294, 295 (casa)

   > Entrenamiento (its=55000)
      - Tiempo (casa): 13d
      - r_diff (goal_policy)= 35
      - r_eventual=-0.25
      - init_policy_entropy=0.77, goal_policy_entropy=0.15
      - term_cond_prob init_policy converge a 0 y la goal_policy oscila alrededor de 2e-3
      - num_objs:
         - city: 2.05
         - airport: 4
         - location: 1.3
         - airplane: 2
         - truck: 3.1
         - package: 4.6

   >>>
      La dificultad es menor que usando goal_policy_entropy=0.1!!!

>  <logistics>
   max_atoms_init_state=15, max_actions_init_state=1, max_actions_goal_state=5.0
   <init_policy_entropy_coeffs: 0.2, None>
   <goal_policy_entropy_coeffs: 0.2, (10000, 0)>
   diversity_rescale_factor=50
   use feature_weights for diversity_reward
   nlm does NOT receive problem max size as input

   > Ejecutándose en Casa
   > logs: init_policy/version_14
   > saved_models: both_policies_0

   > Entrenamiento (its=25000, lo paré a mitad)
      - Tiempo (casa): 7d
      - r_diff (goal_policy)= 45 (y subiendo)
      - r_eventual=-0.2
      - init_policy_entropy=0.65, goal_policy_entropy=0.35
      - term_cond_prob init_policy converge a 0 y la goal_policy oscila alrededor de 4e-4
      - num_objs:
         - city: 2.08
         - airport: 2.8
         - location: 2.35
         - airplane: 2.4
         - truck: 2.7
         - package: 5.5

   > Problemas
      > its=25000
         - max_atoms = 15
            - diff = [40.1 36.2 43.6] (media)
            - diversidad media-alta
               - la goal_policy usa drive y fly!!!
               - Todos los problemas con dos ciudades
               - Problemas bastante diversos (más allá del hecho de que solo tienen dos ciudades)

         - max_atoms = 20
            - diff = [89.4 63.1 73.6] (media-alta)
            - Todos los problemas con dos ciudades

         - max_atoms = 30
            - diff = [150.5 130.6 241.3] (media)
            - Todos los problemas con dos ciudades

         - max_atoms = 40
            - diff = [403.7 296.2 694.6] (media)
            - Todos los problemas con dos ciudades

   >>> Análisis
      - Se generan problema relativamente difíciles (la dificultad seguía aumentando cuando paré el entrenamiento) y diversos
      - No obstante, el número de ciudades no escala con problem_size!!
      - Usar init_policy=0.2 funciona mejor que 0.5
      - Parece que es buena idea usar goal_policy=0.2, (10000, 0)


>>>  <logistics>
   max_atoms_init_state=15, max_actions_init_state=1, max_actions_goal_state=5.0
   <init_policy_entropy_coeffs: 0.1, None>
   goal_policy_entropy_coeffs: 0.2, (10000, 0)
   diversity_rescale_factor=50
   use feature_weights for diversity_reward
   nlm does NOT receive problem max size as input

   > Ejecutándose en Casa
   > logs: init_policy/version_15
   > saved_models: both_policies_1

   > Entrenamiento (its=16000, lo paré a mitad)
      - Tiempo (casa): 4d
      - r_diff (goal_policy)= 38 (y subiendo)
      - r_eventual=-0.3
      - init_policy_entropy=0.53, goal_policy_entropy=0.3
      - term_cond_prob init_policy converge a 0 y la goal_policy también
      - num_objs:
         - city: 2.55
         - airport: 2.67
         - location: 0.3
         - airplane: 3
         - truck: 2.8
         - package: 6.1

   >>> Análisis
      - Se generan problemas relativamente difíciles (la dificultad seguía aumentando cuando paré el entrenamiento) y más o menos diversos
      - El problema es que casi todas las ciudades tienen un único airport y los problemas tienen pocos locations


>  <logistics>
   max_atoms_init_state=15, max_actions_init_state=1, max_actions_goal_state=5.0
   <init_policy_entropy_coeffs: 0.2, None>
   goal_policy_entropy_coeffs: 0.2, (10000, 0)
   <diversity_rescale_factor=100>
   use feature_weights for diversity_reward
   nlm does NOT receive problem max size as input

   > Ejecutándose en Casa
   > logs: init_policy/version_16, 17
   > saved_models: both_policies_2, 3

   > Entrenamiento (its=11000, lo paré a mitad)
      - Tiempo (casa): 2d aprox.
      - r_diff (goal_policy)= 18
      - r_eventual=-0.47 (muy alta!!! Genera problemas inconsistentes!!)
      - init_policy_entropy=0.65, goal_policy_entropy=0.8
      - term_cond_prob init_policy converge a 0 y la goal_policy alrededor de 1e-4
      - num_objs:
         - city: 2.02
         - airport: 2.7
         - location: 2
         - airplane: 2.8
         - truck: 4.1
         - package: 3.4

   >>> Análisis
      - Al aumentar diversity_rescale_factor a 100 no aprende!!
         - r_diff es baja
         - r_eventual es alta (se generan problemas inconsistentes)

      - Además, se generan problemas con pocas ciudades!!!


>  <logistics>
   max_atoms_init_state=15, max_actions_init_state=1, max_actions_goal_state=5.0
   <init_policy_entropy_coeffs: 0.1, None>
   goal_policy_entropy_coeffs: 0.2, (10000, 0)
   <diversity_rescale_factor=100>
   use feature_weights for diversity_reward
   nlm does NOT receive problem max size as input

   > Ejecutándose en Casa
   > logs: init_policy/version_18
   > saved_models: both_policies_4

   > Paré el entrenamiento a mitad porque num_cities era 2.048


>  <logistics>
   max_atoms_init_state=15, max_actions_init_state=1, max_actions_goal_state=5.0
   <init_policy_entropy_coeffs: 0.2, None>
   goal_policy_entropy_coeffs: 0.2, (10000, 0)
   <diversity_rescale_factor=20>
   use feature_weights for diversity_reward
   nlm does NOT receive problem max size as input

   > Ejecutándose en Casa
   > logs: init_policy/version_19, 20
   > saved_models: both_policies_5, 6

   >>> Paré el entrenamiento a mitad porque num_cities era 2.043
      - Además, la dificultad era menos que usando diversity_rescale_factor=50
        (la diff era alrededor de 30)


>  <logistics>
   max_atoms_init_state=15, max_actions_init_state=1, max_actions_goal_state=5.0
   <init_policy_entropy_coeffs: 0.1, None>
   goal_policy_entropy_coeffs: 0.2, (10000, 0)
   <diversity_rescale_factor=20>
   use feature_weights for diversity_reward
   nlm does NOT receive problem max size as input

   > Ejecutándose en Casa
   > logs: init_policy/version_21

   >>> Lo paré a mitad (its=5900)
      - Tiempo (casa): 2d aprox.
      - r_diff (goal_policy)= 60
      - r_eventual=-0.15
      - init_policy_entropy=0.6, goal_policy_entropy=0.85 (y bajando)
      - num_objs:
         - city: 2.09 (y bajando)
         - airport: 2.17
         - location: 2.23
         - airplane: 1.48
         - truck: 2.33
         - package: 6.76

   >>> Análisis
      - La entropía es demasiado baja (se generan problemas muy parecidos)
      - Además, el número de ciudades es bajo y casi todas las ciudades tienen
        un solo aiport
      - No obstante, el número de locations es alto
      
      >>> Nota: quizás debería haber usado goal_policy_entropy_coeffs: 0.2, (5000, 0) o parecido
         (la dificultad aumenta muy rápido y la goal_policy entropy es aún bastante alta)



>  <logistics>
   max_atoms_init_state=15, max_actions_init_state=1, max_actions_goal_state=5.0
   <init_policy_entropy_coeffs: 0.1, None>
   goal_policy_entropy_coeffs: 0.2, (10000, 0)
   <diversity_rescale_factor=50>
   use feature_weights for diversity_reward
   nlm does NOT receive problem max size as input
   <diversity reward without inconsistent problems>

   > Ejecutándose en Casa
   > logs: init_policy/version_22

   > Entrenamiento (its=17000
      - Tiempo (casa): 10d aprox.
      - r_diff (goal_policy)= 100 (muy alta!!! aunque después baja a 70)
      - r_eventual=-0.02
      - init_policy_entropy=0.68, goal_policy_entropy=0.2
      - term_cond_prob init_policy converge a 0 y la goal_policy alrededor de 1e-3
      - num_objs:
         - city: 2.06 (el número de ciudades llegó a 2.8 pero desde ese punto nunca dejó de bajar hasta 2)
         - airport: 2.8
         - location: 2.7
         - airplane: 2.2
         - truck: 2.9
         - package: 4.4

   > Problemas
      > its=15000
         - max_atoms = 15
            - diff = [109. 82.7 114.5] (muy alta!)
            - diversidad media-alta
               - Todos los problemas con 2 ciudades menos uno (con tres ciudades)
               - Problemas con muchos locations
               - El resto de objetos varían bastante

         - max_atoms = 20
            - diff = [238.3 162.8 185.4]
            - Problemas con 2 y 3 ciudades (aumenta el número de problemas con 3 ciudades)

         - max_atoms = 30
            - diff = [830.4 527.6 1686.8]
            - Problemas con 2 y 3 ciudades

         - max_atoms = 40
            - diff = [969.7 709.8 5815.4]
            - Problemas con 2 y 3 ciudades

      > its=10000
         - max_atoms = 15
            - diff = [34.6 30.6 33.4] (media)
            - diversidad alta
               - Problemas con dos y tres ciudades

         - max_atoms = 20
            - diff = [67.2 61.4 74.8]
            - Problemas con 2 y 3 ciudades

         - max_atoms = 30
            - diff = [202.6 160.7 264.3]
            - Problemas con 2 y 3 ciudades y un problema con 4 ciudades

         - max_atoms = 40
            - diff = [323.8 248. 1350.7]
            - Problemas con 2, 3 y 4 ciudades

   >>> Análisis
      - Mejor num its=15000
      - Se generan problemas difíciles y diversos!
         - Los problemas tienen locations y más de dos ciudades (aunque debería aumentar un poco el número de ciudades)

      - La dificultad escala bien, pero no el número de ciudades


>  <logistics>
   max_atoms_init_state=15, max_actions_init_state=1, max_actions_goal_state=5.0
   <init_policy_entropy_coeffs: 0.1, None>
   <goal_policy_entropy_coeffs: 0.2, (5000, 0)>
   <diversity_rescale_factor=50>
   use feature_weights for diversity_reward
   nlm does NOT receive problem max size as input
   <diversity reward without inconsistent problems>

   > Ejecutándose en DaSCI
   > logs: init_policy/version_0 (DaSCI)

   >>> Creo que es mejor disminuir la entropía más rápido 
       (usar goal_policy_entropy_coeffs: 0.2, (5000, 0) que
       goal_policy_entropy_coeffs: 0.2, (10000, 0))


>  <logistics>
   <max_atoms_init_state=(10,20)>, max_actions_init_state=1, max_actions_goal_state=5.0
   init_policy_entropy_coeffs: 0.1, None
   <goal_policy_entropy_coeffs: 0.2, (5000, 0)>
   diversity_rescale_factor=50
   use feature_weights for diversity_reward
   <nlm receives problem max size as input>
   <diversity reward without inconsistent problems>

   > Ejecutándose en Casa
   > logs: init_policy/version_0

   > Entrenamiento (its=10000)
      - Tiempo (casa): 5d
      - r_diff (goal_policy)= 50 (media para problemas entre 10 y 20 átomos)
      - r_eventual=-0.03
      - init_policy_entropy=0.7, goal_policy_entropy=0.15
      - term_cond_prob init_policy converge a 0 y la goal_policy alrededor de 1e-3
      - num_objs:
         - city: 2.04
         - airport: 5.06
         - location: 1
         - airplane: 1.6
         - truck: 2.2
         - package: 5.9

   > Problemas (its=10000)
      - max_atoms = 15
         - diff = [49.6 37.6 43.7]
         - diversidad media
            - Todos los problemas con 2 ciudades
            - Por el resto, problemas muy variados

      - max_atoms = 20
         - diff = [100.7  80.7  85.6]
         - Problemas con 2 ciudades

      - max_atoms = 30
         - diff = [192.8 139.4 142.4]
         - Problemas con 2 ciudades, menos uno con tres ciudades

      - max_atoms = 40
         - diff = [206.6 165.9 203.7]
         - Problemas con 2 ciudades, menos uno con tres ciudades

   >>> Análisis
      - Se obtienen peores resultados que entrenando en problemas de solo 15 átomos
      - En vez de generarse problemas con más ciudades (y generalizar mejor a problemas más grandes),
        se generan problemas con menos ciudades
      - Hipótesis de por qué no se generan problemas con más ciudades:
         - Las generative policies aprenden a generar problemas con solo dos ciudades porque los problemas pequeños
           (ej.: 10 átomos) son más difíciles con solo dos ciudades. Además, quizás a la NLM le cuesta "trabajar"
           con el input del max_num_atoms (quizás debería añadir una "counting reduction" a la NLM?)
         - También puede pasar que incluso los problemas grandes son difíciles con solo dos ciudades (realmente hace
           falta generar problemas con más de dos ciudades?)

>  <logistics>
   <max_atoms_init_state=(15,20)>, max_actions_init_state=1, max_actions_goal_state=5.0
   init_policy_entropy_coeffs: 0.1, None
   goal_policy_entropy_coeffs: 0.2, (5000, 0)
   diversity_rescale_factor=50
   use feature_weights for diversity_reward
   <nlm receives problem max size as input>
   diversity reward without inconsistent problems
   <minibatch_size=20>

   > Ejecutándose en Casa
   > logs: init_policy/version_1, 2, 3, 4, 5

   > Entrenamiento (its=10000)
      - Tiempo (casa): 7d
      - r_diff (goal_policy)= 40 (media para problemas entre 15 y 20 átomos)
         >>> Las gráficas de dificultad son muy inestables!
      - r_eventual=-0.01
      - init_policy_entropy=0.7, goal_policy_entropy=0.3
      - term_cond_prob init_policy converge a 0 y la goal_policy alrededor de 1e-3
      - num_objs:
         - city: 2.04
         - airport: 4.6
         - location: 1.5
         - airplane: 1.9
         - truck: 2.3
         - package: 7

   > Problemas (its=7500) (num its con mayor difficulty)
      - max_atoms = 15
         - diff = [44.8 37.2 39.8]
         - diversidad media
            - Todos los problemas con 2 ciudades, uno con 3 ciudades
            - Por el resto, problemas bastante variados 
      
      - max_atoms = 20
         - diff = [74.6 61.7 67.9]
         - diversidad media-baja
            - Todos los problemas con 2 ciudades

      - max_atoms = 30
         - diff = [188.3 147.5 155.2]
         - Todos los problemas con 2 ciudades, uno con 3 ciudades

      - max_atoms = 40
         - diff = [376.7 268.8 334.7]
         - Todos los problemas con 2 ciudades, dos con 3 ciudades 

   >>> Análisis
      Peores resultados que entrenando solo en problemas de tamaño 15!
      El número de ciudades no aumenta con el tamaño de los problemas.
      Posibles razones:
         >>> menor minibatch_size
         - la NLM no se comporta bien con el tamaño del problema como input
         - Entrenar en problemas de 20 átomos no es suficiente

------------- Blocksworld

>  <blocksworld>
   max_atoms_init_state=15, max_actions_init_state=1, <max_actions_goal_state=3.0>
   init_policy_entropy_coeffs: 0.1, None
   goal_policy_entropy_coeffs: 0.2, (5000, 0)
   diversity_rescale_factor=50
   use feature_weights for diversity_reward
   nlm does NOT receive problem max size as input
   diversity reward without inconsistent problems

   > Ejecutándose en Casa
   > logs: init_policy/1

   > Entrenamiento (its=10000)
      - Tiempo (casa): 4d
      - r_diff (goal_policy)= 600
      - r_eventual=-0.02
      - init_policy_entropy=0.5, goal_policy_entropy=0.1
      - term_cond_prob init_policy converge a 0.04 y la goal_policy a 0.02
      - 11 bloques
      - num_atoms:
         - handempty: 0.5
         - holding: 0.5
         - ontable: 5.2 (num towers)
         - clear: 5.2
         - on: 4.8

   > Problemas (its=10000)
      - max_atoms = 15
         - diff = [376.3 437. 484.] (mucha variabilidad, dificultad alta)
         - diversidad media-alta
            - Problemas con distinto número de bloques
            - Problemas con distinto número de torres
            - A veces se generan problems idénticos

      - max_atoms = 20
         - diff = [763.4 1445.9 1433.6]
         - Se tiende a generar muchos problemas con una única torre de bloques,
           mientras que otros tienen muchas torres pequeñas

      - max_atoms = 30
         - diff = [10293.62 155489.84 4426.78]

      - max_atoms = 40
         - diff = [1965.22 292455.7 7396.78]

   >>> Análisis
      - Se generan problemas muy difíciles y la dificultad escala muy bien con el tamaño del problema!!!
      - No obstante, a veces se generan problemas repetidos
         - Quizás sea necesario aumentar la diversidad (policy_entropy o diversity_scale_factor)

      - En resumen, los mismos hiperparámetros de logistics funcionan bien para blocksworld!

------------ Sokoban

>  <sokoban>
   <max_atoms_init_state=40+15>, max_actions_init_state=1, <max_actions_goal_state=1.0>
   init_policy_entropy_coeffs: 0.1, None
   goal_policy_entropy_coeffs: 0.2, (5000, 0)
   diversity_rescale_factor=50
   use feature_weights for diversity_reward
   nlm does NOT receive problem max size as input
   diversity reward without inconsistent problems

   > Ejecutándose en Casa
   > logs: init_policy/1

   > Entrenamiento (its=8000)
      - Tiempo (casa): 8d
         - El entrenamiento se ralentiza muchísimo debido a la gran dificultad de los problemas
      - r_diff (goal_policy)= 4.5e+6
      - r_eventual=-0.02
      - init_policy_entropy=0.45, goal_policy_entropy=0.15
      - term_cond_prob init_policy 0.1, goal_policy 0.02
      - num_atoms:
         - at-box 5.5
         - at-wall 2
         > No se llega al número máximo de átomos extra (15)

   > Problemas (its=7500)
      - Tamaño 5x5, max_atoms=40+15
         - diff = [4.76e6 4.72e6 2.16e6]
         - diversidad muy baja
            - Muchos problemas son completamente idénticos (cerca del 40%)

   > Análisis
      - La dificultad es extremadamente alta (1000 veces la de los instance generators) pero la diversidad es muy baja
      - Es necesario aumentar la diversidad

>  <sokoban>
   max_atoms_init_state=40+15, max_actions_init_state=1, max_actions_goal_state=1.0
   init_policy_entropy_coeffs: 0.1, None
   goal_policy_entropy_coeffs: 0.2, (5000, 0)
   <diversity_rescale_factor=100>
   use feature_weights for diversity_reward
   nlm does NOT receive problem max size as input
   diversity reward without inconsistent problems

   > Ejecutándose en Casa
   > logs: init_policy/2, 3

   > Entrenamiento (its=35000)
      - Tiempo (casa): 15d
      - r_diff (goal_policy) muy inestable, con muchos picos. Alcanza 2.5e+4, aunque está alrededor de 1e+4 en las últimas iteraciones.
      - r_eventual=-0.02
      - init_policy_entropy=0.92, goal_policy_entropy=0.07
      - term_cond_prob init_policy 0.09, goal_policy 0.01
      - num_atoms:
         - at-box 3.5
         - at-wall 2
         > No se llega al número máximo de átomos extra (15)
         > Gran varianza en el número de objetos

   > Problemas (its=30000)
      - Tamaño 5x5, max_atoms=40+15
         - diff = [4232.65 3468.8  7601.45]
            - Todos los problemas tienen dificultad muy baja (menos de 10) menos uno,
              que tiene dificultad [84595, 69310, 151965]

         - diversidad
            - Diversidad muy alta
            - Número de átomos en los problemas:
               - at-box: [4,8,2,1,5,8,1,6,6,1,8,8,6,1,6,5,8,4,7,1]
               - at-wall: [0,0,12,0,9,6,0,8,8,0,6,6,0,0,8,9,6,10,7,1]
                  - Los problemas que tienen el mismo número de cajas y paredes no son idénticos
                    (estos objetos aparecen en distintas posiciones en cada problema)

   > Análisis
      - La diversidad es mucho más alta que usando diversity_rescale_factor=50
      - Pero ahora solo un pequeño porcentaje de problemas son difíciles (el resto son muy fáciles)
      - Además, las gráficas de loss tienen muchos picos <<(parece que el entrenamiento es inestable)>>
      > Creo que debo bajar la diversity_rescale_factor y debo hacer el entrenamiento más estable

>  <sokoban>
   max_atoms_init_state=40+15, max_actions_init_state=1, max_actions_goal_state=1.0
   <init_policy_entropy_coeffs: 0.2, None>
   goal_policy_entropy_coeffs: 0.2, (5000, 0)
   <diversity_rescale_factor=50>
   use feature_weights for diversity_reward
   nlm does NOT receive problem max size as input
   diversity reward without inconsistent problems
   <gradient-clipping=0.1> 
  



   <Ver si el gradient clipping hace que no haya tantos picos en el entrenamiento y que sea más estable>
      - Si no, probar a usar gradient-clipping=0.01




----- Mejor modelo hasta la fecha (logistics y blocksworld)

max_atoms_init_state=15, max_actions_init_state=1, max_actions_goal_state=5.0
<init_policy_entropy_coeffs: 0.1, None>
goal_policy_entropy_coeffs: 0.2, (10000, 0)
<diversity_rescale_factor=50>
use feature_weights for diversity_reward
nlm does NOT receive problem max size as input
<diversity reward without inconsistent problems>

--------






-------- Siguientes experimentos

> Ver mejores valores de entropía (para que no tenga que hacer early stopping)
   - init=1, goal=0.05
   - init=1, goal=0.1
   -> No funcionan bien!!! La dificultad disminuye al aumentar los entropy_coeffs, pero no
      aumenta el número de ciudades!
   >>> CREO QUE ES NECESARIO USAR DIVERSITY_REWARD ADEMÁS DE POLICY ENTROPY

   - Experimentos con policy_entropy y diversity_reward
      - Ver qué debería aumentar para que se generen más ciudades y a la vez problemas difíciles
         - init_policy entropy o diversity_reward 

         - init_policy_entropy 0.2 vs 0.5 (con goal entropy=0.1)
            - Misma dificultad
            - entropy=0.2 genera problemas con más ciudades (2.16 vs 2.075)
            - entropy=0.2 funciona mejor!

         - init_policy_entropy 0.1 vs 0.2 vs 0.5 (con goal entropy annealed to 0)
            - init_policy_entropy 0.2 funciona mejor que 0.5 (genera problemas más difíciles y con un poco más de ciudades!)
            - Creo que es mejor usar goal_entropy=0.2, (10000, 0) que goal_entropy=0.1

            >>> No sé si es mejor usar init_policy entropy=0.1 o 0.2 (creo que 0.1)
               -0.1: se generan problemas con bastantes ciudades pero pocos airports y locations
               -0.2: se generan problemas con pocas ciudades (no escalan) pero muchos airports y locations 

         - Experimentos distintos diversity_rescale_factor
            - init_policy_entropy 0.2, diversity_rescale_factor=100
               - Peor que usando diversity_rescale_factor=50, las generative policies no aprenden!!! (y se generan pocas ciudades)

            - init_policy_entropy 0.1, diversity_rescale_factor=100        
               - De igual forma, se generan problemas con pocas ciudades 

            - init_policy_entropy 0.2, diversity_rescale_factor=20
               - Se generan problemas con pocas ciudades y peor dificultad que con diversity_rescale_factor=50

            - init_policy_entropy 0.1, diversity_rescale_factor=20        
               - Se generan problemas con pocas ciudades y airports (poca diversidad), aunque con muchas locations.
                 La dificultad es alta

            >>> Conclusiones:
               - diversity_rescale_factor=100 es demasiado alto
               - init_policy_entropy=0.1 es mejor que 0.2
               >>> Mejor modelo de los experimentos:
                     init_policy_entropy_coeffs: 0.1, None>
                     goal_policy_entropy_coeffs: 0.2, (10000, 0)
                     diversity_rescale_factor=50


   - Experimentos diversity_reward without inconsistent problems
      - diversity_rescale_factor=50, init_policy_entropy=0.1, goal_policy_entropy_coeffs: 0.2, (10000, 0)
         - Mejores resultados hasta la fecha!

      - diversity_rescale_factor=50, init_policy_entropy=0.1, goal_policy_entropy_coeffs: 0.2, (5000, 0)
         - Creo que los resultados son mejores que usando goal_entropy=0.2, 5000, 0
            - Se generan más ciudades y los problemas también son difíciles

      >>> Resultados hasta ahora
         - Funciona mucho mejor que cuando doy diversity_reward a todos los problemas!!!!
            - Los problemas son muy diversos, con muchas ciudades, airports y locations
            - Además, la eventual_consistency reward es muy buena
            - Los problemas también son muy difíciles!!


      >>> Hypothesis
         - Creo que la razón por la que el modelo con diversity_rescale_factor=100 no aprende (ej.: alta eventual consistency reward)
           es porque doy diversity_reward también a los problemas inconsistentes. Si esta recompensa es muy alta, la init policy
           puede aprender a generar estados iniciales muy diversos incluso si estos son inconsistentes


   - Experimentos training on problems of different size
      - max_atoms=(10,20)
         - Funciona peor que entrenando en problemas con solo max_atoms=15

      - max_atoms=(15,20)
         - Los resultados son peores que entrenando en problemas con solo max_atoms=15!!!

      >>> Analysis
         - Entrenar en problemas de distintos tamaños no ayuda. Posibles razones:
            - Reducir minibatch_size a 10 (debido a GPU memory) hace que el entrenamiento sea inestable
            - Es necesario entrenar en problemas aún más grandes, como de 30 átomos para que "merezca la pena" generar problemas con más de dos ciudades
            - La NLM no se comporta bien con inputs enteros
            - La NLM no sabe contar? (no hay reduce operation para contar)

----- TODO

   - Probar a añadir a la NLM "counting quantifiers" (para que aprenda a contar el <porcentaje> número de átomos y objetos de cada tipo)
